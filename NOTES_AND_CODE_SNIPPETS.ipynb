{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NOTES.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPm9SYQfgEZNLv5wJKCRlVg",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/BibhuDas123/Exploratory_Data_Analysis/blob/master/NOTES_AND_CODE_SNIPPETS.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7miBUKnkO11b",
        "colab_type": "text"
      },
      "source": [
        "Most of the time, it’s also possible to convert a supervised dataset to unsupervised  to see how they look like when plotted.  For example, let’s take a look at the dataset in figure 3. Figure 3 shows MNIST  dataset which is a very popular dataset of handwritten digits, and it is a supervised  problem in which you are given the images of the numbers and the correct label  associated with them. You have to build a model that can identify which digit is it  when provided only with the image.  This dataset can easily be converted to an unsupervised setting for basic  visualization. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dRbvRc1UO_k5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 335
        },
        "outputId": "874c5ae5-a317-4fd2-cf44-e1a63aa1aaaa"
      },
      "source": [
        "import matplotlib.pyplot as plt  \n",
        "import numpy as np  \n",
        "import pandas as pd  \n",
        "import seaborn as sns  \n",
        "from sklearn import datasets  \n",
        "from sklearn import manifold \n",
        "%matplotlib inline \n",
        "\n",
        "data = datasets.fetch_openml(  'mnist_784',  version=1,  return_X_y=True)  \n",
        "pixel_values, targets = data  \n",
        "targets = targets.astype(int) \n",
        "\n",
        "#pixel_values is a 2-dimensional array of shape 70000x784. There are 70000  different images, each of size 28x28 pixels. \n",
        "#Flattening 28x28 gives 784 data points.  We can visualize the samples in this dataset by reshaping them to their original  \n",
        "#shape and then plotting them using matplotlib. \n",
        "single_image = pixel_values[1, :].reshape(28, 28)  \n",
        "plt.imshow(single_image, cmap='gray') "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
            "  import pandas.util.testing as tm\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7fceecb7e390>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD4CAYAAAAq5pAIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAOF0lEQVR4nO3dcYxV5ZnH8d8jW4xKIagpTkRr2+AfzUYHQUKyprI2bVw0gcakQozDpk2GxJJQszGr3VFIamNjlEZNJE6VFFcqqGjBpi51GaLdmDSOyCpqW1mDFhwZUSNDTKTCs3/cQzPinPcM9557z4Hn+0km997zzLn38TI/z7nnPfe85u4CcPI7peoGAHQGYQeCIOxAEIQdCIKwA0H8QydfzMw49A+0mbvbWMtb2rKb2ZVm9mcz22VmN7fyXADay5odZzezCZL+Iuk7kvZIelHSYnd/PbEOW3agzdqxZZ8jaZe7v+XuhyStl7SghecD0EathP1cSX8d9XhPtuxzzKzXzAbNbLCF1wLQorYfoHP3fkn9ErvxQJVa2bLvlXTeqMfTs2UAaqiVsL8oaYaZfc3MJkpaJGlzOW0BKFvTu/Hu/pmZLZO0RdIESWvc/bXSOgNQqqaH3pp6MT6zA23XlpNqAJw4CDsQBGEHgiDsQBCEHQiCsANBEHYgCMIOBEHYgSAIOxAEYQeCIOxAEIQdCIKwA0EQdiAIwg4EQdiBIAg7EARhB4Ig7EAQhB0IoqNTNuPkM2vWrGR92bJlubWenp7kug8//HCyft999yXr27dvT9ajYcsOBEHYgSAIOxAEYQeCIOxAEIQdCIKwA0EwiyuSuru7k/WBgYFkffLkyWW28zkff/xxsn7WWWe17bXrLG8W15ZOqjGz3ZJGJB2W9Jm7z27l+QC0Txln0P2zu+8v4XkAtBGf2YEgWg27S/q9mb1kZr1j/YKZ9ZrZoJkNtvhaAFrQ6m78Ze6+18y+IulZM/uTuz8/+hfcvV9Sv8QBOqBKLW3Z3X1vdjss6SlJc8poCkD5mg67mZ1hZl8+el/SdyXtLKsxAOVqZTd+mqSnzOzo8/za3f+rlK7QMXPmpHfGNm7cmKxPmTIlWU+dxzEyMpJc99ChQ8l60Tj63Llzc2tF33Uveu0TUdNhd/e3JF1cYi8A2oihNyAIwg4EQdiBIAg7EARhB4LgK64ngdNPPz23dskllyTXfeSRR5L16dOnJ+vZ0Guu1N9X0fDXnXfemayvX78+WU/11tfXl1z3jjvuSNbrLO8rrmzZgSAIOxAEYQeCIOxAEIQdCIKwA0EQdiAIpmw+CTzwwAO5tcWLF3ewk+NTdA7ApEmTkvXnnnsuWZ83b15u7aKLLkquezJiyw4EQdiBIAg7EARhB4Ig7EAQhB0IgrADQTDOfgKYNWtWsn7VVVfl1oq+b16kaCz76aefTtbvuuuu3Nq7776bXPfll19O1j/66KNk/Yorrsittfq+nIjYsgNBEHYgCMIOBEHYgSAIOxAEYQeCIOxAEFw3vga6u7uT9YGBgWR98uTJTb/2M888k6wXfR/+8ssvT9ZT3xt/8MEHk+u+//77yXqRw4cP59Y++eST5LpF/11F17yvUtPXjTezNWY2bGY7Ry0708yeNbM3s9upZTYLoHzj2Y3/laQrj1l2s6St7j5D0tbsMYAaKwy7uz8v6cNjFi+QtDa7v1bSwpL7AlCyZs+Nn+buQ9n99yRNy/tFM+uV1Nvk6wAoSctfhHF3Tx14c/d+Sf0SB+iAKjU79LbPzLokKbsdLq8lAO3QbNg3S1qS3V8iaVM57QBol8JxdjN7VNI8SWdL2idphaTfSHpM0vmS3pb0fXc/9iDeWM8Vcjf+wgsvTNZXrFiRrC9atChZ379/f25taGgotyZJt99+e7L+xBNPJOt1lhpnL/q737BhQ7J+3XXXNdVTJ+SNsxd+Znf3vLMqvt1SRwA6itNlgSAIOxAEYQeCIOxAEIQdCIJLSZfg1FNPTdZTl1OWpPnz5yfrIyMjyXpPT09ubXBwMLnuaaedlqxHdf7551fdQunYsgNBEHYgCMIOBEHYgSAIOxAEYQeCIOxAEIyzl2DmzJnJetE4epEFCxYk60XTKgMSW3YgDMIOBEHYgSAIOxAEYQeCIOxAEIQdCIJx9hKsWrUqWTcb88q+f1c0Ts44enNOOSV/W3bkyJEOdlIPbNmBIAg7EARhB4Ig7EAQhB0IgrADQRB2IAjG2cfp6quvzq11d3cn1y2aHnjz5s1N9YS01Fh60b/Jjh07ym6ncoVbdjNbY2bDZrZz1LKVZrbXzHZkP61dnQFA241nN/5Xkq4cY/kv3L07+/lduW0BKFth2N39eUkfdqAXAG3UygG6ZWb2SrabPzXvl8ys18wGzSw96RiAtmo27KslfUNSt6QhSXfn/aK797v7bHef3eRrAShBU2F3933uftjdj0j6paQ55bYFoGxNhd3MukY9/J6knXm/C6AeCsfZzexRSfMknW1meyStkDTPzLoluaTdkpa2scdaSM1jPnHixOS6w8PDyfqGDRua6ulkVzTv/cqVK5t+7oGBgWT9lltuafq566ow7O6+eIzFD7WhFwBtxOmyQBCEHQiCsANBEHYgCMIOBMFXXDvg008/TdaHhoY61Em9FA2t9fX1Jes33XRTsr5nz57c2t135570KUk6ePBgsn4iYssOBEHYgSAIOxAEYQeCIOxAEIQdCIKwA0Ewzt4BkS8VnbrMdtE4+bXXXpusb9q0KVm/5pprkvVo2LIDQRB2IAjCDgRB2IEgCDsQBGEHgiDsQBCMs4+TmTVVk6SFCxcm68uXL2+qpzq48cYbk/Vbb701tzZlypTkuuvWrUvWe3p6knV8Hlt2IAjCDgRB2IEgCDsQBGEHgiDsQBCEHQiCcfZxcvemapJ0zjnnJOv33ntvsr5mzZpk/YMPPsitzZ07N7nu9ddfn6xffPHFyfr06dOT9XfeeSe3tmXLluS6999/f7KO41O4ZTez88xsm5m9bmavmdnybPmZZvasmb2Z3U5tf7sAmjWe3fjPJP2bu39T0lxJPzKzb0q6WdJWd58haWv2GEBNFYbd3YfcfXt2f0TSG5LOlbRA0trs19ZKSp8TCqBSx/WZ3cwukDRT0h8lTXP3o5OUvSdpWs46vZJ6m28RQBnGfTTezCZJ2ijpx+5+YHTNG0eoxjxK5e797j7b3We31CmAlowr7Gb2JTWCvs7dn8wW7zOzrqzeJWm4PS0CKEPhbrw1vr/5kKQ33H3VqNJmSUsk/Ty7TV/XN7AJEyYk6zfccEOyXnRJ5AMHDuTWZsyYkVy3VS+88EKyvm3bttzabbfdVnY7SBjPZ/Z/knS9pFfNbEe27CdqhPwxM/uhpLclfb89LQIoQ2HY3f1/JOVdneHb5bYDoF04XRYIgrADQRB2IAjCDgRB2IEgrOjrmaW+mFnnXqxkqa9yPv7448l1L7300pZeu+hS1a38G6a+HitJ69evT9ZP5Mtgn6zcfcw/GLbsQBCEHQiCsANBEHYgCMIOBEHYgSAIOxAE4+wl6OrqStaXLl2arPf19SXrrYyz33PPPcl1V69enazv2rUrWUf9MM4OBEfYgSAIOxAEYQeCIOxAEIQdCIKwA0Ewzg6cZBhnB4Ij7EAQhB0IgrADQRB2IAjCDgRB2IEgCsNuZueZ2TYze93MXjOz5dnylWa218x2ZD/z298ugGYVnlRjZl2Sutx9u5l9WdJLkhaqMR/7QXe/a9wvxkk1QNvlnVQznvnZhyQNZfdHzOwNSeeW2x6Adjuuz+xmdoGkmZL+mC1aZmavmNkaM5uas06vmQ2a2WBLnQJoybjPjTezSZKek/Qzd3/SzKZJ2i/JJf1UjV39HxQ8B7vxQJvl7caPK+xm9iVJv5W0xd1XjVG/QNJv3f0fC56HsANt1vQXYaxxadOHJL0xOujZgbujvidpZ6tNAmif8RyNv0zSHyS9KulItvgnkhZL6lZjN363pKXZwbzUc7FlB9qspd34shB2oP34PjsQHGEHgiDsQBCEHQiCsANBEHYgCMIOBEHYgSAIOxAEYQeCIOxAEIQdCIKwA0EQdiCIwgtOlmy/pLdHPT47W1ZHde2trn1J9NasMnv7al6ho99n/8KLmw26++zKGkioa2917Uuit2Z1qjd244EgCDsQRNVh76/49VPq2ltd+5LorVkd6a3Sz+wAOqfqLTuADiHsQBCVhN3MrjSzP5vZLjO7uYoe8pjZbjN7NZuGutL56bI59IbNbOeoZWea2bNm9mZ2O+YcexX1VotpvBPTjFf63lU9/XnHP7Ob2QRJf5H0HUl7JL0oabG7v97RRnKY2W5Js9298hMwzOxbkg5Kevjo1FpmdqekD93959n/KKe6+7/XpLeVOs5pvNvUW9404/+qCt+7Mqc/b0YVW/Y5kna5+1vufkjSekkLKuij9tz9eUkfHrN4gaS12f21avyxdFxOb7Xg7kPuvj27PyLp6DTjlb53ib46ooqwnyvpr6Me71G95nt3Sb83s5fMrLfqZsYwbdQ0W+9JmlZlM2MonMa7k46ZZrw2710z05+3igN0X3SZu18i6V8k/SjbXa0lb3wGq9PY6WpJ31BjDsAhSXdX2Uw2zfhGST929wOja1W+d2P01ZH3rYqw75V03qjH07NlteDue7PbYUlPqfGxo072HZ1BN7sdrrifv3P3fe5+2N2PSPqlKnzvsmnGN0pa5+5PZosrf+/G6qtT71sVYX9R0gwz+5qZTZS0SNLmCvr4AjM7IztwIjM7Q9J3Vb+pqDdLWpLdXyJpU4W9fE5dpvHOm2ZcFb93lU9/7u4d/5E0X40j8v8n6T+q6CGnr69L+t/s57Wqe5P0qBq7dX9T49jGDyWdJWmrpDcl/bekM2vU23+qMbX3K2oEq6ui3i5TYxf9FUk7sp/5Vb93ib468r5xuiwQBAfogCAIOxAEYQeCIOxAEIQdCIKwA0EQdiCI/wcI826NkY1TiQAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rclZkG7DPmrm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 635
        },
        "outputId": "311cc4dc-b727-4946-ec6f-68ba11524dbf"
      },
      "source": [
        "tsne = manifold.TSNE(n_components=2, random_state=42)  \n",
        "transformed_data = tsne.fit_transform(pixel_values[:3000,:]) \n",
        "# This step creates the t-SNE transformation of the data. We use only two components  as we can visualize them well in a two-dimensional setting. \n",
        "#The transformed_data,  in this case, is an array of shape 3000x2 (3000 rows and 2 columns). A data like  this can be converted to a pandas \n",
        "#dataframe by calling pd.DataFrame on the array.   \n",
        "tsne_df = pd.DataFrame(  np.column_stack((transformed_data, targets[:3000])),  \n",
        "                       columns=[\"x\", \"y\", \"targets\"]  )  \n",
        "tsne_df.loc[:, \"targets\"] = tsne_df.targets.astype(int)  \n",
        "\n",
        "grid = sns.FacetGrid(tsne_df, hue=\"targets\", size=8)  \n",
        "grid.map(plt.scatter, \"x\", \"y\").add_legend() "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/seaborn/axisgrid.py:243: UserWarning: The `size` parameter has been renamed to `height`; please update your code.\n",
            "  warnings.warn(msg, UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<seaborn.axisgrid.FacetGrid at 0x7fceec6fb240>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 619.625x576 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V4LYrvjIO1wk",
        "colab_type": "text"
      },
      "source": [
        "Occam’s razor in simple words states that one should not try to complicate things  that can be solved in a much simpler manner. In other words, the simplest solutions  are the most generalizable solutions. In general, whenever your model does not  obey Occam’s razor, it is probably overfitting. \n",
        "\n",
        "There are many different ways one can do cross-validation, and it is the most critical  step when it comes to building a good machine learning model which is  generalizable when it comes to unseen data. Choosing the right cross-validation  depends on the dataset you are dealing with, and one’s choice of cross-validation  on one dataset may or may not apply to other datasets. However, there are a few  types of cross-validation techniques which are the most popular and widely used.  These include:  \n",
        "• k-fold cross-validation  • stratified k-fold cross-validation • hold-out based validation  • leave-one-out cross-validation  • group k-fold cross-validation \n",
        "\n",
        "\n",
        " "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HFbBAMFEQk-c",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#KFOLD CROSS VALIDATION\n",
        "#import pandas and model_selection module of scikit-learn  \n",
        "import pandas as pd  \n",
        "from sklearn import model_selection  \n",
        "if  __name__  ==  \"__main__\":  \n",
        "  #Training data is in a CSV file called train.csv  \n",
        "  df = pd.read_csv(\"train.csv\")  \n",
        "  #we create a new column called kfold and fill it with -1  \n",
        "  df[\"kfold\"] = -1  \n",
        "  #the next step is to randomize the rows of the data  \n",
        "  df = df.sample(frac=1).reset_index(drop=True)  \n",
        "  #initiate the kfold class from model_selection module  \n",
        "  kf = model_selection.KFold(n_splits=5)  \n",
        "  #fill the new kfold column  \n",
        "  for fold, (trn_, val_) in enumerate(kf.split(X=df)):  \n",
        "    df.loc[val_, 'kfold'] = fold  \n",
        "  #save the new csv with kfold column  \n",
        "  df.to_csv(\"train_folds.csv\", index=False) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tpP5Qc2XO1tx",
        "colab_type": "text"
      },
      "source": [
        "The next important type of cross-validation is stratified k-fold. If you have a  skewed dataset for binary classification with 90% positive samples and only 10%  negative samples, you don't want to use random k-fold cross-validation. Using  simple k-fold cross-validation for a dataset like this can result in folds with all  negative samples. In these cases, we prefer using stratified k-fold cross-validation.  Stratified k-fold cross-validation keeps the ratio of labels in each fold constant. So,  in each fold, you will have the same 90% positive and 10% negative samples.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AgPbZAkDO1rT",
        "colab_type": "text"
      },
      "source": [
        "The rule is simple. If it’s a standard classification problem, choose stratified k-fold  blindly.  But what should we do if we have a large amount of data? Suppose we have 1  million samples. A 5 fold cross-validation would mean training on 800k samples  and validating on 200k. Depending on which algorithm we choose, training and  even validation can be very expensive for a dataset which is of this size. In these  cases, we can opt for a hold-out based validation. \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "The process for creating the hold-out remains the same as stratified k-fold. For a  dataset which has 1 million samples, we can create ten folds instead of 5 and keep  one of those folds as hold-out. This means we will have 100k samples in the holdout, and we will always calculate loss, accuracy and other metrics on this set and  train on 900k samples.  Hold-out is also used very frequently with time-series data. \n",
        "\n",
        "Let’s assume the  problem we are provided with is predicting sales of a store for 2020, and you are  provided all the data from 2015-2019. In this case, you can select all the data for  2019 as a hold-out and train your model on all the data from 2015 to 2018. \n",
        "\n",
        "\n",
        "In the example presented in figure 7, let’s say our job is to predict the sales from  time step 31 to 40. We can then keep 21 to 30 as hold-out and train our model from  step 0 to step 20. You should note that when you are predicting from 31 to 40, you  should include the data from 21 to 30 in your model; otherwise, performance will  be sub-par.  \n",
        "\n",
        "In many cases, we have to deal with small datasets and creating big validation sets  means losing a lot of data for the model to learn. In those cases, we can opt for a  type of k-fold cross-validation where k=N, where N is the number of samples in the  dataset. This means that in all folds of training, we will be training on all data  samples except 1. The number of folds for this type of cross-validation is the same  as the number of samples that we have in the dataset. \n",
        "\n",
        " "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lrQZkeYJSdd1",
        "colab_type": "text"
      },
      "source": [
        "The good thing about regression problems is that  we can use all the cross-validation techniques mentioned above for regression  problems except for stratified k-fold. That is we cannot use stratified k-fold directly,  but there are ways to change the problem a bit so that we can use stratified k-fold  for regression problems. Mostly, simple k-fold cross-validation works for any  regression problem. However, if you see that the distribution of targets is not  consistent, you can use stratified k-fold. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HJYu2RFpSeOz",
        "colab_type": "text"
      },
      "source": [
        "To use stratified k-fold for a regression problem, we have first to divide the target  into bins, and then we can use stratified k-fold in the same way as for classification  problems. There are several choices for selecting the appropriate number of bins. If  you have a lot of samples( > 10k, > 100k), then you don’t need to care about the  number of bins. Just divide the data into 10 or 20 bins. If you do not have a lot of  samples, you can use a simple rule like Sturge’s Rule to calculate the appropriate  number of bins.  Sturge’s rule:  \n",
        "Number of Bins = 1 + log2(N)  \n",
        "\n",
        "Where N is the number of samples you have in your dataset.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9I7up53iS0UP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np  \n",
        "import pandas as pd  \n",
        "from sklearn import datasets  \n",
        "from sklearn import model_selection \n",
        "\n",
        "def create_folds(data):  \n",
        "  #we create a new column called kfold and fill it with -1  \n",
        "  data[\"kfold\"] = -1  \n",
        "  #the next step is to randomize the rows of the data  \n",
        "  data = data.sample(frac=1).reset_index(drop=True)  \n",
        "  #calculate the number of bins by Sturge's rule  #I take the floor of the value, you can also  #just round it  \n",
        "  num_bins = int(np.floor(1 + np.log2(len(data))))  \n",
        "  #bin targets  \n",
        "  data.loc[:, \"bins\"] = pd.cut(  data[\"target\"], bins=num_bins, labels=False  )  \n",
        "  #initiate the kfold class from model_selection module  \n",
        "  kf = model_selection.StratifiedKFold(n_splits=5)  \n",
        "  #fill the new kfold column  #note that, instead of targets, we use bins!  \n",
        "  for f, (t_, v_) in enumerate(kf.split(X=data, y=data.bins.values)):  \n",
        "    data.loc[v_, 'kfold'] = f  \n",
        "    #drop the bins column  \n",
        "    data = data.drop(\"bins\", axis=1)  \n",
        "    #return dataframe with folds  \n",
        "    return data  \n",
        "if  __name__  ==  \"__main__\":  \n",
        "  #we create a sample dataset with 15000 samples  #and 100 features and 1 target  \n",
        "  X, y = datasets.make_regression(  n_samples=15000, n_features=100, n_targets=1  )  \n",
        "  #create a dataframe out of our numpy arrays  \n",
        "  df = pd.DataFrame(  X,  columns=[f\"f_{i}\" for i in range(X.shape[1])]  )  \n",
        "  df.loc[:, \"target\"] = y  \n",
        "  #create folds  \n",
        "  df = create_folds(df) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FB50WDiASeRV",
        "colab_type": "text"
      },
      "source": [
        "first. If you have a good crossvalidation scheme in which validation data is representative of training and realworld data, you will be able to build a good machine learning model which is highly  generalizable. \n",
        "\n",
        "The types of cross-validation presented in this chapter can be applied to almost any  machine learning problem. Still, you must keep in mind that cross-validation also  depends a lot on the data and you might need to adopt new forms of cross-validation  depending on your problem and data.  For example, let’s say we have a problem in which we would like to build a model  to detect skin cancer from skin images of patients. \n",
        "\n",
        "Our task is to build a binary  classifier which takes an input image and predicts the probability for it being benign  or malignant.  In these kinds of datasets, you might have multiple images for the same patient in  the training dataset. So, to build a good cross-validation system here, you must have  stratified k-folds, but you must also make sure that patients in training data do not  appear in validation data. Fortunately, scikit-learn offers a type of cross-validation  known as GroupKFold. Here the patients can be considered as groups. But  unfortunately, there is no way to combine GroupKFold with StratifiedKFold in  scikit-learn.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jaewmrwlSeTy",
        "colab_type": "text"
      },
      "source": [
        "If we talk about classification problems, the most common metrics used are: \n",
        " - Accuracy  - Precision (P)  - Recall (R)  - F1 score (F1)  - Area under the ROC (Receiver Operating Characteristic) curve or simply  AUC (AUC)  - Log loss  - Precision at k (P@k)  - Average precision at k (AP@k)  - Mean average precision at k (MAP@k) \n",
        " \n",
        "  When it comes to regression, the most commonly used evaluation metrics are:  \n",
        "  \n",
        "  - Mean absolute error (MAE)  - Mean squared error (MSE)  - Root mean squared error (RMSE)  - Root mean squared logarithmic error (RMSLE)  - Mean percentage error (MPE)  - Mean absolute percentage error (MAPE)  - R2  Knowing about how the aforementioned metrics work is not the\n",
        "\n",
        "When we have an equal number of positive and negative samples in a binary  classification metric, we generally use accuracy, precision, recall and f1. \n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6py8cJd-VYKE",
        "colab_type": "text"
      },
      "source": [
        "You will notice that it’s challenging to choose a value of threshold that gives both  good precision and recall values. If the threshold is too high, you have a smaller  number of true positives and a high number of false negatives. This decreases your  recall; however, your precision score will be high. If you reduce the threshold too  low, false positives will increase a lot, and precision will be less. \n",
        "\n",
        "Both precision and recall range from 0 to 1 and a value closer to 1 is better.  F1 score is a metric that combines both precision and recall. It is defined as a simple  weighted average (harmonic mean) of precision and recall. If we denote precision  using P and recall using R, we can represent the F1 score as: \n",
        "\n",
        " F1 = 2PR/ (P + R)  \n",
        " \n",
        " A little bit of mathematics will lead you to the following equation of F1 based on  TP, FP and FN  \n",
        " \n",
        " F1 = 2TP/ (2TP + FP + FN) \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KAHjZnk3Vp9U",
        "colab_type": "text"
      },
      "source": [
        "Instead of looking at precision and recall individually, you can also just look at F1  score. Same as for precision, recall and accuracy, F1 score also ranges from 0 to 1,  and a perfect prediction model has an F1 of 1. When dealing with datasets that have  skewed targets, we should look at F1 (or precision and recall) instead of accuracy.  Then there are other crucial terms that we should know about.  The first one is TPR or True Positive Rate, which is the same as recall.  \n",
        "TPR = TP/ (TP + FN) \n",
        "\n",
        "TPR or recall is also known as sensitivity. \n",
        "\n",
        "And FPR or False Positive Rate, which is defined as:  \n",
        "\n",
        "FPR = FP/ (TN + FP) \n",
        "\n",
        "\n",
        " "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mvs4aRobV9-E",
        "colab_type": "text"
      },
      "source": [
        "Let’s assume that we have only 15 samples and their target values are binary:  \n",
        "\n",
        "Actual targets: [0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1]  We train a model like the random forest, and we can get the probability of when a  sample is positive.  \n",
        "Predicted probabilities for 1: [0.1, 0.3, 0.2, 0.6, 0.8, 0.05, 0.9, 0.5, 0.3, 0.66, 0.3,  0.2, 0.85, 0.15, 0.99]  \n",
        "\n",
        "For a typical threshold of >= 0.5, we can evaluate all the above values of precision,  recall/TPR, F1 and FPR. But we can do the same if we choose the value of the  threshold to be 0.4 or 0.6. In fact, we can choose any value between 0 and 1 and  calculate all the metrics described above. \n",
        "\n",
        " Let’s calculate only two values, though: TPR and FPR. \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iyKZbnh4WNC3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tpr_list = []  \n",
        "fpr_list = []  \n",
        "\n",
        "Thakur, Abhishek. Approaching (Almost) Any Machine Learning Problem (p. 42). Abhishek Thakur. Kindle Edition. \n",
        "#actual targets  \n",
        "y_true = [0, 0, 0, 0, 1, 0, 1,  0, 0, 1, 0, 1, 0, 0, 1]  \n",
        "#predicted probabilities of a sample being 1  \n",
        "y_pred = [0.1, 0.3, 0.2, 0.6, 0.8, 0.05,  0.9, 0.5, 0.3, 0.66, 0.3, 0.2,  0.85, 0.15, 0.99]  \n",
        "#handmade thresholds  \n",
        "thresholds = [0, 0.1, 0.2, 0.3, 0.4, 0.5,  0.6, 0.7, 0.8, 0.85, 0.9, 0.99, 1.0]  \n",
        "#loop over all thresholds  \n",
        "for thresh in thresholds:  \n",
        "  #calculate predictions for a given threshold  \n",
        "  temp_pred = [1 if x >= thresh else 0 for x in y_pred]  \n",
        "  #calculate tpr  \n",
        "  temp_tpr = tpr(y_true, temp_pred)  \n",
        "  #calculate fpr  \n",
        "  temp_fpr = fpr(y_true, temp_pred)  \n",
        "  #append tpr and fpr to lists  \n",
        "  tpr_list.append(temp_tpr)  \n",
        "  fpr_list.append(temp_fpr)   \n",
        "\n",
        "\n",
        "plt.figure(figsize=(7, 7))  \n",
        "plt.fill_between(fpr_list, tpr_list, alpha=0.4)  \n",
        "plt.plot(fpr_list, tpr_list, lw=3)  \n",
        "plt.xlim(0, 1.0)  \n",
        "plt.ylim(0, 1.0)  \n",
        "plt.xlabel('FPR', fontsize=15)  \n",
        "plt.ylabel('TPR', fontsize=15)  \n",
        "plt.show() "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0J6rpmXIV98q",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oxrxGUCvVp8E",
        "colab_type": "text"
      },
      "source": [
        "This curve is also known as the Receiver Operating Characteristic (ROC). And  if we calculate the area under this ROC curve, we are calculating another metric  which is used very often when you have a dataset which has skewed binary targets.  This metric is known as the Area Under ROC Curve or Area Under Curve or  just simply AUC.\n",
        " "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "teLPwh_bVWVd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "c09808f5-7253-45db-adb3-0b7eb7d1133c"
      },
      "source": [
        "from sklearn import metrics \n",
        "y_true = [0, 0, 0, 0, 1, 0, 1,0, 0, 1, 0, 1, 0, 0, 1]\n",
        "y_pred = [0.1, 0.3, 0.2, 0.6, 0.8, 0.05, 0.9, 0.5, 0.3, 0.66, 0.3, 0.2, 0.85, 0.15, 0.99]  \n",
        "metrics.roc_auc_score(y_true, y_pred)   \n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8300000000000001"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JWoruCCcSeWC",
        "colab_type": "text"
      },
      "source": [
        "AUC values range from 0 to 1. \n",
        "\n",
        " - AUC = 1 implies you have a perfect model. Most of the time, it means that  you made some mistake with validation and should revisit data processing  and validation pipeline of yours. If you didn’t make any mistakes, then  congratulations, you have the best model one can have for the dataset you  built it on.  \n",
        " \n",
        " - AUC = 0 implies that your model is very bad (or very good!). Try inverting  the probabilities for the predictions, for example, if your probability for the  positive class is p, try substituting it with 1-p. This kind of AUC may also  mean that there is some problem with your validation or data processing.  \n",
        " \n",
        " - AUC = 0.5 implies that your predictions are random. So, for any binary  classification problem, if I predict all targets as 0.5, I will get an AUC of  0.5.  AUC values between 0 and 0.5 imply that your model is worse than random. Most  of the time, it’s because you inverted the classes. \n",
        " \n",
        " \n",
        " If you try to invert your  predictions, your AUC might become more than 0.5. AUC values closer to 1 are  considered good.  But what does AUC say about our model?  Suppose you get an AUC of 0.85 when you build a model to detect pneumothorax  from chest x-ray images. This means that if you select a random image from your  dataset with pneumothorax (positive sample) and another random image without  pneumothorax (negative sample), then the pneumothorax image will rank higher  than a non-pneumothorax image with a probability of 0.85. \n",
        "\n",
        " "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3iePnfcZSdbP",
        "colab_type": "text"
      },
      "source": [
        "After calculating probabilities and AUC, you would want to make predictions on  the test set. Depending on the problem and use-case, you might want to either have  probabilities or actual classes. If you want to have probabilities, it’s effortless. You  already have them. If you want to have classes, you need to select a threshold. In  the case of binary classification, you can do something like the following.  \n",
        "\n",
        "Prediction = Probability >= Threshold  \n",
        "\n",
        "Which means, that prediction is a new list which contains only binary variables. An  item in prediction is 1 if the probability is greater than or equal to a given threshold  else the value is 0.  And guess what, you can use the ROC curve to choose this threshold! \n",
        "\n",
        "The ROC  curve will tell you how the threshold impacts false positive rate and true positive  rate and thus, in turn, false positives and true positives. You should choose the  threshold that is best suited for your problem and datasets.  For example, if you don’t want to have too many false positives, you should have a  high threshold value. \n",
        "\n",
        "This will, however, also give you a lot more false negatives.  Observe the trade-off and select the best threshold. Let’s see how these thresholds  impact true positive and false positive values.  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wOIRYVUWY6jC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def true_positive(y_true, y_pred): \n",
        "  \"\"\"  Function to calculate True Positives  \n",
        "  :param y_true: list of true values  \n",
        "  :param y_pred: list of predicted values \n",
        "   :return: number of true positives  \"\"\"  \n",
        "  tp=0\n",
        "   #initialize\n",
        "  for yt, yp in zip(y_true, y_pred):\n",
        "    if yt == 1 and yp == 1:  \n",
        "       tp += 1  \n",
        "  return tp  \n",
        "def true_negative(y_true, y_pred):  \n",
        "  \"\"\"  Function to calculate True Negatives  :param y_true: list of true values  :param y_pred: list of predicted values  :return: number of true negatives  \"\"\"\n",
        "    #initialize  \n",
        "  tn = 0  \n",
        "  for yt, yp in zip(y_true, y_pred):  \n",
        "    if yt == 0 and yp == 0:  \n",
        "        tn += 1  \n",
        "  return tn  \n",
        "def false_positive(y_true, y_pred):  \n",
        "  \"\"\"  Function to calculate False Positives  :param y_true: list of true values  :param y_pred: list of predicted values  :return: number of false positives  \"\"\"  \n",
        "  #initialize \n",
        "  fp = 0  \n",
        "  for yt, yp in zip(y_true, y_pred):  \n",
        "    if yt == 0 and yp == 1:  \n",
        "      fp += 1 \n",
        "  return fp  \n",
        "def false_negative(y_true, y_pred):  \n",
        "  \"\"\"  Function to calculate False Negatives  :param y_true: list of true values  :param y_pred: list of predicted values  :return: number of false negatives  \"\"\"  \n",
        "  #initialize  \n",
        "  fn = 0  \n",
        "  for yt, yp in zip(y_true, y_pred):  \n",
        "    if yt == 1 and yp == 0:  \n",
        "      fn += 1 \n",
        "  return fn "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q5b9rKgWOcV9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#empty lists to store true positive  \n",
        "#and false positive values  \n",
        "tp_list = []  \n",
        "fp_list = []  \n",
        "#actual targets  \n",
        "y_true = [0, 0, 0, 0, 1, 0, 1,  0, 0, 1, 0, 1, 0, 0, 1]  \n",
        "#predicted probabilities of a sample being 1  \n",
        "y_pred = [0.1, 0.3, 0.2, 0.6, 0.8, 0.05,  0.9, 0.5, 0.3, 0.66, 0.3, 0.2,  0.85, 0.15, 0.99]  \n",
        "#some handmade thresholds  \n",
        "thresholds = [0, 0.1, 0.2, 0.3, 0.4, 0.5,  0.6, 0.7, 0.8, 0.85, 0.9, 0.99, 1.0]  \n",
        "#loop over all thresholds  \n",
        "for thresh in thresholds: \n",
        "  #calculate predictions for a given threshold  \n",
        "  temp_pred = [1 if x >= thresh else 0 for x in y_pred]  \n",
        "  #calculate tp  \n",
        "  temp_tp = true_positive(y_true, temp_pred)  \n",
        "  #calculate fp  \n",
        "  temp_fp = false_positive(y_true, temp_pred)  \n",
        "  #append tp and fp to lists  \n",
        "  tp_list.append(temp_tp)  \n",
        "  fp_list.append(temp_fp)  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W-ThGHNiaZ7P",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6M7dl03kaZ_z",
        "colab_type": "text"
      },
      "source": [
        "AUC is a widely used metric for skewed binary classification tasks in the industry,  and a metric everyone should know about. Once you understand the idea behind  AUC, as explained in the paragraphs above, it is also easy to explain it to nontechnical people who would probably be assessing your models in the industry.  Another important metric you should learn after learning AUC is log loss. In case  of a binary classification problem, we define log loss as:  \n",
        "\n",
        "Log Loss = - 1.0 * (target * log(prediction) + (1 - target) * log(1 - prediction))  \n",
        "Where target is either 0 or 1 and prediction is a probability of a sample belonging  to class 1.  \n",
        "\n",
        "For multiple samples in the dataset, the log-loss over all samples is a mere average  of all individual log losses. One thing to remember is that log loss penalizes quite  high for an incorrect or a far-off prediction, i.e. log loss punishes you for being very  sure and very wrong.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SzmpnyJ5aZ6J",
        "colab_type": "text"
      },
      "source": [
        "Implementation of log loss is easy.  Interpretation may seem a bit difficult. You must remember that log loss penalizes  a lot more than other metrics.  For example, \n",
        "if you are 51% sure about a sample belonging to class 1, log loss  would be:  - \n",
        "\n",
        "1.0 * (1 * log(0.51) + (1 - 1) * log(1 – 0.51)) = 0.67  \n",
        "\n",
        "And if you are 49% sure for a sample belonging to class 0, log loss would be:  \n",
        "\n",
        "- 1.0 * (0 * log(0.49) + (1 - 0) * log(1 – 0.49)) = 0.67  \n",
        "\n",
        "So, even though we can choose a cut off at 0.5 and get perfect predictions, we will  still have a very high log loss. So, when dealing with log loss, you need to be very  careful; any non-confident prediction will have a very high log loss. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m36yIoVbbIMs",
        "colab_type": "text"
      },
      "source": [
        "multi-class classification problem.  \n",
        "\n",
        "There are three different ways to calculate this which might get confusing from time  to time. \n",
        "Let’s assume we are interested in precision first. We know that precision  depends on true positives and false positives. \n",
        "\n",
        " - Macro averaged precision: calculate precision for all classes individually  and then average them  \n",
        " \n",
        " - Micro averaged precision: calculate class wise true positive and false  positive and then use that to calculate overall precision \n",
        " \n",
        "  - Weighted precision: same as macro but in this case, it is weighted average  depending on the number of items in each class \n",
        "\n",
        "  WEighted F1:\n",
        "\n",
        "Thus, we have precision, recall and F1 implemented for multi-class problems. You  can similarly convert AUC and log loss to multi-class formats too. This format of  conversion is known as one-vs-all.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TmAVzhgFcc2A",
        "colab_type": "text"
      },
      "source": [
        "Mutlilabel classification.\n",
        " In multi-label classification, each sample can have one or more  classes associated with it. One simple example of this type of problem would be a  task in which you are asked to predict different objects in a given image. \n",
        "\n",
        "In am imagw, we can  have a chair, flower-pot, window, but we don’t have other objects such as computer,  bed, tv, etc. So, one image can have multiple targets associated with it. This type of  problem is the multi-label classification problem. \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bn1ikSTQc-iW",
        "colab_type": "text"
      },
      "source": [
        "precision at k or P@k. \n",
        "\n",
        "One must not confuse this precision with  the precision discussed earlier. If you have a list of original classes for a given  sample and list of predicted classes for the same, precision is defined as the number  of hits in the predicted list considering only top-k predictions, divided by k. \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ecJzfkxXdHsg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def pk(y_true, y_pred, k): \n",
        "   \"\"\"  This function calculates precision at k  for a single sample  \n",
        "   :param y_true: list of values, actual classes  :param y_pred: list of values, predicted classes \n",
        "    :param k: the value for k  :return: precision at a given value k  \"\"\"  \n",
        "    #if k is 0, return 0. we should never have this \n",
        "     #as k is always >= 1  \n",
        "     if k == 0:  \n",
        "       return 0  \n",
        "    #we are interested only in top-k predictions  \n",
        "    y_pred = y_pred[:k]  \n",
        "    #convert predictions to set  \n",
        "    pred_set = set(y_pred)  \n",
        "    #convert actual values to set \n",
        "    true_set = set(y_true)  \n",
        "    #find common values  \n",
        "    common_values = pred_set.intersection(true_set)  \n",
        "    #return length of common values over k  \n",
        "    return len(common_values)/ len(y_pred[:k]) \n",
        "\n",
        "    \n",
        "\n",
        "Thakur, Abhishek. Approaching (Almost) Any Machine Learning Problem (p. 61). Abhishek Thakur. Kindle Edition. "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GNFBSk31dGMb",
        "colab_type": "text"
      },
      "source": [
        "Regression\n",
        "Absolute Error = Abs (True Value – Predicted Value)\n",
        "Squared Error = (True Value – Predicted Value )2 \n",
        "\n",
        "Another type of error in same class is squared logarithmic error. Some people  call it SLE, and when we take mean of this error across all samples, it is known as  MSLE (mean squared logarithmic error)\n",
        "\n",
        "\n",
        "Root mean squared logarithmic error is just a square root of this. It is also known  as RMSLE.  \n",
        "Then we have the percentage error:  \n",
        "Percentage Error = ((True Value – Predicted Value)/ True Value) * 100 \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OyfBmCH4easc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def mean_squared_log_error(y_true, y_pred): \n",
        "  #initialize error at 0  \n",
        "  error = 0  \n",
        "  #loop over all samples in true and predicted list  \n",
        "  for yt, yp in zip(y_true, y_pred):  \n",
        "    #calculate squared log error  \n",
        "    #and add to error  \n",
        "    error += (np.log(1 + yt) - np.log(1 + yp)) ** 2  \n",
        "    #return mean error  \n",
        "  return error/ len(y_true)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ty38uDEc-mG",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d3aLx2PRc-gg",
        "colab_type": "text"
      },
      "source": [
        "quadratic weighted kappa, also known  as QWK. It is also known as Cohen’s kappa. QWK measures the “agreement”  between two “ratings”. The ratings can be any real numbers in 0 to N. And  predictions are also in the same range. An agreement can be defined as how close  these ratings are to each other. So, it’s suitable for a classification problem with N  different categories/classes. If the agreement is high, the score is closer towards 1.0.  In the case of low agreement, the score is close to 0.\n",
        "\n",
        "\n",
        "An important metric is Matthew’s Correlation Coefficient (MCC). MCC ranges  from -1 to 1. 1 is perfect prediction, -1 is imperfect prediction, and 0 is random  prediction. \n",
        "\n",
        "The formula for MCC is quite simple. \n",
        "  \n",
        "   MCC = TP * TN - FP * FN/ [(TP + FP) * (FN + TN) * (FP + TN) * (TP + FN)] ^ (0.5) \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O0B8W6GjymZd",
        "colab_type": "text"
      },
      "source": [
        "While using Labelencoder use fillna from pandas. The reason is LabelEncoder from scikitlearn does not handle NaN values.\n",
        "\n",
        "Index Feature_0 Feature_1 Feature_2  \n",
        "0         0          0         1  \n",
        "1         1          0         0  \n",
        "2         1          0         1 \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jv_Ng6QTaZFL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "a3050e6e-44b7-44ae-a337-ec78d2483d53"
      },
      "source": [
        "\n",
        "import numpy as np  \n",
        "#create our example feature matrix  \n",
        "example = np.array(  [  [0, 0, 1],  [1, 0, 0],  [1, 0, 1]  ]  )  \n",
        "#print size in bytes  \n",
        "print(example.nbytes)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "72\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YbTZo0Dh0ilv",
        "colab_type": "text"
      },
      "source": [
        "This code will print 72 as we calculated before. But do we need to store all the  elements of this matrix? No. As mentioned before we are only interested in 1s. 0s  are not that important because anything multiplied with 0 will be zero and 0  added/subtracted to/from anything doesn’t make any difference. One way to  represent this matrix only with ones would be some kind of dictionary method in  which keys are indices of rows and columns and value is 1: \n",
        "\n",
        "(0, 2) 1  \n",
        "(1, 0) 1  \n",
        "(2, 0) 1  \n",
        "(2, 2) 1  \n",
        "\n",
        "A notation like this will occupy much less memory because it has to store only four  values (in this case). The total memory used will be 8x4 = 32 bytes. Any numpy  array can be converted to a sparse matrix by simple python code.  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CDmWMS4JaZI-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "199de712-7f57-44e2-9fca-56e1157b6c9c"
      },
      "source": [
        "import numpy as np  \n",
        "from scipy import sparse  \n",
        "#create our example feature matrix  \n",
        "example = np.array(  [  [0, 0, 1],  [1, 0, 0],  [1, 0, 1]  ]  )  \n",
        "#convert numpy array to sparse CSR matrix  \n",
        "sparse_example = sparse.csr_matrix(example)  \n",
        "#print size of this sparse matrix  \n",
        "print(sparse_example.data.nbytes)\n",
        "      \n",
        "#This will print 32, which is so less than our dense array! The total size of the sparse  csr matrix is the sum of three values. \n",
        "print(sparse_example.data.nbytes + \n",
        "      sparse_example.indptr.nbytes +  \n",
        "      sparse_example.indices.nbytes)  "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "32\n",
            "64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_7sdpHa61kjH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        },
        "outputId": "46f65e26-c4b7-4768-eee5-a7d097cb9f97"
      },
      "source": [
        "#number of rows  \n",
        "n_rows = 10000  \n",
        "#number of columns  \n",
        "n_cols = 100000  \n",
        "#create random binary matrix with only 5% values as 1s  \n",
        "example = np.random.binomial(1, p=0.05, size=(n_rows, n_cols))  \n",
        "#print size in bytes  \n",
        "print(f\"Size of dense array: {example.nbytes}\")  \n",
        "#convert numpy array to sparse CSR matrix  \n",
        "sparse_example = sparse.csr_matrix(example)  \n",
        "#print size of this sparse matrix  \n",
        "print(f\"Size of sparse array: {sparse_example.data.nbytes}\")  \n",
        "\n",
        "full_size = (  sparse_example.data.nbytes +  sparse_example.indptr.nbytes +  sparse_example.indices.nbytes  )  \n",
        "#print full size of this sparse matrix  \n",
        "print(f\"Full size of sparse array: {full_size}\") \n",
        "\n",
        "#So, dense array takes ~8000MB or approximately 8GB of memory. The sparse  array, on the other hand, takes only 399MB of memory.  \n",
        "#And, that’s why we prefer sparse arrays over dense whenever we have a lot of zeros  in our features. \n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Size of dense array: 8000000000\n",
            "Size of sparse array: 400037256\n",
            "Full size of sparse array: 600095888\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NKl_YGNA2mWT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        },
        "outputId": "a25e27ec-fd3c-472d-83d9-a3a042e41895"
      },
      "source": [
        "#ONe HOt Encoding\n",
        "import numpy as np  \n",
        "from sklearn import preprocessing  \n",
        "#create random 1-d array with 1001 different categories (int)  \n",
        "example = np.random.randint(1000, size=1000000)  \n",
        "#initialize OneHotEncoder from scikit-learn  #keep sparse = False to get dense array  \n",
        "ohe = preprocessing.OneHotEncoder(sparse=False)  \n",
        "#fit and transform data with dense one hot encoder  \n",
        "ohe_example = ohe.fit_transform(example.reshape(-1, 1))  \n",
        "#print size in bytes for dense array  \n",
        "print(f\"Size of dense array: {ohe_example.nbytes}\")  \n",
        "#initialize OneHotEncoder from scikit-learn  #keep sparse = True to get sparse array  \n",
        "ohe = preprocessing.OneHotEncoder(sparse=True)  \n",
        "#fit and transform data with sparse one-hot encoder  \n",
        "ohe_example = ohe.fit_transform(example.reshape(-1, 1))  \n",
        "#print size of this sparse matrix  \n",
        "print(f\"Size of sparse array: {ohe_example.data.nbytes}\")  \n",
        "full_size = (  ohe_example.data.nbytes +  ohe_example.indptr.nbytes + ohe_example.indices.nbytes  )  \n",
        "#print full size of this sparse matrix  \n",
        "print(f\"Full size of sparse array: {full_size}\") \n",
        "\n",
        "\n",
        "#Dense array size here is approximately 8GB and sparse array is 8MB.\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Size of dense array: 8000000000\n",
            "Size of sparse array: 8000000\n",
            "Full size of sparse array: 16000004\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LT5F1tFY3aj5",
        "colab_type": "text"
      },
      "source": [
        "One more trick is to create new features from these categorical variables. You can  create new categorical features from existing features, and this can be done in an  effortless manner.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ox-yCUXt2mVM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df[\"new_feature\"] = (  df.ord_1.astype(str)  + \"_\"  + df.ord_2.astype(str))\n",
        "df.new_feature \n",
        "\n",
        "#OR\n",
        "\n",
        "df[\"new_feature\"] = (  df.ord_1.astype(str)  + \"_\"  + df.ord_2.astype(str)+\"_\"  + df.ord_3.astype(str))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kpwrue7930Va",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df.ord_2.fillna(\"NONE\").value_counts() \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wQflleWo5bSC",
        "colab_type": "text"
      },
      "source": [
        "If you have a fixed test set, you can add your test data to training to know about the  categories in a given feature. This is very similar to semi-supervised learning in  which you use data which is not available for training to improve your model. This  will also take care of rare values that appear very less number of times in training  data but are in abundance in test data. Your model will be more robust.  Many people think that this idea overfits. It may or may not overfit. There is a  simple fix for that. If you design your cross-validation in such a way that it  replicates the prediction process when you run your model on test data, then it’s  never going to overfit.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZlCFgb4y7PVT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd  \n",
        "from sklearn import preprocessing \n",
        " #read training data  \n",
        " train = pd.read_csv(\"../input/cat_train.csv\")  \n",
        " #read test data  \n",
        " test = pd.read_csv(\"../input/cat_test.csv\")  \n",
        " #create a fake target column for test data  \n",
        " #since this column doesn't exist  \n",
        " test.loc[:, \"target\"] = -1  \n",
        " #concatenate both training and test data  \n",
        " data = pd.concat([train, test]).reset_index(drop=True)  \n",
        " #make a list of features we are interested in  #id and target is something we should not encode  \n",
        " features = [x for x in train.columns if x not in [\"id\", \"target\"]]  \n",
        " #loop over the features list  for feat in features:  \n",
        " #create a new instance of LabelEncoder for each feature  \n",
        " lbl_enc = preprocessing.LabelEncoder()  \n",
        " #note the trick here  #since its categorical data, we fillna with a string  #and we convert all the data to string type  \n",
        " #so, no matter its int or float, its converted to string  #int/float but categorical!!!  \n",
        " temp_col = data[feat].fillna(\"NONE\").astype(str).values  \n",
        " #we can use fit_transform here as we do not  \n",
        " #have any extra test data that we need to  \n",
        " #transform on separately  \n",
        " data.loc[:, feat] = lbl_enc.fit_transform(temp_col) \n",
        " #split the training and test data again  \n",
        " train = data[data.target != -1].reset_index(drop=True)  \n",
        " test = data[data.target == -1].reset_index(drop=True) \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BzhstdCI7n6q",
        "colab_type": "text"
      },
      "source": [
        "This trick works when you have a problem where you already have the test dataset.  It must be noted that this trick will not work in a live setting. For example, let’s say  you are in a company that builds a real-time bidding solution (RTB). RTB systems  bid on every user they see online to buy ad space. The features that can be used for  such a model may include pages viewed in a website.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xGvCTE9970oh",
        "colab_type": "text"
      },
      "source": [
        "Let us say We see that some values appear only a couple thousand times, and some appear  almost 40000 times. NaNs are also seen a lot. Please note that I have removed some  values from the output.  We can now define our criteria for calling a value “rare”. Let’s say the requirement  for a value being rare in this column is a count of less than 2000. So, it seems, J and  L can be marked as rare values. With pandas, it is quite easy to replace categories  based on count threshold. Let’s take a look at how it’s done. \n",
        "\n",
        "We say that wherever the value count for a certain category is less than 2000,  replace it with rare. So, now, when it comes to test data, all the new, unseen  categories will be mapped to “RARE”, and all missing values will be mapped to  “NONE”.  This approach will also ensure that the model works in a live setting, even if you  have new categories.  Now we have everything we need to approach any kind of problem with categorical  variables in it. Let’s try building our first model and try to improve its performance  in a step-wise manner. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aLKtU63b8JvK",
        "colab_type": "text"
      },
      "source": [
        "Before going to any kind of model building, it’s essential to take care of crossvalidation. We have already seen the label/target distribution, and we know that it  is a binary classification problem with skewed targets. Thus, we will be using  StratifiedKFold to split the data here. \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_SmmgtQF8Ykt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#create_folds.py  \n",
        "#import pandas and model_selection module of scikit-learn  \n",
        "import pandas as pd  \n",
        "from sklearn import model_selection  \n",
        "if  __name__  ==  \"__main__\":  \n",
        "  #Read training data  \n",
        "  df = pd.read_csv(\"../input/cat_train.csv\")  \n",
        "  #we create a new column called kfold and fill it with -1  \n",
        "  df[\"kfold\"] = -1  \n",
        "  #the next step is to randomize the rows of the data  \n",
        "  df = df.sample(frac=1).reset_index(drop=True)  \n",
        "  #fetch labels  \n",
        "  y = df.target.values  \n",
        "  #initiate the kfold class from model_selection module  \n",
        "  kf = model_selection.StratifiedKFold(n_splits=5)  \n",
        "  #fill the new kfold column  \n",
        "  for f, (t_, v_) in enumerate(kf.split(X=df, y=y)):  \n",
        "    df.loc[v_, 'kfold'] = f  \n",
        "    #save the new csv with kfold column  \n",
        "  df.to_csv(\"../input/cat_train_folds.csv\", index=False) \n",
        "\n",
        "df = pd.read_csv(\"../input/cat_train_folds.csv\")\n",
        "df.kfold.value_counts() \n",
        "\n",
        "df[df.kfold==0].target.value_counts()#0 97536  1 22464  \n",
        "df[df.kfold==1].target.value_counts()  \n",
        "df[df.kfold==2].target.value_counts()  \n",
        "df[df.kfold==3].target.value_counts()  \n",
        "df[df.kfold==4].target.value_counts() "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gxTE-j4q_AVH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd  \n",
        "from sklearn import linear_model  \n",
        "from sklearn import metrics  \n",
        "from sklearn import preprocessing  \n",
        "def run(fold):  \n",
        "  #load the full training data with folds  \n",
        "  df = pd.read_csv(\"../input/cat_train_folds.csv\")  \n",
        "  #all columns are features except id, target and kfold columns  \n",
        "  features = [  f for f in df.columns if f not in (\"id\", \"target\", \"kfold\")  ]  \n",
        "  #fill all NaN values with NONE  #note that I am converting all columns to \"strings\"  \n",
        "  #it doesn’t matter because all are categories  for col in features:  \n",
        "  df.loc[:, col] = df[col].astype(str).fillna(\"NONE\")  \n",
        "  #get training data using folds  \n",
        "  df_train = df[df.kfold != fold].reset_index(drop=True)  \n",
        "  #get validation data using folds  \n",
        "  df_valid = df[df.kfold == fold].reset_index(drop=True)  \n",
        "  #initialize OneHotEncoder from scikit-learn  \n",
        "  ohe = preprocessing.OneHotEncoder()  \n",
        "  #fit ohe on training + validation features  \n",
        "  full_data = pd.concat(  [df_train[features], df_valid[features]],  axis=0  )  \n",
        "  ohe.fit(full_data[features])  \n",
        "  #transform training data  \n",
        "  x_train = ohe.transform(df_train[features])  \n",
        "  #transform validation data  \n",
        "  x_valid = ohe.transform(df_valid[features]) \n",
        "  model = linear_model.LogisticRegression()\n",
        "  #fit model on training data (ohe)  \n",
        "  model.fit(x_train, df_train.target.values) \n",
        "  #predict on validation data  \n",
        "  #we need the probability values as we are calculating AUC  \n",
        "  #we will use the probability of 1s  \n",
        "  valid_preds = model.predict_proba(x_valid)[:, 1]  \n",
        "  #get roc auc score  auc = metrics.roc_auc_score(df_valid.target.values, valid_preds)  \n",
        "  #print auc  \n",
        "  print(auc)  \n",
        "  if  __name__  ==  \"__main__\":  \n",
        "    #run function for fold = 0  \n",
        "    #we can just replace this number and  #run this for any fold  \n",
        "    run(0) \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "daOG9qWf_7uC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if  __name__  ==  \"__main__\":  \n",
        "  for fold_ in range(5):  \n",
        "    run(fold_) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Nn2tbKlASrY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#USING THe same code with random forest you might bad results \n",
        "import pandas as pd  \n",
        "from sklearn import linear_model  \n",
        "from sklearn import metrics  \n",
        "from sklearn import preprocessing  \n",
        "def run(fold):  \n",
        "  #load the full training data with folds  \n",
        "  df = pd.read_csv(\"../input/cat_train_folds.csv\")  \n",
        "  #all columns are features except id, target and kfold columns  \n",
        "  features = [  f for f in df.columns if f not in (\"id\", \"target\", \"kfold\")  ]  \n",
        "  #fill all NaN values with NONE  #note that I am converting all columns to \"strings\"  \n",
        "  #it doesn’t matter because all are categories  for col in features:  \n",
        "  df.loc[:, col] = df[col].astype(str).fillna(\"NONE\")  \n",
        "  #get training data using folds  \n",
        "  df_train = df[df.kfold != fold].reset_index(drop=True)  \n",
        "  #get validation data using folds  \n",
        "  df_valid = df[df.kfold == fold].reset_index(drop=True)  \n",
        "  #initialize OneHotEncoder from scikit-learn  \n",
        "  ohe = preprocessing.OneHotEncoder()  \n",
        "  #fit ohe on training + validation features  \n",
        "  full_data = pd.concat(  [df_train[features], df_valid[features]],  axis=0  )  \n",
        "  ohe.fit(full_data[features])  \n",
        "  #transform training data  \n",
        "  x_train = ohe.transform(df_train[features])  \n",
        "  #transform validation data  \n",
        "  x_valid = ohe.transform(df_valid[features]) \n",
        "  model = ensemble.RandomForestClassifier(n_jobs=-1) \n",
        " \n",
        "  #fit model on training data (ohe)  \n",
        "  model.fit(x_train, df_train.target.values) \n",
        "  #predict on validation data  \n",
        "  #we need the probability values as we are calculating AUC  \n",
        "  #we will use the probability of 1s  \n",
        "  valid_preds = model.predict_proba(x_valid)[:, 1]  \n",
        "  #get roc auc score  auc = metrics.roc_auc_score(df_valid.target.values, valid_preds)  \n",
        "  #print auc  \n",
        "  print(auc)  \n",
        "  if  __name__  ==  \"__main__\":  \n",
        "    #run function for fold = 0  \n",
        "    #we can just replace this number and  #run this for any fold  \n",
        "    run(0) \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "whiRLDcBArDf",
        "colab_type": "text"
      },
      "source": [
        "The random forest model, without any tuning of  hyperparameters, performs a lot worse than simple logistic regression.  And this is a reason why we should always start with simple models first. A fan of  random forest would begin with it here and will ignore logistic regression model  thinking it’s a very simple model that cannot bring any value better than random  forest. That kind of person will make a huge mistake. In our implementation of  random forest, the folds take a much longer time to complete compared to logistic  regression. So, we are not only losing on AUC but also taking much longer to  complete the training. Please note that inference is also time-consuming with  random forest and it also takes much larger space.  If we want, we can also try to run random forest on sparse one-hot encoded data,  but that is going to take a lot of time. We can also try reducing the sparse one-hot  encoded matrices using singular value decomposition. This is a very common  method of extracting topics in natural language processing. \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "meIcaklxBXRb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "  \n",
        "import pandas as pd  \n",
        "from scipy import sparse \n",
        "from sklearn import decomposition  \n",
        "from sklearn import ensemble  \n",
        "from sklearn import metrics  \n",
        "from sklearn import preprocessing  \n",
        "def run(fold):  \n",
        "  #load the full training data with folds  \n",
        "  df = pd.read_csv(\"../input/cat_train_folds.csv\")  \n",
        "  #all columns are features except id, target and kfold columns  \n",
        "  features = [  f for f in df.columns if f not in (\"id\", \"target\", \"kfold\") \n",
        "  ]  \n",
        "  #fill all NaN values with NONE  \n",
        "  #note that I am converting all columns to \"strings\"  \n",
        "  #it doesnt matter because all are categories  for col in features:  \n",
        "  df.loc[:, col] = df[col].astype(str).fillna(\"NONE\")  \n",
        "  #get training data using folds  \n",
        "  df_train = df[df.kfold != fold].reset_index(drop=True)  \n",
        "  #get validation data using folds  \n",
        "  df_valid = df[df.kfold == fold].reset_index(drop=True)  \n",
        "  #initialize OneHotEncoder from scikit-learn  \n",
        "  ohe = preprocessing.OneHotEncoder()  \n",
        "  #fit ohe on training + validation features  \n",
        "  full_data = pd.concat(  [df_train[features], df_valid[features]],  axis=0  )  \n",
        "  ohe.fit(full_data[features])  \n",
        "  #transform training data  \n",
        "  x_train = ohe.transform(df_train[features])  \n",
        "  #transform validation data  \n",
        "  x_valid = ohe.transform(df_valid[features])  \n",
        "  #initialize Truncated SVD  \n",
        "  #we are reducing the data to 120 components  \n",
        "  svd = decomposition.TruncatedSVD(n_components=120)  \n",
        "  #fit svd on full sparse training data  \n",
        "  full_sparse = sparse.vstack((x_train, x_valid))  \n",
        "  svd.fit(full_sparse)  \n",
        "  #transform sparse training data  \n",
        "  x_train = svd.transform(x_train)  \n",
        "  #transform sparse validation data  \n",
        "  x_valid = svd.transform(x_valid)  \n",
        "  #initialize random forest model  \n",
        "  model = ensemble.RandomForestClassifier(n_jobs=-1)\n",
        "  #fit model on training data (ohe)  \n",
        "  model.fit(x_train, df_train.target.values)  \n",
        "  #predict on validation data  \n",
        "  #we need the probability values as we are calculating AUC  \n",
        "  #we will use the probability of 1s  \n",
        "  valid_preds = model.predict_proba(x_valid)[:, 1]  \n",
        "  #get roc auc score  \n",
        "  auc = metrics.roc_auc_score(df_valid.target.values, valid_preds)  \n",
        "  #print auc  \n",
        "  print(f\"Fold = {fold}, AUC = {auc}\")  \n",
        "  if  __name__  ==  \"__main__\":  \n",
        "    for fold_ in range(5):  \n",
        "      run(fold_) \n",
        "\n",
        "#We one-hot encode the full data and then fit TruncatedSVD from scikit-learn on  sparse matrix with training + validation data. \n",
        "#In this way, we reduce the high  dimensional sparse matrix to 120 features and then fit random forest classifier. \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TKp3L9nCCOB8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#USing XGBOOST\n",
        "\n",
        "#USING THe same code with random forest you might bad results \n",
        "import pandas as pd  \n",
        "from sklearn import linear_model  \n",
        "from sklearn import metrics  \n",
        "from sklearn import preprocessing  \n",
        "def run(fold):  \n",
        "  #load the full training data with folds  \n",
        "  df = pd.read_csv(\"../input/cat_train_folds.csv\")  \n",
        "  #all columns are features except id, target and kfold columns  \n",
        "  features = [  f for f in df.columns if f not in (\"id\", \"target\", \"kfold\")  ]  \n",
        "  #fill all NaN values with NONE  #note that I am converting all columns to \"strings\"  \n",
        "  #it doesn’t matter because all are categories  for col in features:  \n",
        "  df.loc[:, col] = df[col].astype(str).fillna(\"NONE\")  \n",
        "  #get training data using folds  \n",
        "  df_train = df[df.kfold != fold].reset_index(drop=True)  \n",
        "  #get validation data using folds  \n",
        "  df_valid = df[df.kfold == fold].reset_index(drop=True)  \n",
        "  #initialize OneHotEncoder from scikit-learn  \n",
        "  ohe = preprocessing.OneHotEncoder()  \n",
        "  #fit ohe on training + validation features  \n",
        "  full_data = pd.concat(  [df_train[features], df_valid[features]],  axis=0  )  \n",
        "  ohe.fit(full_data[features])  \n",
        "  #transform training data  \n",
        "  x_train = ohe.transform(df_train[features])  \n",
        "  #transform validation data  \n",
        "  x_valid = ohe.transform(df_valid[features]) \n",
        "  model = xgb.XGBClassifier(n_jobs=-1,  max_depth=7,  n_estimators=200  ) \n",
        " \n",
        "  #fit model on training data (ohe)  \n",
        "  model.fit(x_train, df_train.target.values) \n",
        "  #predict on validation data  \n",
        "  #we need the probability values as we are calculating AUC  \n",
        "  #we will use the probability of 1s  \n",
        "  valid_preds = model.predict_proba(x_valid)[:, 1]  \n",
        "  #get roc auc score  auc = metrics.roc_auc_score(df_valid.target.values, valid_preds)  \n",
        "  #print auc  \n",
        "  print(auc)  \n",
        "  if  __name__  ==  \"__main__\":  \n",
        "    #run function for fold = 0  \n",
        "    #we can just replace this number and  #run this for any fold  \n",
        "    run(0) \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Rt18SEDOL-k",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        },
        "outputId": "511216b2-5033-46f5-b5ab-7ab028b16dae"
      },
      "source": [
        "import pandas as pd \n",
        "df = pd.read_csv(\"/content/adult.csv\") #kaggle audit dataset\n",
        "df.income.value_counts() \n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<=50K    29336\n",
              ">50K      9127\n",
              "Name: income, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "51WTzS9hQvFj",
        "colab_type": "text"
      },
      "source": [
        "We see that there are 7841 instances with income greater than 50K USD. This is  ~24% of the total number of samples. Thus, we will keep the evaluation same as  the cat-in-the-dat dataset, i.e. AUC. Before we start modelling, for simplicity, we  will be dropping a few columns, which are numerical, namely:\n",
        "\n",
        " • fnlwgt  • age  • capital.gain  • capital.loss  • hours.per.week  \n",
        " \n",
        " Let’s try to quickly throw in one hot encoder with logistic regression and see what  happens. The first step is always making cross-validation.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mdpoJ_zdQ4PX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        },
        "outputId": "ac216d69-0271-4793-b548-9daabcaab436"
      },
      "source": [
        "#create_folds.py  \n",
        "#import pandas and model_selection module of scikit-learn  \n",
        "import pandas as pd  \n",
        "from sklearn import model_selection  \n",
        "if  __name__  ==  \"__main__\":  \n",
        "  #Read training data  \n",
        "  df = pd.read_csv(\"/content/adult.csv\")  \n",
        "  #we create a new column called kfold and fill it with -1  \n",
        "  df[\"kfold\"] = -1  \n",
        "  #the next step is to randomize the rows of the data  \n",
        "  df = df.sample(frac=1).reset_index(drop=True)  \n",
        "  #fetch labels  \n",
        "  y = df.income.values  \n",
        "  #initiate the kfold class from model_selection module  \n",
        "  kf = model_selection.StratifiedKFold(n_splits=5)  \n",
        "  #fill the new kfold column  \n",
        "  for f, (t_, v_) in enumerate(kf.split(X=df, y=y)):  \n",
        "    df.loc[v_, 'kfold'] = f  \n",
        "    #save the new csv with kfold column  \n",
        "  df.to_csv(\"/content/adult_folds.csv\", index=False) \n",
        "\n",
        "df = pd.read_csv(\"/content/adult_folds.csv\")\n",
        "df.kfold.value_counts() \n",
        "\n",
        "print(df[df.kfold==0].income.value_counts())#0 97536  1 22464  \n",
        "print(df[df.kfold==1].income.value_counts())  \n",
        "print(df[df.kfold==2].income.value_counts())  \n",
        "print(df[df.kfold==3].income.value_counts())  \n",
        "print(df[df.kfold==4].income.value_counts()) "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<=50K    7431\n",
            ">50K     2338\n",
            "Name: income, dtype: int64\n",
            "<=50K    7431\n",
            ">50K     2338\n",
            "Name: income, dtype: int64\n",
            "<=50K    7431\n",
            ">50K     2337\n",
            "Name: income, dtype: int64\n",
            "<=50K    7431\n",
            ">50K     2337\n",
            "Name: income, dtype: int64\n",
            "<=50K    7431\n",
            ">50K     2337\n",
            "Name: income, dtype: int64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UtK6-DgfSCgM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 773
        },
        "outputId": "4a87890f-a46d-4992-f7f5-a37c17b03ac7"
      },
      "source": [
        "import pandas as pd  \n",
        "from sklearn import linear_model  \n",
        "from sklearn import metrics  \n",
        "from sklearn import preprocessing \n",
        "\n",
        "def run(fold):  \n",
        "  #load the full training data with folds  \n",
        "  df = pd.read_csv(\"/content/adult_folds.csv\")  \n",
        "  #list of numerical columns  \n",
        "  num_cols = [  \"fnlwgt\",  \"age\",  'capital-gain', 'capital-loss',  'hours-per-week' ]  \n",
        "  #drop numerical columns  \n",
        "  df = df.drop(num_cols, axis=1)  \n",
        "  #map targets to 0s and 1s  \n",
        "  target_mapping = {  \"<=50K\": 0,  \">50K\": 1  }  \n",
        "  df.loc[:, \"income\"] = df.income.map(target_mapping)  \n",
        "  #all columns are features except income and kfold columns  \n",
        "  features = [  f for f in df.columns if f not in (\"kfold\", \"income\")  ]  \n",
        "  #fill all NaN values with NONE  #note that I am converting all columns to \"strings\"  \n",
        "  #it doesnt matter because all are categories  \n",
        "  for col in features:  \n",
        "    df.loc[:, col] = df[col].astype(str).fillna(\"NONE\")  \n",
        "  #get training data using folds  \n",
        "  df_train = df[df.kfold != fold].reset_index(drop=True)  \n",
        "  #get validation data using folds  \n",
        "  df_valid = df[df.kfold == fold].reset_index(drop=True)  \n",
        "  #initialize OneHotEncoder from scikit-learn  \n",
        "  ohe = preprocessing.OneHotEncoder()  \n",
        "  #fit ohe on training + validation features  \n",
        "  full_data = pd.concat(  [df_train[features], df_valid[features]],  axis=0 )\n",
        "  ohe.fit(full_data[features])  \n",
        "  #transform training data  \n",
        "  x_train = ohe.transform(df_train[features])  \n",
        "  #transform validation data  \n",
        "  x_valid = ohe.transform(df_valid[features])  \n",
        "  #initialize Logistic Regression model  \n",
        "  model = linear_model.LogisticRegression()  \n",
        "  #fit model on training data (ohe)  \n",
        "  model.fit(x_train, df_train.income.values)  \n",
        "  #predict on validation data  #we need the probability values as we are calculating AUC  \n",
        "  #we will use the probability of 1s  \n",
        "  valid_preds = model.predict_proba(x_valid)[:, 1]  \n",
        "  #get roc auc score  \n",
        "  auc = metrics.roc_auc_score(df_valid.income.values, valid_preds)  \n",
        "  #print auc  \n",
        "  print(f\"Fold = {fold}, AUC = {auc}\")  \n",
        "if  __name__  ==  \"__main__\":  \n",
        "  for fold_ in range(5):  \n",
        "    run(fold_)\n",
        "\n",
        "#TO run the python code--❯ python -W ignore yourfilename.py\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Fold = 0, AUC = 0.8715323548646406\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Fold = 1, AUC = 0.8804461266060071\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Fold = 2, AUC = 0.8783298717333687\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Fold = 3, AUC = 0.8820409498955071\n",
            "Fold = 4, AUC = 0.8825620181493445\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8PVtyYEMTW3m",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 101
        },
        "outputId": "33b829a7-30b3-4e04-e150-66a7e93dbcbb"
      },
      "source": [
        "#RUN THOROUGH XGBOOST\n",
        "import pandas as pd  \n",
        "import xgboost as xgb  \n",
        "from sklearn import metrics  \n",
        "from sklearn import preprocessing  \n",
        "def run(fold):  \n",
        "  #load the full training data with folds  \n",
        "  df = pd.read_csv(\"/content/adult_folds.csv\")  \n",
        "  #list of numerical columns  \n",
        "  num_cols = [  \"fnlwgt\",  \"age\",  'capital-gain', 'capital-loss',  'hours-per-week'  ]  \n",
        "  #drop numerical columns  \n",
        "  df = df.drop(num_cols, axis=1)  \n",
        "  #map targets to 0s and 1s  \n",
        "  target_mapping = {  \"<=50K\": 0,  \">50K\": 1  }  \n",
        "  df.loc[:, \"income\"] = df.income.map(target_mapping)  \n",
        "  #all columns are features except kfold & income columns  \n",
        "  features = [  f for f in df.columns if f not in (\"kfold\", \"income\")  ]  \n",
        "  #fill all NaN values with NONE  #note that I am converting all columns to \"strings\" \n",
        "   #it doesnt matter because all are categories  \n",
        "  for col in features:  \n",
        "    df.loc[:, col] = df[col].astype(str).fillna(\"NONE\")  \n",
        "  #now its time to label encode the features  \n",
        "  for col in features:  \n",
        "    #initialize LabelEncoder for each feature column\n",
        "    lbl = preprocessing.LabelEncoder()  \n",
        "    #fit label encoder on all data  \n",
        "    lbl.fit(df[col])  \n",
        "    #transform all the data  \n",
        "    df.loc[:, col] = lbl.transform(df[col])  \n",
        "  #get training data using folds  \n",
        "  df_train = df[df.kfold != fold].reset_index(drop=True) \n",
        "   #get validation data using folds  \n",
        "  df_valid = df[df.kfold == fold].reset_index(drop=True)  \n",
        "   #get training data  \n",
        "  x_train = df_train[features].values  \n",
        "   #get validation data  \n",
        "  x_valid = df_valid[features].values  \n",
        "   #initialize xgboost model  \n",
        "  model = xgb.XGBClassifier(  n_jobs=-1  )  \n",
        "   #fit model on training data (ohe)  \n",
        "  model.fit(x_train, df_train.income.values) \n",
        "   #predict on validation data  #we need the probability values as we are calculating AUC  \n",
        "   #we will use the probability of 1s  \n",
        "  valid_preds = model.predict_proba(x_valid)[:, 1]  \n",
        "   #get roc auc score  \n",
        "  auc = metrics.roc_auc_score(df_valid.income.values, valid_preds)  \n",
        "   #print auc  \n",
        "  print(f\"Fold = {fold}, AUC = {auc}\")  \n",
        "if  __name__  ==  \"__main__\":  \n",
        "  for fold_ in range(5):  \n",
        "    run(fold_) "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fold = 0, AUC = 0.8702282556405155\n",
            "Fold = 1, AUC = 0.8779882417528402\n",
            "Fold = 2, AUC = 0.8782399559329082\n",
            "Fold = 3, AUC = 0.881098086420169\n",
            "Fold = 4, AUC = 0.8825762123503138\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yh0geQJiWZkd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 101
        },
        "outputId": "cdb844ac-155a-465f-ecb7-7b2419f96954"
      },
      "source": [
        "#Including numerical features in the xgboost model\n",
        "\n",
        "#RUN THOROUGH XGBOOST\n",
        "import pandas as pd  \n",
        "import xgboost as xgb  \n",
        "from sklearn import metrics  \n",
        "from sklearn import preprocessing  \n",
        "def run(fold):  \n",
        "  #load the full training data with folds  \n",
        "  df = pd.read_csv(\"/content/adult_folds.csv\")  \n",
        "  #list of numerical columns  \n",
        "  num_cols = [  \"fnlwgt\",  \"age\",  'capital-gain', 'capital-loss',  'hours-per-week'  ]  \n",
        "  #map targets to 0s and 1s  \n",
        "  target_mapping = {  \"<=50K\": 0,  \">50K\": 1  }  \n",
        "  df.loc[:, \"income\"] = df.income.map(target_mapping)  \n",
        "  #all columns are features except kfold & income columns  \n",
        "  features = [  f for f in df.columns if f not in (\"kfold\", \"income\")  ]  \n",
        "  #fill all NaN values with NONE  #note that I am converting all columns to \"strings\" \n",
        "   #it doesnt matter because all are categories  \n",
        "  for col in features:\n",
        "    if col not in num_cols: \n",
        "      df.loc[:, col] = df[col].astype(str).fillna(\"NONE\")   \n",
        "  #now its time to label encode the features  \n",
        "  for col in features:\n",
        "    if col not in num_cols:\n",
        "    #initialize LabelEncoder for each feature column\n",
        "      lbl = preprocessing.LabelEncoder()  \n",
        "      #fit label encoder on all data  \n",
        "      lbl.fit(df[col])  \n",
        "      #transform all the data  \n",
        "      df.loc[:, col] = lbl.transform(df[col])  \n",
        "  #get training data using folds  \n",
        "  df_train = df[df.kfold != fold].reset_index(drop=True) \n",
        "   #get validation data using folds  \n",
        "  df_valid = df[df.kfold == fold].reset_index(drop=True)  \n",
        "   #get training data  \n",
        "  x_train = df_train[features].values  \n",
        "   #get validation data  \n",
        "  x_valid = df_valid[features].values  \n",
        "   #initialize xgboost model  \n",
        "  model = xgb.XGBClassifier(  n_jobs=-1  )  \n",
        "   #fit model on training data (ohe)  \n",
        "  model.fit(x_train, df_train.income.values) \n",
        "   #predict on validation data  #we need the probability values as we are calculating AUC  \n",
        "   #we will use the probability of 1s  \n",
        "  valid_preds = model.predict_proba(x_valid)[:, 1]  \n",
        "   #get roc auc score  \n",
        "  auc = metrics.roc_auc_score(df_valid.income.values, valid_preds)  \n",
        "   #print auc  \n",
        "  print(f\"Fold = {fold}, AUC = {auc}\")  \n",
        "if  __name__  ==  \"__main__\":  \n",
        "  for fold_ in range(5):  \n",
        "    run(fold_) \n",
        "\n",
        "#Increasing the AUC by addinf the numerical features"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fold = 0, AUC = 0.9126234525585197\n",
            "Fold = 1, AUC = 0.918790195144632\n",
            "Fold = 2, AUC = 0.9205112077468436\n",
            "Fold = 3, AUC = 0.9219918097445003\n",
            "Fold = 4, AUC = 0.9258593984065757\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0XlU2HgEYVDz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 101
        },
        "outputId": "3ba92593-c8bb-457d-bf71-c9cfd3dfe382"
      },
      "source": [
        "# feature_engineering function with hyperparametr tuning can imporve the the performance of model a bit more.\n",
        "import itertools  \n",
        "import pandas as pd  \n",
        "import xgboost as xgb  \n",
        "from sklearn import metrics  \n",
        "from sklearn import preprocessing  \n",
        "def feature_engineering(df, cat_cols):  \n",
        "  \"\"\"  This function is used for feature engineering  :param df: the pandas dataframe with train/test data  \n",
        "  :param cat_cols: list of categorical columns  :return: dataframe with new features  \"\"\"  \n",
        "  #this will create all 2-combinations of values  \n",
        "  #in this list  #for example:  \n",
        "  #list(itertools.combinations([1,2,3], 2)) will return  #[(1, 2), (1, 3), (2, 3)]  \n",
        "  combi = list(itertools.combinations(cat_cols, 2))  \n",
        "  for c1, c2 in combi:  \n",
        "    df.loc[  :,  c1 + \"_\" + c2  ] = df[c1].astype(str) + \"_\" + df[c2].astype(str)  \n",
        "  return df  \n",
        "def run(fold):  \n",
        "  #load the full training data with folds\n",
        "  df = pd.read_csv(\"/content/adult_folds.csv\")  \n",
        "  #list of numerical columns  \n",
        "  num_cols = [  \"fnlwgt\",  \"age\",  \"capital.gain\",  \"capital.loss\",  \"hours.per.week\"  ]  \n",
        "  #map targets to 0s and 1s  \n",
        "  target_mapping = {  \"<=50K\": 0,  \">50K\": 1  }  \n",
        "  df.loc[:, \"income\"] = df.income.map(target_mapping)  \n",
        "  #list of categorical columns for feature engineering  \n",
        "  cat_cols = [  c for c in df.columns if c not in num_cols  and c not in (\"kfold\", \"income\")  ]  \n",
        "  #add new features  \n",
        "  df = feature_engineering(df, cat_cols)  \n",
        "  #all columns are features except kfold & income columns  \n",
        "  features = [  f for f in df.columns if f not in (\"kfold\", \"income\")  ]  \n",
        "  #fill all NaN values with NONE  \n",
        "  #note that I am converting all columns to \"strings\"  \n",
        "  #it doesnt matter because all are categories  \n",
        "  for col in features:  \n",
        "  #do not encode the numerical columns  \n",
        "    if col not in num_cols:  \n",
        "      df.loc[:, col] = df[col].astype(str).fillna(\"NONE\")  \n",
        "  #now its time to label encode the features  \n",
        "  for col in features:  \n",
        "    if col not in num_cols:  \n",
        "      #initialize LabelEncoder for each feature column  \n",
        "      lbl = preprocessing.LabelEncoder()  \n",
        "      #fit label encoder on all data\n",
        "      lbl.fit(df[col])  \n",
        "      #transform all the data  \n",
        "      df.loc[:, col] = lbl.transform(df[col])  \n",
        "  #get training data using folds  \n",
        "  df_train = df[df.kfold != fold].reset_index(drop=True)  \n",
        "  #get validation data using folds  \n",
        "  df_valid = df[df.kfold == fold].reset_index(drop=True)  \n",
        "  #get training data  \n",
        "  x_train = df_train[features].values  \n",
        "  #get validation data  \n",
        "  x_valid = df_valid[features].values  \n",
        "  #initialize xgboost model  \n",
        "  model = xgb.XGBClassifier(  n_jobs=-1  )  \n",
        "  #fit model on training data (ohe)  \n",
        "  model.fit(x_train, df_train.income.values)  \n",
        "  #predict on validation data  \n",
        "  #we need the probability values as we are calculating AUC  \n",
        "  #we will use the probability of 1s  \n",
        "  valid_preds = model.predict_proba(x_valid)[:, 1]  \n",
        "  #get roc auc score  \n",
        "  auc = metrics.roc_auc_score(df_valid.income.values, valid_preds)  \n",
        "  #print auc  \n",
        "  print(f\"Fold = {fold}, AUC = {auc}\")  \n",
        "if  __name__  ==  \"__main__\":  \n",
        "  for fold_ in range(5):  \n",
        "    run(fold_)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fold = 0, AUC = 0.9103857571206282\n",
            "Fold = 1, AUC = 0.9162543475250318\n",
            "Fold = 2, AUC = 0.9177142879517952\n",
            "Fold = 3, AUC = 0.9187697837074412\n",
            "Fold = 4, AUC = 0.9218548486613141\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "40jS4HgNdmME",
        "colab_type": "text"
      },
      "source": [
        "###**Target  Encoding. **\n",
        "\n",
        "However, you have to be very careful here as this might overfit your  model. Target encoding is a technique in which you map each category in a given  feature to its mean target value, but this must always be done in a cross-validated  manner. It means that the first thing you do is create the folds, and then use those  folds to create target encoding features for different columns of the data in the same  way you fit and predict the model on folds. So, if you have created 5 folds, you have  to create target encoding 5 times such that in the end, you have encoding for  variables in each fold which are not derived from the same fold. And then when  you fit your model, you must use the same folds again. Target encoding for unseen  test data can be derived from the full training data or can be an average of all the 5  folds."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l6S3xJ3pdN8G",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zYds-jozYVGk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import copy  \n",
        "import pandas as pd  \n",
        "from sklearn import metrics  \n",
        "from sklearn import preprocessing  \n",
        "import xgboost as xgb  \n",
        "def mean_target_encoding(data):  \n",
        "  #make a copy of dataframe  \n",
        "  df = copy.deepcopy(data)  \n",
        "    #list of numerical columns  \n",
        "  num_cols = [  \"fnlwgt\",  \"age\",  \"capital.gain\",  \"capital.loss\",  \"hours.per.week\"  ]  \n",
        "    #map targets to 0s and 1s  \n",
        "  target_mapping = {  \"<=50K\": 0,  \">50K\": 1  }  \n",
        "  df.loc[:, \"income\"] = df.income.map(target_mapping)  \n",
        "    #all columns are features except income and kfold columns  \n",
        "  features = [  f for f in df.columns if f not in (\"kfold\", \"income\")  and f not in num_cols  ]  \n",
        "    #fill all NaN values with NONE  #note that I am converting all columns to \"strings\"  \n",
        "    #it doesnt matter because all are categories  \n",
        "  for col in features:  \n",
        "    #do not encode the numerical columns  \n",
        "    if col not in num_cols: \n",
        "      df.loc[:, col] = df[col].astype(str).fillna(\"NONE\")  \n",
        "        #now its time to label encode the features  \n",
        "  for col in features:  \n",
        "    if col not in num_cols:  \n",
        "        #initialize LabelEncoder for each feature column  \n",
        "      lbl = preprocessing.LabelEncoder()  \n",
        "        #fit label encoder on all data  \n",
        "      lbl.fit(df[col])  \n",
        "        #transform all the data  \n",
        "      df.loc[:, col] = lbl.transform(df[col])  \n",
        "        #a list to store 5 validation dataframes  \n",
        "      encoded_dfs = []  \n",
        "        #go over all folds  \n",
        "  for fold in range(5):  \n",
        "          #fetch training and validation data  \n",
        "    df_train = df[df.kfold != fold].reset_index(drop=True)  \n",
        "    df_valid = df[df.kfold == fold].reset_index(drop=True)  \n",
        "          #for all feature columns, i.e. categorical columns  \n",
        "    for column in features:  \n",
        "            #create dict of category:mean target  \n",
        "      mapping_dict = dict(  df_train.groupby(column)[\"income\"].mean()  )  \n",
        "            #column_enc is the new column we have with mean encoding  \n",
        "      df_valid.loc[  :, column + \"_enc\"  ] = df_valid[column].map(mapping_dict)  \n",
        "            #append to our list of encoded validation dataframes  \n",
        "      encoded_dfs.append(df_valid)  \n",
        "            #create full data frame again and return  \n",
        "  encoded_df = pd.concat(encoded_dfs, axis=0)  \n",
        "  return encoded_df  \n",
        "     "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0PYuf6MmgLvR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 101
        },
        "outputId": "3ee78e4b-0729-4667-9ec0-a48b7a3e164c"
      },
      "source": [
        " def run(df, fold):  \n",
        "        #note that folds are same as before  \n",
        "        #get training data using folds  \n",
        "        df_train = df[df.kfold != fold].reset_index(drop=True)  \n",
        "        #get validation data using folds  \n",
        "        df_valid = df[df.kfold == fold].reset_index(drop=True) \n",
        "        #all columns are features except income and kfold columns  \n",
        "        features = [  f for f in df.columns if f not in (\"kfold\", \"income\")  ]  \n",
        "        #scale training data  \n",
        "        x_train = df_train[features].values \n",
        "         #scale validation data  \n",
        "        x_valid = df_valid[features].values  \n",
        "         #initialize xgboost model  \n",
        "        model = xgb.XGBClassifier(  n_jobs=-1,  max_depth=7  )  \n",
        "         #fit model on training data (ohe)  \n",
        "        model.fit(x_train, df_train.income.values)  \n",
        "         #predict on validation data  \n",
        "         #we need the probability values as we are calculating AUC  #we will use the probability of 1s  \n",
        "        valid_preds = model.predict_proba(x_valid)[:, 1]  \n",
        "         #get roc auc score  \n",
        "        auc = metrics.roc_auc_score(df_valid.income.values, valid_preds)  \n",
        "         #print auc  \n",
        "        print(f\"Fold = {fold}, AUC = {auc}\")  \n",
        "if  __name__  ==  \"__main__\":  \n",
        "  #read data  \n",
        "  df = pd.read_csv(\"/content/adult_folds.csv\")  \n",
        "  #create mean target encoded categories and  \n",
        "  #munge data  \n",
        "  df = mean_target_encoding(df)  \n",
        "  #run training and validation for 5 folds  \n",
        "  for fold_ in range(5):  \n",
        "    run(df, fold_)\n",
        " "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fold = 0, AUC = 0.9263274592748869\n",
            "Fold = 1, AUC = 0.9297613608356273\n",
            "Fold = 2, AUC = 0.9305119292614001\n",
            "Fold = 3, AUC = 0.9333911926969598\n",
            "Fold = 4, AUC = 0.9343563407799049\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ledilvAErgS3",
        "colab_type": "text"
      },
      "source": [
        "It must be noted that in the above snippet, I had not dropped categorical columns  when I did the target encoding. I kept all the features and added target encoded features on top of it. Also, I used mean. You can use mean, median, standard  deviation or any other function of targets. \n",
        "\n",
        "Nice! It seems like we have improved again. However, you must be very careful  when using target encoding as it is too prone to overfitting. When we use target  encoding, it’s better to use some kind of smoothing or adding noise in the encoded  values. Scikit-learn has contrib repository which has target encoding with  smoothing, or you can create your own smoothing. Smoothing introduces some  kind of regularization that helps with not overfitting the model. It’s not very  difficult. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VFUZTad1nRCC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def add_noise(series, noise_level):\n",
        "    return series * (1 + noise_level * np.random.randn(len(series)))\n",
        "\n",
        "def target_encode(trn_series=None, \n",
        "                  tst_series=None, \n",
        "                  target=None, \n",
        "                  min_samples_leaf=1, \n",
        "                  smoothing=1,\n",
        "                  noise_level=0):\n",
        "    \"\"\"\n",
        "    Smoothing is computed like in the following paper by Daniele Micci-Barreca\n",
        "    https://kaggle2.blob.core.windows.net/forum-message-attachments/225952/7441/high%20cardinality%20categoricals.pdf\n",
        "    trn_series : training categorical feature as a pd.Series\n",
        "    tst_series : test categorical feature as a pd.Series\n",
        "    target : target data as a pd.Series\n",
        "    min_samples_leaf (int) : minimum samples to take category average into account\n",
        "    smoothing (int) : smoothing effect to balance categorical average vs prior  \n",
        "    \"\"\" \n",
        "    assert len(trn_series) == len(target)\n",
        "    assert trn_series.name == tst_series.name\n",
        "    temp = pd.concat([trn_series, target], axis=1)\n",
        "    # Compute target mean \n",
        "    averages = temp.groupby(by=trn_series.name)[target.name].agg([\"mean\", \"count\"])\n",
        "    # Compute smoothing\n",
        "    smoothing = 1 / (1 + np.exp(-(averages[\"count\"] - min_samples_leaf) / smoothing))\n",
        "    # Apply average function to all target data\n",
        "    prior = target.mean()\n",
        "    # The bigger the count the less full_avg is taken into account\n",
        "    averages[target.name] = prior * (1 - smoothing) + averages[\"mean\"] * smoothing\n",
        "    averages.drop([\"mean\", \"count\"], axis=1, inplace=True)\n",
        "    # Apply averages to trn and tst series\n",
        "    ft_trn_series = pd.merge(\n",
        "        trn_series.to_frame(trn_series.name),\n",
        "        averages.reset_index().rename(columns={'index': target.name, target.name: 'average'}),\n",
        "        on=trn_series.name,\n",
        "        how='left')['average'].rename(trn_series.name + '_mean').fillna(prior)\n",
        "    # pd.merge does not keep the index so restore it\n",
        "    ft_trn_series.index = trn_series.index \n",
        "    ft_tst_series = pd.merge(\n",
        "        tst_series.to_frame(tst_series.name),\n",
        "        averages.reset_index().rename(columns={'index': target.name, target.name: 'average'}),\n",
        "        on=tst_series.name,\n",
        "        how='left')['average'].rename(trn_series.name + '_mean').fillna(prior)\n",
        "    # pd.merge does not keep the index so restore it\n",
        "    ft_tst_series.index = tst_series.index\n",
        "    return add_noise(ft_trn_series, noise_level), add_noise(ft_tst_series, noise_level)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j2B_Dn9OnkHU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "\n",
        "folds = StratifiedKFold(n_splits=5, shuffle=True, random_state=0)\n",
        "f_cats = [f for f in trn_df.columns if \"_cat\" in f]\n",
        "print(\"%20s   %20s | %20s\" % (\"\", \"Raw Categories\", \"Encoded Categories\"))\n",
        "for f in f_cats:\n",
        "    print(\"%-20s : \" % f, end=\"\")\n",
        "    e_scores = []\n",
        "    f_scores = []\n",
        "    for trn_idx, val_idx in folds.split(trn_df.values, trn_df.target.values):\n",
        "        trn_f, trn_tgt = trn_df[f].iloc[trn_idx], trn_df.target.iloc[trn_idx]\n",
        "        val_f, val_tgt = trn_df[f].iloc[trn_idx], trn_df.target.iloc[trn_idx]\n",
        "        trn_tf, val_tf = target_encode(trn_series=trn_f, \n",
        "                                       tst_series=val_f, \n",
        "                                       target=trn_tgt, \n",
        "                                       min_samples_leaf=100, \n",
        "                                       smoothing=20,\n",
        "                                       noise_level=0.01)\n",
        "        f_scores.append(max(roc_auc_score(val_tgt, val_f), 1 - roc_auc_score(val_tgt, val_f)))\n",
        "        e_scores.append(roc_auc_score(val_tgt, val_tf))\n",
        "    print(\" %.6f + %.6f | %6f + %.6f\" \n",
        "          % (np.mean(f_scores), np.std(f_scores), np.mean(e_scores), np.std(e_scores)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p5nee5HItypX",
        "colab_type": "text"
      },
      "source": [
        "**ENTITY EMBEDDING**\n",
        "\n",
        "In entity embeddings, the  categories are represented as vectors. We represent categories by vectors in both  binarization and one hot encoding approaches. But what if we have tens of  thousands of categories. This will create huge matrices and will take a long time for  us to train complicated models. We can thus represent them by vectors with float  values instead.  The idea is super simple. \n",
        "\n",
        "The idea is super simple. You have an embedding layer for each categorical feature.  So, every category in a column can now be mapped to an embedding (like mapping  words to embeddings in natural language processing). You then reshape these  embeddings to their dimension to make them flat and then concatenate all the  flattened inputs embeddings. Then add a bunch of dense layers, an output layer and  you are done. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1KYeo2YMn_F6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#USING TF/KERAS\n",
        "\n",
        "import os  \n",
        "import gc  \n",
        "import joblib  \n",
        "import pandas as pd  \n",
        "import numpy as np  \n",
        "from sklearn import metrics, preprocessing  \n",
        "from tensorflow.keras import layers  \n",
        "from tensorflow.keras import optimizers  \n",
        "from tensorflow.keras.models import Model, load_model  \n",
        "from tensorflow.keras import callbacks  \n",
        "from tensorflow.keras import backend as K  \n",
        "from tensorflow.keras import utils \n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U8R7BB1uuunX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_model(data, catcols):  \n",
        "  \"\"\"  This function returns a compiled tf.keras model  for entity embeddings  :param data: \n",
        "  this is a pandas dataframe  :param catcols: list of categorical column names  :return: compiled tf.keras model  \"\"\"  \n",
        "  #init list of inputs for embeddings  \n",
        "  inputs = []  \n",
        "  #init list of outputs for embeddings  \n",
        "  outputs = []  \n",
        "  #loop over all categorical columns  \n",
        "  for c in catcols:  \n",
        "    #find the number of unique values in the column  \n",
        "    num_unique_values = int(data[c].nunique())  \n",
        "    #simple dimension of embedding calculator  \n",
        "    #min size is half of the number of unique values  \n",
        "    #max size is 50. max size depends on the number of unique  #categories too. 50 is quite sufficient most of the times  \n",
        "    #but if you have millions of unique values, you might need  #a larger dimension  \n",
        "    embed_dim = int(min(np.ceil((num_unique_values)/2), 50))  \n",
        "    #simple keras input layer with size 1  \n",
        "    inp = layers.Input(shape=(1,))  \n",
        "    #add embedding layer to raw input  \n",
        "    #embedding size is always 1 more than unique values in input  \n",
        "    out = layers.Embedding(  num_unique_values + 1, embed_dim, name=c  )(inp)  \n",
        "    #1-d spatial dropout is the standard for emebedding layers  \n",
        "    #you can use it in NLP tasks too  \n",
        "    out = layers.SpatialDropout1D(0.3)(out)  \n",
        "    #reshape the input to the dimension of embedding  \n",
        "    #this becomes our output layer for current feature  \n",
        "    out = layers.Reshape(target_shape=(embed_dim, ))(out)  \n",
        "    #add input to input list  \n",
        "    inputs.append(inp)  \n",
        "    #add output to output list\n",
        "    outputs.append(out)  \n",
        "    #concatenate all output layers  \n",
        "  x = layers.Concatenate()(outputs)  \n",
        "    #add a batchnorm layer.  \n",
        "    #from here, everything is up to you  \n",
        "    #you can try different architectures  \n",
        "    #this is the architecture I like to use  \n",
        "    #if you have numerical features, you should add  \n",
        "    #them here or in concatenate layer  \n",
        "  x = layers.BatchNormalization()(x)  \n",
        "    #a bunch of dense layers with dropout.  \n",
        "    #start with 1 or two layers only  \n",
        "  x = layers.Dense(300, activation=\"relu\")(x)  \n",
        "  x = layers.Dropout(0.3)(x)  \n",
        "  x = layers.BatchNormalization()(x)  \n",
        "  x = layers.Dense(300, activation=\"relu\")(x)  \n",
        "  x = layers.Dropout(0.3)(x)  \n",
        "  x = layers.BatchNormalization()(x)  \n",
        "    #using softmax and treating it as a two class problem  \n",
        "    #you can also use sigmoid, then you need to use only one  \n",
        "    #output class  \n",
        "  y = layers.Dense(2, activation=\"softmax\")(x)  \n",
        "    #create final model  \n",
        "  model = Model(inputs=inputs, outputs=y)  \n",
        "    #compile the model  \n",
        "    #we use adam and binary cross entropy.  \n",
        "    #feel free to use something else and see how model behaves  \n",
        "  model.compile(loss='binary_crossentropy', optimizer='adam')  \n",
        "  return model  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-9YgMep4o28l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def run(fold):  \n",
        "  #load the full training data with folds  \n",
        "  df = pd.read_csv(\"../input/cat_train_folds.csv\")  \n",
        "  #all columns are features except id, target and kfold columns  \n",
        "  features = [  f for f in df.columns if f not in (\"id\", \"target\", \"kfold\")  ]\n",
        "  #fill all NaN values with NONE  #note that I am converting all columns to \"strings\"  \n",
        "  #it doesnt matter because all are categories  for col in features:  \n",
        "  df.loc[:, col] = df[col].astype(str).fillna(\"NONE\")  \n",
        "  #encode all features with label encoder individually  \n",
        "  #in a live setting you need to save all label encoders  \n",
        "  for feat in features:  \n",
        "    lbl_enc = preprocessing.LabelEncoder()  \n",
        "    df.loc[:, feat] = lbl_enc.fit_transform(df[feat].values)  \n",
        "    #get training data using folds  \n",
        "    df_train = df[df.kfold != fold].reset_index(drop=True)  \n",
        "    #get validation data using folds  \n",
        "    df_valid = df[df.kfold == fold].reset_index(drop=True)  \n",
        "    #create tf.keras model  \n",
        "    model = create_model(df, features)  \n",
        "    #our features are lists of lists  \n",
        "    xtrain = [  df_train[features].values[:, k] for k in range(len(features))  ]  \n",
        "    xvalid = [  df_valid[features].values[:, k] for k in range(len(features))  ]  \n",
        "    #fetch target columns  \n",
        "    ytrain = df_train.target.values  \n",
        "    yvalid = df_valid.target.values  \n",
        "    #convert target columns to categories  \n",
        "    #this is just binarization  \n",
        "    ytrain_cat = utils.to_categorical(ytrain)  \n",
        "    yvalid_cat = utils.to_categorical(yvalid)  \n",
        "    #fit the model  \n",
        "    model.fit(xtrain,  ytrain_cat,  validation_data=(xvalid, yvalid_cat),  verbose=1,  batch_size=1024,  epochs=3  )  \n",
        "    #generate validation predictions \n",
        "    valid_preds = model.predict(xvalid)[:, 1]  \n",
        "    #print roc auc score  \n",
        "    print(metrics.roc_auc_score(yvalid, valid_preds))  \n",
        "    #clear session to free up some GPU memory  \n",
        "    K.clear_session()  \n",
        "  if  __name__  ==  \"__main__\":  \n",
        "    run(0)  \n",
        "    run(1)  \n",
        "    run(2)  \n",
        "    run(3)  \n",
        "    run(4) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xJ2bJ8VUwxT4",
        "colab_type": "text"
      },
      "source": [
        "You will notice that this approach gives the best results and is also super-fast if you  have a GPU! This can also be improved further, and you don’t need to worry about  feature engineering as neural network handles it on its own. This is definitely worth  a try when dealing with a large dataset of categorical features. When embedding  size is the same as the number of unique categories, we have one-hot-encoding. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zb2sxE5txCPG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a5g2kVkwxAjj",
        "colab_type": "text"
      },
      "source": [
        "**FEATURE ENGINEEERING**\n",
        "\n",
        "Feature  engineering is not just about creating new features from data but also includes  different types of normalization and transformations. \n",
        "\n",
        "Let’s start with the most simple but most widely used feature engineering  techniques. Let’s say that you are dealing with date and time data. So, we have a  pandas dataframe with a datetime type column. Using this column, we can create  features like:  \n",
        "\n",
        "- Year \n",
        "- Week of year \n",
        "- Weekend  \n",
        "- Hour  \n",
        "- And many more. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1-WaAUlkxUih",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df.loc[:, 'year'] = df['datetime_column'].dt.year  \n",
        "df.loc[:, 'weekofyear'] = df['datetime_column'].dt.weekofyear  \n",
        "df.loc[:, 'month'] = df['datetime_column'].dt.month  \n",
        "df.loc[:, 'dayofweek'] = df['datetime_column'].dt.dayofweek\n",
        "df.loc[:, 'weekend'] = (df.datetime_column.dt.weekday >=5).astype(int)  \n",
        "df.loc[:, 'hour'] = df['datetime_column'].dt.hour  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mla2QBa4xnFW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd  \n",
        "#create a series of datetime with a frequency of 10 hours  \n",
        "s = pd.date_range('2020-01-06', '2020-01-10', freq='10H').to_series()  \n",
        "#create some features based on datetime  \n",
        "features = {  \"dayofweek\": s.dt.dayofweek.values,  \"dayofyear\": s.dt.dayofyear.values, \n",
        "            \"hour\": s.dt.hour.values,  \"is_leap_year\": s.dt.is_leap_year.values,  \"quarter\": s.dt.quarter.values, \n",
        "            \"weekofyear\": s.dt.weekofyear.values  }  \n",
        "            "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZVHFdeeRxx8M",
        "colab_type": "text"
      },
      "source": [
        "This will generate a dictionary of features from a given series. You can apply this  to any datetime column in a pandas dataframe. These are some of the many date  time features that pandas offer. Date time features are critical when you are dealing  with time-series data, for example, predicting sales of a store but would like to use  a model like xgboost on aggregated features. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1K_eP6J1yU3V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "def generate_features(df):  \n",
        "  #create a bunch of features using the date column  \n",
        "  df.loc[:, 'year'] = df['date'].dt.year  \n",
        "  df.loc[:, 'weekofyear'] = df['date'].dt.weekofyear  \n",
        "  df.loc[:, 'month'] = df['date'].dt.month  \n",
        "  df.loc[:, 'dayofweek'] = df['date'].dt.dayofweek  \n",
        "  df.loc[:, 'weekend'] = (df['date'].dt.weekday >=5).astype(int)  \n",
        "  #create an aggregate dictionary  \n",
        "  aggs = {}  \n",
        "  #for aggregation by month, we calculate the  #number of unique month values and also the mean  \n",
        "  aggs['month'] = ['nunique', 'mean']  \n",
        "  aggs['weekofyear'] = ['nunique', 'mean']  \n",
        "  #we aggregate by num1 and calculate sum, max, min  #and mean values of this column  \n",
        "  aggs['num1'] = ['sum','max','min','mean']  \n",
        "  #for customer_id, we calculate the total count  \n",
        "  aggs['customer_id'] = ['size']  \n",
        "  #again for customer_id, we calculate the total unique  \n",
        "  aggs['customer_id'] = ['nunique']  \n",
        "  #we group by customer_id and calculate the aggregates  \n",
        "  agg_df = df.groupby('customer_id').agg(aggs)  \n",
        "  agg_df = agg_df.reset_index()  \n",
        "  return agg_df \n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GXAZHSPBzLLv",
        "colab_type": "text"
      },
      "source": [
        "Sometimes, for example, when dealing with time-series problems, you might have  features which are not individual values but a list of values. For example,  transactions by a customer in a given period of time. In these cases, we create  different types of features such as: with numerical features, when you are grouping  on a categorical column, you will get features like a list of values which are time  distributed. In these cases, you can create a bunch of statistical features such as:  \n",
        "- Mean  \n",
        "- Max  \n",
        "- Min  \n",
        "- Unique  \n",
        "- Skew  \n",
        "- Kurtosis  \n",
        "- Kstat  \n",
        "- Percentile  \n",
        "- Quantile \n",
        "- Peak to peak  \n",
        "- And many more  \n",
        "These can be created using simple numpy functions, as shown in the following  python snippet. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WADl-vOVzckw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np  \n",
        "feature_dict = {}  \n",
        "#calculate mean  \n",
        "feature_dict['mean'] = np.mean(x)  \n",
        "#calculate max  \n",
        "feature_dict['max'] = np.max(x)  \n",
        "#calculate min  \n",
        "feature_dict['min'] = np.min(x)  \n",
        "#calculate standard deviation  \n",
        "feature_dict['std'] = np.std(x)  \n",
        "#calculate variance  \n",
        "feature_dict['var'] = np.var(x)  \n",
        "#peak-to-peak  \n",
        "feature_dict['ptp'] = np.ptp(x)  \n",
        "#percentile features  \n",
        "feature_dict['percentile_10'] = np.percentile(x, 10)  \n",
        "feature_dict['percentile_60'] = np.percentile(x, 60)  \n",
        "feature_dict['percentile_90'] = np.percentile(x, 90)  \n",
        "#quantile features  \n",
        "feature_dict['quantile_5'] = np.quantile(x, 0.05)  \n",
        "feature_dict['quantile_95'] = np.quantile(x, 0.95)  \n",
        "feature_dict['quantile_99'] = np.quantile(x, 0.99) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DHmsLbRXzTVH",
        "colab_type": "text"
      },
      "source": [
        "The time series data (list of values) can be converted to a lot of features.  A python library called tsfresh is instrumental in this case. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "06OjAj71z-nj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tsfresh.feature_extraction import feature_calculators as fc  \n",
        "#tsfresh based features  \n",
        "feature_dict['abs_energy'] = fc.abs_energy(x)  \n",
        "feature_dict['count_above_mean'] = fc.count_above_mean(x)  \n",
        "feature_dict['count_below_mean'] = fc.count_below_mean(x)  f\n",
        "eature_dict['mean_abs_change'] = fc.mean_abs_change(x)  \n",
        "feature_dict['mean_change'] = fc.mean_change(x) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bpQ_Ei4u0FH3",
        "colab_type": "text"
      },
      "source": [
        "This is not all; tsfresh offers hundreds of features and tens of variations of different  features that you can use for time series (list of values) based features. In the  examples above, x is a list of values. But that’s not all. There are many other features  that you can create for numerical data with or without categorical data. A simple  way to generate many features is just to create a bunch of polynomial features. For  example, a second-degree polynomial feature from two features “a” and “b” would  include: “a”, “b”, “ab”, “a2 ” and “b2 ”. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HsOjjz680WgL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        },
        "outputId": "f9ff3056-64da-4db5-9714-a8089ce1b50e"
      },
      "source": [
        "#EXAMPLE\n",
        "import numpy as np  \n",
        "#generate a random dataframe with  #2 columns and 100 rows  \n",
        "df = pd.DataFrame(  np.random.rand(100, 2),  \n",
        "                  columns=[f\"f_{i}\" for i in range(1, 3)]  ) \n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>f_1</th>\n",
              "      <th>f_2</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.351459</td>\n",
              "      <td>0.649085</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.298495</td>\n",
              "      <td>0.529528</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.568660</td>\n",
              "      <td>0.352811</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.460669</td>\n",
              "      <td>0.296580</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.401606</td>\n",
              "      <td>0.999588</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "        f_1       f_2\n",
              "0  0.351459  0.649085\n",
              "1  0.298495  0.529528\n",
              "2  0.568660  0.352811\n",
              "3  0.460669  0.296580\n",
              "4  0.401606  0.999588"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "thW5ufTm0oJX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        },
        "outputId": "cdc26280-88a6-445c-e0f4-d7c2e5087743"
      },
      "source": [
        "#Creating two degree polynomial features \n",
        "from sklearn import preprocessing  \n",
        "#initialize polynomial features class object  #for two-degree polynomial features  \n",
        "pf = preprocessing.PolynomialFeatures(  degree=2,  interaction_only=False,  include_bias=False  )  \n",
        "#fit to the features  \n",
        "pf.fit(df)  \n",
        "#create polynomial features  \n",
        "poly_feats = pf.transform(df)  \n",
        "#create a dataframe with all the features  \n",
        "num_feats = poly_feats.shape[1]  \n",
        "df_transformed = pd.DataFrame(  poly_feats,  columns=[f\"f_{i}\" for i in range(1, num_feats + 1)]  ) \n",
        "df_transformed.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>f_1</th>\n",
              "      <th>f_2</th>\n",
              "      <th>f_3</th>\n",
              "      <th>f_4</th>\n",
              "      <th>f_5</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.351459</td>\n",
              "      <td>0.649085</td>\n",
              "      <td>0.123523</td>\n",
              "      <td>0.228127</td>\n",
              "      <td>0.421312</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.298495</td>\n",
              "      <td>0.529528</td>\n",
              "      <td>0.089099</td>\n",
              "      <td>0.158062</td>\n",
              "      <td>0.280400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.568660</td>\n",
              "      <td>0.352811</td>\n",
              "      <td>0.323374</td>\n",
              "      <td>0.200629</td>\n",
              "      <td>0.124475</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.460669</td>\n",
              "      <td>0.296580</td>\n",
              "      <td>0.212216</td>\n",
              "      <td>0.136625</td>\n",
              "      <td>0.087960</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.401606</td>\n",
              "      <td>0.999588</td>\n",
              "      <td>0.161287</td>\n",
              "      <td>0.401441</td>\n",
              "      <td>0.999177</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "        f_1       f_2       f_3       f_4       f_5\n",
              "0  0.351459  0.649085  0.123523  0.228127  0.421312\n",
              "1  0.298495  0.529528  0.089099  0.158062  0.280400\n",
              "2  0.568660  0.352811  0.323374  0.200629  0.124475\n",
              "3  0.460669  0.296580  0.212216  0.136625  0.087960\n",
              "4  0.401606  0.999588  0.161287  0.401441  0.999177"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UuaTskV-1SIB",
        "colab_type": "text"
      },
      "source": [
        "So, now we have created some polynomial features. If you create third-degree  polynomial features, you will end up with nine features in total. The more the number of features, the more the number of polynomial features and you must also  remember that if you have a lot of samples in the dataset, it is going to take a while  creating these kinds of features. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i4Y9uQSP1idp",
        "colab_type": "text"
      },
      "source": [
        "**BINNING**\n",
        "\n",
        "\n",
        "Another interesting feature converts the numbers to categories. It’s known as  binning. Let’s look at figure 5, which shows a sample histogram of a random  numerical feature. We use ten bins for this figure, and we see that we can divide the  data into ten parts. This is accomplished using the pandas’ cut function. \n",
        "\n",
        "When you bin, you can use both the bin and the original feature. We will learn a bit  more about selecting features later in this chapter. Binning also enables you to treat  numerical features as categorical. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eAbH2lNT06oy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        },
        "outputId": "34b3c179-730e-4c62-a3b3-e8dc3d7c848f"
      },
      "source": [
        "#create bins of the numerical columns  \n",
        "#10 bins  \n",
        "df[\"f_bin_10\"] = pd.cut(df[\"f_1\"], bins=10, labels=False)  \n",
        "#100 bins  \n",
        "df[\"f_bin_100\"] = pd.cut(df[\"f_1\"], bins=100, labels=False) \n",
        "\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>f_1</th>\n",
              "      <th>f_2</th>\n",
              "      <th>f_bin_10</th>\n",
              "      <th>f_bin_100</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.351459</td>\n",
              "      <td>0.649085</td>\n",
              "      <td>3</td>\n",
              "      <td>34</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.298495</td>\n",
              "      <td>0.529528</td>\n",
              "      <td>2</td>\n",
              "      <td>28</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.568660</td>\n",
              "      <td>0.352811</td>\n",
              "      <td>5</td>\n",
              "      <td>56</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.460669</td>\n",
              "      <td>0.296580</td>\n",
              "      <td>4</td>\n",
              "      <td>45</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.401606</td>\n",
              "      <td>0.999588</td>\n",
              "      <td>3</td>\n",
              "      <td>39</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "        f_1       f_2  f_bin_10  f_bin_100\n",
              "0  0.351459  0.649085         3         34\n",
              "1  0.298495  0.529528         2         28\n",
              "2  0.568660  0.352811         5         56\n",
              "3  0.460669  0.296580         4         45\n",
              "4  0.401606  0.999588         3         39"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e3c3vtFy2bAw",
        "colab_type": "text"
      },
      "source": [
        "**LOG TRANSFORMATION**\n",
        "\n",
        "If there is a special feature with a very high variance. Compared to other features that  have a low variance (let’s assume that). Thus, we would want to reduce the variance  of this column, and that can be done by taking a log transformation. IF we draw a histogram of that variable we can view its variance and f the variance is high we can apply log(1 + x) to this column to reduce its variance.\n",
        "\n",
        "In [X]: df.f_3.var()  \n",
        "Out[X]: 8077265.875858586  \n",
        "In [X]: df.f_3.apply(lambda x: np.log(1 + x)).var()  \n",
        "Out[X]: 0.6058771732119975  \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ihGdZd5c3IaB",
        "colab_type": "text"
      },
      "source": [
        "Sometimes, instead of log, you can also take exponential. A very interesting case is  when you use a log-based evaluation metric, for example, RMSLE. In that case,  you can train on log-transformed targets and convert back to original using  exponential on the prediction. That would help optimize the model for the metric.  Most of the time, these kinds of numerical features are created based on intuition.  \n",
        "\n",
        "There is no formula. If you are working in an industry, you will create your industryspecific features.  When dealing with both categorical and numerical variables, you might encounter  missing values. We saw some ways to handle missing values in categorical features  in the previous chapter, but there are many more ways to handle missing/NaN  values. This is also considered feature engineering. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kg1ZITvi3wbn",
        "colab_type": "text"
      },
      "source": [
        "For categorical features, let’s keep it super simple. If you ever encounter missing  values in categorical features, treat is as a new category! As simple as this is, it  (almost) always works! \n",
        "\n",
        " One way to fill missing values in numerical data would be to choose a value that  does not appear in the specific feature and fill using that. For example, let’s say 0  is not seen in the feature. So, we fill all the missing values using 0. This is one of  the ways but might not be the most effective. One of the methods that works better  than filling 0s for numerical data is to fill with mean instead. You can also try to fill  with the median of all the values for that feature, or you can use the most common  value to fill the missing values. There are just so many ways to do this. \n",
        " \n",
        "  A fancy way of filling in the missing values would be to use a k-nearest neighbour  method. You can select a sample with missing values and find the nearest  neighbours utilising some kind of distance metric, for example, Euclidean distance.  Then you can take the mean of all nearest neighbours and fill up the missing value.  You can use the KNN imputer implementation for filling missing values like this. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YtoKCqD_2Zqk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np  \n",
        "from sklearn import impute  \n",
        "#create a random numpy array with 10 samples  \n",
        "#and 6 features and values ranging from 1 to 15  \n",
        "X = np.random.randint(1, 15, (10, 6))\n",
        "#convert the array to float  \n",
        "X = X.astype(float)  \n",
        "#randomly assign 10 elements to NaN (missing)  \n",
        "X.ravel()[np.random.choice(X.size, 10, replace=False)] = np.nan\n",
        "#use 2 nearest neighbours to fill na values  \n",
        "knn_imputer = impute.KNNImputer(n_neighbors=2)  \n",
        "knn_imputer.fit_transform(X)  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xIRcVlxW4nZR",
        "colab_type": "text"
      },
      "source": [
        "Another way of imputing missing values in a column would be to train a regression  model that tries to predict missing values in a column based on other columns. So,  you start with one column that has a missing value and treat this column as the  target column for regression model without the missing values. Using all the other  columns, you now train a model on samples for which there is no missing value in  the concerned column and then try to predict target (the same column) for the  samples that were removed earlier. This way, you have a more robust model based  imputation. \n",
        "\n",
        "\n",
        "***Always remember that imputing values for tree-based models is unnecessary as they  can handle it themselves. ***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u1qppun250wm",
        "colab_type": "text"
      },
      "source": [
        "***Now, let’s say you are working on a problem of predicting store sales of different  items (per week or month). You have items, and you have store ids. So, you can  create features like items per store. Now, this is one of the features that is not  discussed above. These kinds of features cannot be generalized and come purely  from domain, data and business knowledge. Look at the data and see what fits and create features accordingly. And always remember to scale or normalize your  features if you are using linear models like logistic regression or a model like SVM.  Tree-based models will always work fine without any normalization of features.***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YiXvtad66UN4",
        "colab_type": "text"
      },
      "source": [
        "**FEATURE SELECTION**\n",
        "\n",
        "The simplest form of selecting features would be to remove features with very  low variance. If the features have a very low variance (i.e. very close to 0), they  are close to being constant and thus, do not add any value to any model at all. It  would just be nice to get rid of them and hence lower the complexity. Please note  that the variance also depends on scaling of the data. Scikit-learn has an  implementation for VarianceThreshold that does precisely this.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FFx9zmsZ6ftz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.feature_selection import VarianceThreshold  \n",
        "data =...  \n",
        "var_thresh = VarianceThreshold(threshold=0.1)  \n",
        "transformed_data = var_thresh.fit_transform(data)  \n",
        "#transformed data will have all columns with variance less  \n",
        "#than 0.1 removed "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3dXMlpDS6URa",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vhWTIBTO405O",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 330
        },
        "outputId": "a93eee2e-ccc4-4785-e8ad-720759460752"
      },
      "source": [
        "import pandas as pd  \n",
        "from sklearn.datasets import fetch_california_housing  \n",
        "#fetch a regression dataset  \n",
        "data = fetch_california_housing()  \n",
        "X = data[\"data\"]  \n",
        "col_names = data[\"feature_names\"]  \n",
        "y = data[\"target\"]\n",
        "df = pd.DataFrame(X, columns=col_names)  \n",
        "#introduce a highly correlated column  \n",
        "df.loc[:, \"MedInc_Sqrt\"] = df.MedInc.apply(np.sqrt)  \n",
        "#get correlation matrix (pearson)  \n",
        "df.corr()\n",
        "\n",
        "#We see that the feature MedInc_Sqrt has a very high correlation with MedInc. We  can thus remove one of them. "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading Cal. housing from https://ndownloader.figshare.com/files/5976036 to /root/scikit_learn_data\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>MedInc</th>\n",
              "      <th>HouseAge</th>\n",
              "      <th>AveRooms</th>\n",
              "      <th>AveBedrms</th>\n",
              "      <th>Population</th>\n",
              "      <th>AveOccup</th>\n",
              "      <th>Latitude</th>\n",
              "      <th>Longitude</th>\n",
              "      <th>MedInc_Sqrt</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>MedInc</th>\n",
              "      <td>1.000000</td>\n",
              "      <td>-0.119034</td>\n",
              "      <td>0.326895</td>\n",
              "      <td>-0.062040</td>\n",
              "      <td>0.004834</td>\n",
              "      <td>0.018766</td>\n",
              "      <td>-0.079809</td>\n",
              "      <td>-0.015176</td>\n",
              "      <td>0.984329</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>HouseAge</th>\n",
              "      <td>-0.119034</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>-0.153277</td>\n",
              "      <td>-0.077747</td>\n",
              "      <td>-0.296244</td>\n",
              "      <td>0.013191</td>\n",
              "      <td>0.011173</td>\n",
              "      <td>-0.108197</td>\n",
              "      <td>-0.132797</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>AveRooms</th>\n",
              "      <td>0.326895</td>\n",
              "      <td>-0.153277</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.847621</td>\n",
              "      <td>-0.072213</td>\n",
              "      <td>-0.004852</td>\n",
              "      <td>0.106389</td>\n",
              "      <td>-0.027540</td>\n",
              "      <td>0.326688</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>AveBedrms</th>\n",
              "      <td>-0.062040</td>\n",
              "      <td>-0.077747</td>\n",
              "      <td>0.847621</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>-0.066197</td>\n",
              "      <td>-0.006181</td>\n",
              "      <td>0.069721</td>\n",
              "      <td>0.013344</td>\n",
              "      <td>-0.066910</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Population</th>\n",
              "      <td>0.004834</td>\n",
              "      <td>-0.296244</td>\n",
              "      <td>-0.072213</td>\n",
              "      <td>-0.066197</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.069863</td>\n",
              "      <td>-0.108785</td>\n",
              "      <td>0.099773</td>\n",
              "      <td>0.018415</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>AveOccup</th>\n",
              "      <td>0.018766</td>\n",
              "      <td>0.013191</td>\n",
              "      <td>-0.004852</td>\n",
              "      <td>-0.006181</td>\n",
              "      <td>0.069863</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.002366</td>\n",
              "      <td>0.002476</td>\n",
              "      <td>0.015266</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Latitude</th>\n",
              "      <td>-0.079809</td>\n",
              "      <td>0.011173</td>\n",
              "      <td>0.106389</td>\n",
              "      <td>0.069721</td>\n",
              "      <td>-0.108785</td>\n",
              "      <td>0.002366</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>-0.924664</td>\n",
              "      <td>-0.084303</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Longitude</th>\n",
              "      <td>-0.015176</td>\n",
              "      <td>-0.108197</td>\n",
              "      <td>-0.027540</td>\n",
              "      <td>0.013344</td>\n",
              "      <td>0.099773</td>\n",
              "      <td>0.002476</td>\n",
              "      <td>-0.924664</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>-0.015569</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>MedInc_Sqrt</th>\n",
              "      <td>0.984329</td>\n",
              "      <td>-0.132797</td>\n",
              "      <td>0.326688</td>\n",
              "      <td>-0.066910</td>\n",
              "      <td>0.018415</td>\n",
              "      <td>0.015266</td>\n",
              "      <td>-0.084303</td>\n",
              "      <td>-0.015569</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "               MedInc  HouseAge  AveRooms  ...  Latitude  Longitude  MedInc_Sqrt\n",
              "MedInc       1.000000 -0.119034  0.326895  ... -0.079809  -0.015176     0.984329\n",
              "HouseAge    -0.119034  1.000000 -0.153277  ...  0.011173  -0.108197    -0.132797\n",
              "AveRooms     0.326895 -0.153277  1.000000  ...  0.106389  -0.027540     0.326688\n",
              "AveBedrms   -0.062040 -0.077747  0.847621  ...  0.069721   0.013344    -0.066910\n",
              "Population   0.004834 -0.296244 -0.072213  ... -0.108785   0.099773     0.018415\n",
              "AveOccup     0.018766  0.013191 -0.004852  ...  0.002366   0.002476     0.015266\n",
              "Latitude    -0.079809  0.011173  0.106389  ...  1.000000  -0.924664    -0.084303\n",
              "Longitude   -0.015176 -0.108197 -0.027540  ... -0.924664   1.000000    -0.015569\n",
              "MedInc_Sqrt  0.984329 -0.132797  0.326688  ... -0.084303  -0.015569     1.000000\n",
              "\n",
              "[9 rows x 9 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CmuaC55A7Mwt",
        "colab_type": "text"
      },
      "source": [
        "And now we can move to some univariate ways of feature selection. Univariate  feature selection is nothing but a scoring of each feature against a given target.  Mutual information, ANOVA F-test and chi2 are some of the most popular  methods for univariate feature selection.\n",
        "\n",
        " There are two ways of using these in scikitlearn. \n",
        "  \n",
        "- SelectKBest: It keeps the top-k scoring features \n",
        "- SelectPercentile: It keeps the top features which are in a percentage  specified by the user  It must be noted that you can use chi2 only for data which is non-negative in nature.  \n",
        "  \n",
        "  This is a particularly useful feature selection technique in natural language  processing when we have a bag of words or tf-idf based features. It’s best to create  a wrapper for univariate feature selection that you can use for almost any new  problem. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tfKisXaJ7dJX",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mrmSWWxk71tv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.feature_selection import chi2  \n",
        "from sklearn.feature_selection import f_classif  \n",
        "from sklearn.feature_selection import f_regression  \n",
        "from sklearn.feature_selection import mutual_info_classif  \n",
        "from sklearn.feature_selection import mutual_info_regression  \n",
        "from sklearn.feature_selection import SelectKBest  \n",
        "from sklearn.feature_selection import SelectPercentile  \n",
        "class UnivariateFeatureSelction:  \n",
        "  def __init__(self, n_features, problem_type, scoring):  \n",
        "    \"\"\"  Custom univariate feature selection wrapper on  different univariate feature selection models \n",
        "    from  scikit-learn.  :param n_features: SelectPercentile if float else SelectKBest  :param problem_type: \n",
        "    classification or regression  :param scoring: scoring function, string  \"\"\"  \n",
        "    #for a given problem type, there are only  \n",
        "    #a few valid scoring methods  \n",
        "    #you can extend this with your own custom  #methods if you wish  \n",
        "    if problem_type == \"classification\":  \n",
        "      valid_scoring = {  \"f_classif\": f_classif,  \"chi2\": chi2,  \"mutual_info_classif\": mutual_info_classif  }  \n",
        "    else:  \n",
        "      valid_scoring = {  \"f_regression\": f_regression,  \"mutual_info_regression\": mutual_info_regression  } \n",
        "\n",
        "      #raise exception if we do not have a valid scoring method  \n",
        "      if scoring not in valid_scoring:  \n",
        "        raise Exception(\"Invalid scoring function\")  \n",
        "      #if n_features is int, we use selectkbest  \n",
        "      #if n_features is float, we use selectpercentile  \n",
        "      #please note that it is int in both cases in sklearn  \n",
        "      if isinstance(n_features, int):  \n",
        "        self.selection = SelectKBest(  valid_scoring[scoring],  k=n_features\n",
        "                                     )  \n",
        "      elif isinstance(n_features, float):  \n",
        "        self.selection = SelectPercentile(  valid_scoring[scoring],  percentile=int(n_features * 100)  ) \n",
        "       else:  \n",
        "         raise Exception(\"Invalid type of feature\")  \n",
        "    #same fit function  \n",
        "    def fit(self, X, y):  \n",
        "      return self.selection.fit(X, y)  \n",
        "    #same transform function  \n",
        "    def transform(self, X):  \n",
        "      return self.selection.transform(X)  \n",
        "    #same fit_transform function  \n",
        "    def fit_transform(self, X, y):  \n",
        "      return self.selection.fit_transform(X, y)  \n",
        "\n",
        "ufs = UnivariateFeatureSelction(  n_features=0.1,  problem_type=\"regression\",  scoring=\"f_regression\"  )  \n",
        "ufs.fit(X, y)  \n",
        "X_transformed = ufs.transform(X) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FQP2cwSs7M_O",
        "colab_type": "text"
      },
      "source": [
        "The simplest form of feature selection that uses a model for selection is known as  greedy feature selection. In greedy feature selection, the first step is to choose a  model. The second step is to select a loss/scoring function. And the third and final  step is to iteratively evaluate each feature and add it to the list of “good” features if it improves loss/score. It can’t get simpler than this. But you must keep in mind that  this is known as greedy feature selection for a reason. This feature selection process  will fit a given model each time it evaluates a feature. The computational cost  associated with this kind of method is very high. It will also take a lot of time for  this kind of feature selection to finish. And if you do not use this feature selection  properly, then you might even end up overfitting the model. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oKiTnBHO9TVT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd  \n",
        "from sklearn import linear_model  \n",
        "from sklearn import metrics  \n",
        "from sklearn.datasets import make_classification  \n",
        "class GreedyFeatureSelection:  \n",
        "  \"\"\"  A simple and custom class for greedy feature selection. \n",
        "   You will need to modify it quite a bit to make it suitable  for your dataset.  \"\"\"  \n",
        "  def evaluate_score(self, X, y):\n",
        "    \"\"\"  This function evaluates model on data and returns  Area Under ROC Curve \n",
        "     (AUC)  NOTE: We fit the data and calculate AUC on same data.  WE ARE OVERFITTING HERE. \n",
        "      But this is also a way to achieve greedy selection.  k-fold will take k times longer.  \n",
        "      If you want to implement it in really correct way,  calculate OOF AUC and return mean AUC\n",
        "       over k folds.  :param X: training data  :param y: targets  :return:\n",
        "         overfitted area under the roc curve  \"\"\" \n",
        "    #fit the logistic regression model,  \n",
        "    #and calculate AUC on same data  \n",
        "    #again: BEWARE  \n",
        "    #you can choose any model that suits your data \n",
        "    model = linear_model.LogisticRegression()  \n",
        "    model.fit(X, y)  \n",
        "    predictions = model.predict_proba(X)[:, 1]  \n",
        "    auc = metrics.roc_auc_score(y, predictions)  \n",
        "    return auc\n",
        "  def _feature_selection(self, X, y):\n",
        "\n",
        "    \"\"\"  This function does the actual greedy selection  :param X: data, numpy array  \n",
        "          :param y: targets, numpy array  :return: (best scores, best features)  \"\"\"  \n",
        "    #initialize good features list  \n",
        "    #and best scores to keep track of both  \n",
        "    good_features = []  \n",
        "    best_scores = []  \n",
        "          #calculate the number of features  \n",
        "    num_features = X.shape[1]  \n",
        "          #infinite loop  \n",
        "    while True:  \n",
        "            #initialize best feature and score of this loop  \n",
        "      this_feature = None  \n",
        "      best_score = 0  \n",
        "            #loop over all features  \n",
        "      for feature in range(num_features):  \n",
        "              #if feature is already in good features,  \n",
        "              #skip this for loop  \n",
        "        if feature in good_features:  \n",
        "          continue  \n",
        "              #selected features are all good features till now  \n",
        "              #and current feature  \n",
        "          selected_features = good_features + [feature]  \n",
        "              #remove all other features from data  \n",
        "          xtrain = X[:, selected_features]  \n",
        "              #calculate the score, in our case, AUC  \n",
        "          score = self.evaluate_score(xtrain, y)  \n",
        "              #if score is greater than the best score  \n",
        "              #of this loop, change best score and best feature  \n",
        "          if score > best_score:  \n",
        "            this_feature = feature  \n",
        "            best_score = score  \n",
        "                #if we have selected a feature, add\n",
        "                #to the good feature list and update best scores list  \n",
        "          if this_feature != None:  \n",
        "              good_features.append(this_feature)  \n",
        "              best_scores.append(best_score) \n",
        "                #if we didnt improve during the previous round,  \n",
        "                #exit the while loop  \n",
        "          if len(best_scores) > 2:  \n",
        "            if best_scores[-1] < best_scores[-2]:  \n",
        "              break  \n",
        "                    #return best scores and good features  \n",
        "                    #why do we remove the last data point?  \n",
        "    return best_scores[:-1], good_features[:-1]  \n",
        "  def __call__(self, X, y):  \n",
        "    \"\"\"  Call function will call the class on a set of arguments  \"\"\"  \n",
        "      #select features, return scores and selected indices  \n",
        "    scores, features = self._feature_selection(X, y)  \n",
        "      #transform data with selected features  \n",
        "    return X[:, features], scores \n",
        "    \n",
        "\n",
        "\n",
        " \n",
        "   \n",
        " "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mJlhMUSu_PdO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if  __name__  ==  \"__main__\":  \n",
        "  #generate binary classification data  \n",
        "  X, y = make_classification(n_samples=1000, n_features=100)  \n",
        "  #transform data by greedy feature selection  \n",
        "  X_transformed, scores = GreedyFeatureSelection()(X, y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Keb2wUx_CPEV",
        "colab_type": "text"
      },
      "source": [
        "The greedy feature selection implemented the way returns scores and a list of  feature indices. Figure 2 shows how this score improves with the addition of a new  feature in every iteration. We see that we are not able to improve our score after a  certain point, and that’s where we stop.  Another greedy approach is known as recursive feature elimination (RFE).\n",
        "\n",
        " In the  previous method, we started with one feature and kept adding new features, but in  RFE, we start with all features and keep removing one feature in every iteration that  provides the least value to a given model. But how to do we know which feature  offers the least value? Well, if we use models like linear support vector machine  (SVM) or logistic regression, we get a coefficient for each feature which decides  the importance of the features. In case of any tree-based models, we get feature  importance in place of coefficients. In each iteration, we can eliminate the least important feature and keep eliminating it until we reach the number of features  needed. So, yes, we have the ability to decide how many features we want to keep. \n",
        "\n",
        "\n",
        "\n",
        " We saw two different greedy ways to select features from a model. But you can also  fit the model to the data and select features from the model by the feature  coefficients or the importance of features. If you use coefficients, you can select  a threshold, and if the coefficient is above that threshold, you can keep the feature  else eliminate it.  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5pP_9SxxCS9C",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 134
        },
        "outputId": "6535d6a8-b6af-4517-d8fb-30cf7a1ae15c"
      },
      "source": [
        "import pandas as pd  \n",
        "from sklearn.feature_selection import RFE  \n",
        "from sklearn.linear_model import LinearRegression  \n",
        "from sklearn.datasets import fetch_california_housing  \n",
        "#fetch a regression dataset  \n",
        "data = fetch_california_housing()  \n",
        "X = data[\"data\"]  \n",
        "col_names = data[\"feature_names\"]  \n",
        "y = data[\"target\"] \n",
        "#initialize the model  \n",
        "model = LinearRegression()  \n",
        "#initialize RFE  \n",
        "rfe = RFE(  estimator=model,  n_features_to_select=3  )  \n",
        "#fit RFE  \n",
        "rfe.fit(X, y)  \n",
        "#get the transformed data with  \n",
        "#selected columns  \n",
        "X_transformed = rfe.transform(X) \n",
        "X_transformed"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[   8.3252,   37.88  , -122.23  ],\n",
              "       [   8.3014,   37.86  , -122.22  ],\n",
              "       [   7.2574,   37.85  , -122.24  ],\n",
              "       ...,\n",
              "       [   1.7   ,   39.43  , -121.22  ],\n",
              "       [   1.8672,   39.43  , -121.32  ],\n",
              "       [   2.3886,   39.37  , -121.24  ]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DPwk5126D9U-",
        "colab_type": "text"
      },
      "source": [
        "Well, selecting the best features from the model is nothing new. You can choose  features from one model and use another model to train. For example, you can use  Logistic Regression coefficients to select the features and then use Random Forest  to train the model on chosen features. Scikit-learn also offers SelectFromModel  class that helps you choose features directly from a given model. You can also  specify the threshold for coefficients or feature importance if you want and the  maximum number of features you want to select. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tszcwUyCDFqM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "outputId": "1ebae443-3179-4c31-f48b-f0e1e1e538f0"
      },
      "source": [
        "  #IN RANDOM FOREST\n",
        "  import pandas as pd  \n",
        "  from sklearn.datasets import load_diabetes  \n",
        "  from sklearn.ensemble import RandomForestRegressor\n",
        "  import matplotlib.pyplot as plt \n",
        "  #fetch a regression dataset  #in diabetes data we predict diabetes progression  \n",
        "  #after one year based on some features  \n",
        "  data = load_diabetes()  \n",
        "  X = data[\"data\"]  \n",
        "  col_names = data[\"feature_names\"]  \n",
        "  y = data[\"target\"]  \n",
        "  #initialize the model  \n",
        "  model = RandomForestRegressor()  \n",
        "  #fit the model  \n",
        "  model.fit(X, y) \n",
        "\n",
        "importances = model.feature_importances_  \n",
        "idxs = np.argsort(importances)  \n",
        "plt.title('Feature Importances')  \n",
        "plt.barh(range(len(idxs)), importances[idxs], align='center') \n",
        "plt.yticks(range(len(idxs)), [col_names[i] for i in idxs])  \n",
        "plt.xlabel('Random Forest Feature Importance')  \n",
        "plt.show()  "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hxDlstTGD_dS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "1b90851e-8a8e-43f6-bce0-c4a3d74f89ec"
      },
      "source": [
        "import pandas as pd  \n",
        "from sklearn.datasets import load_diabetes  \n",
        "from sklearn.ensemble import RandomForestRegressor  \n",
        "from sklearn.feature_selection import SelectFromModel  \n",
        "#fetch a regression dataset  \n",
        "#in diabetes data we predict diabetes progression  \n",
        "#after one year based on some features  \n",
        "data = load_diabetes()  \n",
        "X = data[\"data\"]  \n",
        "col_names = data[\"feature_names\"] \n",
        "y = data[\"target\"]  \n",
        " #initialize the model  \n",
        "model = RandomForestRegressor()  \n",
        " #select from the model  \n",
        "sfm = SelectFromModel(estimator=model)  \n",
        "X_transformed = sfm.fit_transform(X, y)  \n",
        " #see which features were selected  \n",
        "support = sfm.get_support()  \n",
        " #get feature names  \n",
        "print([  x for x, y in zip(col_names, support) if y == True  ]) "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['bmi', 's5']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "860Dl-06FbdI",
        "colab_type": "text"
      },
      "source": [
        "forest. One more thing that we are missing here is feature  selection using models that have L1 (Lasso) penalization. When we have L1  penalization for regularization, most coefficients will be 0 (or close to 0), and we  select the features with non-zero coefficients. You can do it by just replacing  random forest in the snippet of selection from a model with a model that supports  L1 penalty, e.g. lasso regression. All tree-based models provide feature importance  so all the model-based snippets shown in this chapter can be used for XGBoost,  LightGBM or CatBoost. The feature importance function names might be different  and may produce results in a different format, but the usage will remain the same.  In the end, you must be careful when doing feature selection. Select features on training data and validate the model on validation data for proper selection of  features without overfitting the model. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y_REX9BEFzh0",
        "colab_type": "text"
      },
      "source": [
        "** Hyperparameter optimization **\n",
        "\n",
        "The parameters that the model  has here are known as hyper-parameters, i.e. the parameters that control the  training/fitting process of the model. If we train a linear regression with SGD,  parameters of a model are the slope and the bias and hyperparameter is learning  rate."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M4sux2ETDZQZ",
        "colab_type": "text"
      },
      "source": [
        "Let’s look at the random forest model from scikit-learn.  \n",
        "\n",
        "RandomForestClassifier(  n_estimators=100, \n",
        " criterion='gini', \n",
        "  max_depth=None,  \n",
        "  min_samples_split=2,  \n",
        "  min_samples_leaf=1,  \n",
        "  min_weight_fraction_leaf=0.0, \n",
        " max_features='auto', \n",
        " max_leaf_nodes=None, \n",
        " min_impurity_decrease=0.0,  \n",
        "min_impurity_split=None,  bootstrap=True, \n",
        " oob_score=False, \n",
        "  n_jobs=None, \n",
        "   random_state=None, \n",
        "  verbose=0,  \n",
        "  warm_start=False, \n",
        " class_weight=None,  \n",
        " ccp_alpha=0.0, \n",
        "  max_samples=None,  )  \n",
        "\n",
        "There are nineteen parameters, and all the combinations of all these parameters for  all the values they can assume are going to be infinite. Normally, we don’t have the  resource and time to do this. Thus, we specify a grid of parameters. A search over  this grid to find the best combination of parameters is known as grid search. We  can say that n_estimators can be 100, 200, 250, 300, 400, 500; max_depth can be  1, 2, 5, 7, 11, 15 and criterion can be gini or entropy. These may not look like a lot  of parameters, but it would take a lot of time for computation if the dataset is too  large. We can make this grid search work by creating three for loops like before and calculating the score on the validation set. It must also be noted that if you have kfold cross-validation, you need even more loops which implies even more time to  find the perfect parameters. Grid search is therefore not very popular. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nPy6shRjDZVU",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DIEbRnQvDZOX",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GBTcY3S1DVVd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#rf_grid_search.py  \n",
        "import numpy as np  \n",
        "import pandas as pd  \n",
        "from sklearn import ensemble  \n",
        "from sklearn import metrics  \n",
        "from sklearn import model_selection  \n",
        "if  __name__  ==  \"__main__\":  \n",
        "  #read the training data  \n",
        "  df = pd.read_csv(\"../input/mobile_train.csv\")  \n",
        "  #features are all columns without price_range  \n",
        "  #note that there is no id column in this dataset \n",
        "  #here we have training features \n",
        "  X = df.drop(\"price_range\", axis=1).values  \n",
        "  #and the targets  \n",
        "  y = df.price_range.values  \n",
        "  #define the model here  \n",
        "  #i am using random forest with n_jobs=-1  \n",
        "  #n_jobs=-1 => use all cores  \n",
        "  classifier = ensemble.RandomForestClassifier(n_jobs=-1)  \n",
        "  #define a grid of parameters  \n",
        "  #this can be a dictionary or a list of  #dictionaries  \n",
        "  param_grid = {  \"n_estimators\": [100, 200, 250, 300, 400, 500],  \"max_depth\": [1, 2, 5, 7, 11, 15],  \"criterion\": [\"gini\", \"entropy\"]  }  \n",
        "  #initialize grid search  \n",
        "  #estimator is the model that we have defined  \n",
        "  #param_grid is the grid of parameters  \n",
        "  #we use accuracy as our metric. you can define your own  \n",
        "  #higher value of verbose implies a lot of details are printed  \n",
        "  #cv=5 means that we are using 5 fold cv (not stratified)  \n",
        "  model = model_selection.GridSearchCV(  estimator=classifier,  param_grid=param_grid,  scoring=\"accuracy\",  verbose=10,  n_jobs=1,  cv=5  )  \n",
        "  #fit the model and extract best score  \n",
        "  model.fit(X, y)  \n",
        "  print(f\"Best score: {model.best_score_}\")  print(\"Best parameters set:\")  \n",
        "  best_parameters = model.best_estimator_.get_params()  \n",
        "  for param_name in sorted(param_grid.keys()):  \n",
        "    print(f\"\\t{param_name}: {best_parameters[param_name]}\") "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iqiuN7eRGQ92",
        "colab_type": "text"
      },
      "source": [
        "In random search, we randomly select a combination of parameters and  calculate the cross-validation score. The time consumed here is less than grid search  because we do not evaluate over all different combinations of parameters. We  choose how many times we want to evaluate our models, and that’s what decides  how much time the search takes. The code is not very different from above. Except  for GridSearchCV, we use RandomizedSearchCV. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9W0bEp0gGR9z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#rf_random_search.py  .  .  .  \n",
        "if  __name__  ==  \"__main__\":  \n",
        "  #define the model here  \n",
        "  #i am using random forest with n_jobs=-1  \n",
        "  #n_jobs=-1 => use all cores \n",
        "  classifier = ensemble.RandomForestClassifier(n_jobs=-1)  \n",
        "  #define a grid of parameters  \n",
        "  #this can be a dictionary or a list of  #dictionaries  \n",
        "  param_grid = {  \"n_estimators\": np.arange(100, 1500, 100),  \"max_depth\": np.arange(1, 31),  \"criterion\": [\"gini\", \"entropy\"]  }  \n",
        "  #initialize random search  \n",
        "  #estimator is the model that we have defined  \n",
        "  #param_distributions is the grid/distribution of parameters  \n",
        "  #we use accuracy as our metric. you can define your own  #higher value of verbose implies a lot of details are printed \n",
        "   #cv=5 means that we are using 5 fold cv (not stratified) \n",
        "  #n_iter is the number of iterations we want  \n",
        "  #if param_distributions has all the values as list,  \n",
        "  #random search will be done by sampling without replacement  \n",
        "  #if any of the parameters come from a distribution,  \n",
        "  #random search uses sampling with replacement  \n",
        "  model = model_selection.RandomizedSearchCV(  estimator=classifier,  param_distributions=param_grid,  n_iter=20,  scoring=\"accuracy\",  verbose=10,  n_jobs=1,  cv=5  )  \n",
        "  #fit the model and extract best score  \n",
        "  model.fit(X, y)  \n",
        "  print(f\"Best score: {model.best_score_}\")  \n",
        "  print(\"Best parameters set:\")  \n",
        "  best_parameters = model.best_estimator_.get_params()  \n",
        "  for param_name in sorted(param_grid.keys()):  \n",
        "    print(f\"\\t{param_name}: {best_parameters[param_name]}\") "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aRzKUSiNHwka",
        "colab_type": "text"
      },
      "source": [
        "Random search is faster than grid search if the number of iterations is less. Using  these two, you can find the optimal (?) parameters for all kinds of models as long  as they have a fit and predict function, which is the standard of scikit-learn.  Sometimes, you might want to use a pipeline. For example, let’s say that we are  dealing with a multiclass classification problem. In this problem, the training data  consists of two text columns, and you are required to build a model to predict the  class. Let’s assume that the pipeline you choose is to first apply tf-idf in a semisupervised manner and then use SVD with SVM classifier. Now, the problem is we  have to select the components of SVD and also need to tune the parameters of SVM.  How to do this is shown in the following snippet. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-HKTnAF2Hv2G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#pipeline_search.py  \n",
        "#The pipeline shown here has SVD (Singular Value Decomposition), standard  scaling and an SVM (Support Vector Machines) model.\n",
        "import numpy as np  \n",
        "import pandas as pd  \n",
        "from sklearn import metrics  \n",
        "from sklearn import model_selection  \n",
        "from sklearn import pipeline  \n",
        "from sklearn.decomposition \n",
        "import TruncatedSVD  \n",
        "from sklearn.feature_extraction.text import TfidfVectorizer  \n",
        "from sklearn.preprocessing import StandardScaler  \n",
        "from sklearn.svm import SVC  \n",
        "def quadratic_weighted_kappa(y_true, y_pred):  \n",
        "  \"\"\"  Create a wrapper for cohen's kappa  with quadratic weights  \"\"\"  \n",
        "  return metrics.cohen_kappa_score(  y_true,  y_pred,  weights=\"quadratic\"  )  \n",
        "\n",
        "if  __name__  ==  '__main__':  \n",
        "  #Load the training file \n",
        "  train = pd.read_csv('../input/train.csv')  \n",
        "  #we dont need ID columns  \n",
        "  idx = test.id.values.astype(int)  \n",
        "  train = train.drop('id', axis=1)  \n",
        "  test = test.drop('id', axis=1)  \n",
        "  #create labels. drop useless columns  \n",
        "  y = train.relevance.values  \n",
        "  #do some lambda magic on text columns  \n",
        "  traindata = list(  train.apply(lambda x:'%s %s'% (x['text1'], x['text2']),axis=1)  )  \n",
        "  testdata = list(  test.apply(lambda x:'%s %s'% (x['text1'], x['text2']),axis=1)  )  \n",
        "  #tfidf vectorizer  \n",
        "  tfv = TfidfVectorizer(  min_df=3,  max_features=None,  strip_accents='unicode',  analyzer='word',  token_pattern=r'\\w{1,}',  ngram_range=(1, 3),  use_idf=1,  smooth_idf=1,  sublinear_tf=1,  stop_words='english'  )  \n",
        "  #Fit TFIDF  \n",
        "  tfv.fit(traindata)  \n",
        "  X = tfv.transform(traindata)  \n",
        "  X_test = tfv.transform(testdata)  \n",
        "  #Initialize SVD  \n",
        "  svd = TruncatedSVD()  \n",
        "  #Initialize the standard scaler  \n",
        "  scl = StandardScaler()  \n",
        "  #We will use SVM here..  \n",
        "  svm_model = SVC()  \n",
        "  #Create the pipeline\n",
        "  clf = pipeline.Pipeline(  [  ('svd', svd),  ('scl', scl),  ('svm', svm_model)  ]  )  \n",
        "  #Create a parameter grid to search for  \n",
        "  #best parameters for everything in the pipeline  \n",
        "  param_grid = {  'svd__n_components': [200, 300],  'svm__C': [10, 12]  }  \n",
        "  #Kappa Scorer  \n",
        "  kappa_scorer = metrics.make_scorer(  quadratic_weighted_kappa,  greater_is_better=True  )  \n",
        "  #Initialize Grid Search Model  \n",
        "  model = model_selection.GridSearchCV(  estimator=clf,  param_grid=param_grid,  scoring=kappa_scorer,  verbose=10,  n_jobs=-1,  refit=True,  cv=5  )  \n",
        "  #Fit Grid Search Model  \n",
        "  model.fit(X, y)  \n",
        "  print(\"Best score: %0.3f\"% model.best_score_)  \n",
        "  print(\"Best parameters set:\")  \n",
        "  best_parameters = model.best_estimator_.get_params()  \n",
        "  for param_name in sorted(param_grid.keys()):  \n",
        "    print(\"\\t%s: %r\"% (param_name, best_parameters[param_name]))  \n",
        "  #Get best model  \n",
        "  best_model = model.best_estimator_  \n",
        "  #Fit model with best parameters optimized for QWK  \n",
        "  best_model.fit(X, y)  \n",
        "  preds = best_model.predict(...) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gs04xJO_J0yN",
        "colab_type": "text"
      },
      "source": [
        "When we go into advanced hyperparameter optimization techniques, we can take a  look at minimization of functions using different kinds of minimization  algorithms. This can be achieved by using many minimization functions such as  downhill simplex algorithm, Nelder-Mead optimization, using a Bayesian  technique with Gaussian process for finding optimal parameters or by using a  genetic algorithm.First, let’s see how the gaussian  process can be used for hyper-parameter optimization. These kinds of algorithms  need a function they can optimize. Most of the time, it’s about the minimization of  this function, like we minimize loss.  So, let’s say, you want to find the best parameters for best accuracy and obviously,  the more the accuracy is better. Now we cannot minimize the accuracy, but we can  minimize it when we multiply it by -1. This way, we are minimizing the negative  of accuracy, but in fact, we are maximizing accuracy. Using Bayesian optimization  with gaussian process can be accomplished by using gp_minimize function from  scikit-optimize (skopt) library. Let’s take a look at how we can tune the parameters  of our random forest model using this function. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jkU9lcB7KCrq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#rf_gp_minimize.py  \n",
        "import numpy as np  \n",
        "import pandas as pd  \n",
        "from functools import partial  \n",
        "from sklearn import ensemble  \n",
        "from sklearn import metrics  \n",
        "from sklearn import model_selection  \n",
        "from skopt import gp_minimize  \n",
        "from skopt import space  \n",
        "def optimize(params, param_names, x, y):  \n",
        "  \"\"\"  The main optimization function.  This function takes all the arguments from the search space  and training features and targets. It then initializes \n",
        "  the models by setting the chosen parameters and runs  cross-validation and returns a negative accuracy score  :param params: list of params from 4\n",
        "  gp_minimize  :param param_names: list of param names. order is important!  \n",
        "  :param x: training data  :param y: labels/targets  :return: negative accuracy after 5 folds  \"\"\" \n",
        "\n",
        "  #convert params to dictionary  \n",
        "  params = dict(zip(param_names, params))  \n",
        "  #initialize model with current parameters  \n",
        "  model = ensemble.RandomForestClassifier(**params)  \n",
        "  #initialize stratified k-fold  \n",
        "  kf = model_selection.StratifiedKFold(n_splits=5)  \n",
        "  #initialize accuracy list  \n",
        "  accuracies = []  \n",
        "  #loop over all folds  \n",
        "  for idx in kf.split(X=x, y=y):  \n",
        "    train_idx, test_idx = idx[0], idx[1]  \n",
        "    xtrain = x[train_idx]  ytrain = y[train_idx]  \n",
        "    xtest = x[test_idx]  ytest = y[test_idx]  \n",
        "  #fit model for current fold  \n",
        "  model.fit(xtrain, ytrain)  \n",
        "  #create predictions  \n",
        "  preds = model.predict(xtest)  \n",
        "  #calculate and append accuracy  \n",
        "  fold_accuracy = metrics.accuracy_score(  ytest,  preds  )  \n",
        "  accuracies.append(fold_accuracy)  \n",
        "  #return negative accuracy  \n",
        "  return -1 * np.mean(accuracies) \n",
        "\n",
        "if  __name__  ==  \"__main__\": \n",
        "  #read the training data  \n",
        "  df = pd.read_csv(\"../input/mobile_train.csv\")  \n",
        "  #features are all columns without price_range  \n",
        "  #note that there is no id column in this dataset  \n",
        "  #here we have training features  \n",
        "  X = df.drop(\"price_range\", axis=1).values  \n",
        "  #and the targets  \n",
        "  y = df.price_range.values  \n",
        "  #define a parameter space  \n",
        "  param_space = [  \n",
        "    #max_depth is an integer between 3 and 10  \n",
        "    space.Integer(3, 15, name=\"max_depth\"),  \n",
        "    #n_estimators is an integer between 50 and 1500  \n",
        "    space.Integer(100, 1500, name=\"n_estimators\"),  \n",
        "    #criterion is a category. here we define list of categories  \n",
        "    space.Categorical([\"gini\", \"entropy\"], name=\"criterion\"),  \n",
        "    #you can also have Real numbered space and define a  \n",
        "    #distribution you want to pick it from  \n",
        "    space.Real(0.01, 1, prior=\"uniform\", name=\"max_features\")  ]  \n",
        "    #make a list of param names  \n",
        "    #this has to be same order as the search space  \n",
        "    #inside the main function  \n",
        "    param_names = [  \"max_depth\",  \"n_estimators\",  \"criterion\",  \"max_features\"  ]  \n",
        "    #by using functools partial, i am creating a  \n",
        "    #new function which has same parameters as the  \n",
        "    #optimize function except for the fact that  \n",
        "    #only one param, i.e. the \"params\" parameter is  \n",
        "    #required. this is how gp_minimize expects the  \n",
        "    #optimization function to be. you can get rid of this  \n",
        "    #by reading data inside the optimize function or by  \n",
        "    #defining the optimize function here.  \n",
        "    optimization_function = partial(  optimize,  param_names=param_names,x=X,y=Y)\n",
        "    #now we call gp_minimize from scikit-optimize  \n",
        "    #gp_minimize uses bayesian optimization for  \n",
        "    #minimization of the optimization function.  \n",
        "    #we need a space of parameters, the function itself,  \n",
        "    #the number of calls/iterations we want to have  \n",
        "    result = gp_minimize(  optimization_function,  dimensions=param_space,  n_calls=15,  n_random_starts=10,  verbose=10  )  \n",
        "    #create best params dict and print it  \n",
        "    best_params = dict(  zip(  param_names,  result.x  )  )  \n",
        "    print(best_params) \n",
        "\n",
        "  from skopt.plots import plot_convergence  \n",
        "  plot_convergence(result) \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uqb6fQX8Nc7C",
        "colab_type": "text"
      },
      "source": [
        "**OUTPUT**\n",
        "\n",
        "\n",
        "Iteration No: 14 started. Searching for the next optimal point.  Iteration No: 14 ended. Search finished for the next optimal point.  Time taken: 4.7793  Function value obtained: -0.9075  Current minimum: -0.9075  Iteration No: 15 started. Searching for the next optimal point.  Iteration No: 15 ended. Search finished for the next optimal point.  Time taken: 49.4186  Function value obtained: -0.9075  Current minimum: -0.9075  {'max_depth': 12, 'n_estimators': 100, 'criterion': 'entropy',  'max_features': 1.0}  \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SuQjMmSDUSNP",
        "colab_type": "text"
      },
      "source": [
        "There are many libraries available that offer hyperparameter optimization. scikitoptimize is one such library that you can use. Another useful library for  hyperparameter optimization is hyperopt. hyperopt uses Tree-structured Parzen  Estimator (TPE) to find the most optimal parameters."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3vBhItl4Nlnw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#rf_hyperopt.py  \n",
        "import numpy as np  \n",
        "import pandas as pd  \n",
        "from functools import partial  \n",
        "from sklearn import ensemble  \n",
        "from sklearn import metrics  \n",
        "from sklearn import model_selection  \n",
        "from hyperopt import hp, fmin, tpe, Trials \n",
        "from hyperopt.pyll.base import scope  \n",
        "def optimize(params, x, y):  \n",
        "  \"\"\"  The main optimization function.  This function takes all the arguments from the search space  and training features and targets. \n",
        "  It then initializes  the models by setting the chosen parameters and runs  cross-validation and returns a negative accuracy score  \n",
        "  :param params: dict of params from hyperopt  :param x: training data  :param y: labels/targets  :return: negative accuracy after 5 folds  \"\"\"  \n",
        "  #initialize model with current parameters  \n",
        "  model = ensemble.RandomForestClassifier(**params)  \n",
        "  #initialize stratified k-fold  \n",
        "  kf = model_selection.StratifiedKFold(n_splits=5)    \n",
        "  \n",
        "  #return negative accuracy  \n",
        "  return -1 * np.mean(accuracies)  \n",
        "if  __name__  ==  \"__main__\":  \n",
        "  #read the training data  \n",
        "  df = pd.read_csv(\"../input/mobile_train.csv\")  \n",
        "  #features are all columns without price_range  \n",
        "  #note that there is no id column in this dataset  \n",
        "  #here we have training features  \n",
        "  X = df.drop(\"price_range\", axis=1).values  \n",
        "  #and the targets  \n",
        "  y = df.price_range.values  \n",
        "  #define a parameter space  \n",
        "  #now we use hyperopt  \n",
        "  param_space = {  \n",
        "      #quniform gives round(uniform(low, high)/ q) * q  \n",
        "      #we want int values for depth and estimators\n",
        "      \"max_depth\": scope.int(hp.quniform(\"max_depth\", 1, 15, 1)),  \"n_estimators\": scope.int(  hp.quniform(\"n_estimators\", 100, 1500, 1)  ),  \n",
        "      #choice chooses from a list of values  \n",
        "      \"criterion\": hp.choice(\"criterion\", [\"gini\", \"entropy\"]),  \n",
        "      #uniform chooses a value between two values  \n",
        "      \"max_features\": hp.uniform(\"max_features\", 0, 1)  }  \n",
        "    \n",
        "    #partial function  \n",
        "    optimization_function = partial(  optimize,  x=X,  y=y  )  \n",
        "    \n",
        "    #initialize trials to keep logging information  \n",
        "    trials = Trials()  \n",
        "    #run hyperopt  \n",
        "    hopt = fmin(  fn=optimization_function,  space=param_space,  algo=tpe.suggest,  max_evals=15,  trials=trials  )  \n",
        "    print(hopt) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AG2kfHZlVb2Z",
        "colab_type": "text"
      },
      "source": [
        "As you can see, this is not very different from the previous code. You have to define  the parameter space in a different format, and you also need to change the actual  optimization part by using hyperopt instead of gp_minimize. The results are quite  good! \n",
        "\n",
        "\n",
        "❯ python rf_hyperopt.py  100%|██████████████████| 15/15 [04:38<00:00, 18.57s/trial, best loss: -  0.9095000000000001]  \n",
        "\n",
        "{'criterion': 1, 'max_depth': 11.0, 'max_features': 0.821163568049807,  'n_estimators': 806.0}  \n",
        "\n",
        "\n",
        "\n",
        "We get an accuracy which is a little better than before and a set of parameters that  we can use. Please note that criterion is 1 in the final result. This implies that choice  1 was selected, i.e., entropy. The ways of tuning hyperparameters described above  are the most common, and these will work with almost all models: linear regression,  logistic regression, tree-based methods, gradient boosting models such as xgboost,  lightgbm, and even neural networks! "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WUBM77rJWVx2",
        "colab_type": "text"
      },
      "source": [
        "Although, these methods exist, to learn, one must start with tuning the hyperparameters manually, i.e., by hand. Hand tuning will help you learn the basics, for  example, in gradient boosting, when you increase the depth, you should reduce the  learning rate. It won’t be possible to learn this if you use automated tools. Refer to  the following table to know what to tune. RS* implies random search should be  better.  Once you get better with hand-tuning the parameters, you might not even need any  automated hyper-parameter tuning. When you create large models or introduce a  lot of features, you also make it susceptible to overfitting the training data.\n",
        "\n",
        " To avoid  overfitting, you need to introduce noise in training data features or penalize the cost  function. This penalization is called regularization and helps with generalizing the  model. In linear models, the most common types of regularizations are L1 and L2.  L1 is also known as Lasso regression and L2 as Ridge regression. When it comes  to neural networks, we use dropouts, the addition of augmentations, noise, etc. to  regularize our models. Using hyper-parameter optimization, you can also find the  correct penalty to use. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X3IN7DkLZeqm",
        "colab_type": "text"
      },
      "source": [
        "**Approach to Image classification and segmentation**\n",
        "\n",
        "Image is nothing  but a matrix of numbers. The computer cannot see the images as humans do. It only  looks at numbers, and that’s what the images are. A grayscale image is a twodimensional matrix with values ranging from 0 to 255. 0 is black, 255 is white and  in between you have all shades of grey. Previously, when there was no deep learning  (or when deep learning was not popular), people used to look at pixels. Each pixel  was a feature. You can do this easily in Python. Just read the grayscale image using  OpenCV or Python-PIL, convert to a numpy array and ravel (flatten) the matrix. If  you are dealing with RGB images, then you have three matrices instead of one. But  the idea remains the same. \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4tmwc4UbZxBu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np  \n",
        "import matplotlib.pyplot as plt  \n",
        "#generate random numpy array with values from 0 to 255  \n",
        "#and a size of 256x256  \n",
        "random_image = np.random.randint(0, 256, (256, 256))  \n",
        "#initialize plot  \n",
        "plt.figure(figsize=(7, 7))  \n",
        "#show grayscale image, nb: cmap, vmin and vmax  p\n",
        "lt.imshow(random_image, cmap='gray', vmin=0, vmax=255)  \n",
        "plt.show()  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o8XmKVMwZukY",
        "colab_type": "text"
      },
      "source": [
        "The code above generates a random matrix using numpy. This matrix consists of  values ranging from 0 to 255 (included) and is of size 256x256 (also known as  pixels). \n",
        "\n",
        "As you can see that the ravelled version is nothing but a vector of size M, where M  = N * N. In this case, this vector is of the size 256 * 256 = 65536.  Now, if we go ahead and do it for all the images in our dataset, we have 65536  features for each sample. We can now quickly build a decision tree model or  random forest or SVM-based model on this data. The models will look at pixel  values and would try to separate positive samples from negative samples (in case  of a binary classification problem). \n",
        "\n",
        "\n",
        "All of you must have heard about the cats vs dogs problem. It's a classic one. But  let's try something different. If you remember, at the beginning of the chapter on  evaluation metrics, I introduced you to a dataset of pneumothorax images. So, let’s  try building a model to detect if an X-ray image of a lung has pneumothorax or not.  That is, a (not so) simple binary classification. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z6UhLdBytmKO",
        "colab_type": "text"
      },
      "source": [
        "The dataset  consists of 10675 unique images and 2379 have pneumothorax (note that these  numbers are after some cleaning of data and thus do not match original dataset). As  a data doctor would say: this is a classic case of skewed binary classification.  \n",
        "\n",
        "Therefore, we choose the evaluation metric to be AUC and go for a stratified k-fold  cross-validation scheme.  You can flatten out the features and try some classical methods like SVM, RF for  doing classification, which is perfectly fine, but it won't get you anywhere near state  of the art. Also, the images are of size 1024x1024. It’s going to take a long time to  train a model on this dataset. For what it’s worth, let’s try building a simple random  forest model on this data. Since the images are grayscale, we do not need to do any  kind of conversion. We will resize the images to 256x256 to make them smaller and  use AUC as a metric"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m7fyeir9tovG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os  \n",
        "import numpy as np  \n",
        "import pandas as pd  \n",
        "from PIL import Image  \n",
        "from sklearn import ensemble  \n",
        "from sklearn import metrics  \n",
        "from sklearn import model_selection  \n",
        "from tqdm import tqdm  \n",
        "\n",
        "def create_dataset(training_df, image_dir):  \n",
        "  \"\"\"  This function takes the training dataframe  and outputs training array and labels  \n",
        "  :param training_df: dataframe with ImageId, \n",
        "  Target columns  :param image_dir: location of images (folder), string  \n",
        "  :return: X, y (training array with features and labels) \n",
        "  \"\"\"\n",
        "  #create empty list to store image vectors  \n",
        "  images = []  \n",
        "  #create empty list to store targets  \n",
        "  targets = []  \n",
        "  #loop over the dataframe  \n",
        "  for index, row in tqdm(  training_df.iterrows(),  total=len(training_df),  desc=\"processing images\"  ):  \n",
        "    #get image id  \n",
        "    image_id = row[\"ImageId\"]  \n",
        "    #create image path  \n",
        "    image_path = os.path.join(image_dir, image_id)  \n",
        "    #open image using PIL  \n",
        "    image = Image.open(image_path + \".png\")  \n",
        "    #resize image to 256x256. we use bilinear resampling  \n",
        "    image = image.resize((256, 256), resample=Image.BILINEAR)  \n",
        "    #convert image to array  image = np.array(image)  \n",
        "    #ravel  image = image.ravel()  \n",
        "    #append images and targets lists  \n",
        "    images.append(image)  \n",
        "    targets.append(int(row[\"target\"]))  \n",
        "    #convert list of list of images to numpy array  \n",
        "    images = np.array(images)  \n",
        "    #print size of this array  \n",
        "    print(images.shape)  \n",
        "    return images, targets  \n",
        "    \n",
        "  if  __name__  ==  \"__main__\":  \n",
        "    csv_path = \"/home/abhishek/workspace/siim_png/train.csv\"  \n",
        "    image_path = \"/home/abhishek/workspace/siim_png/train_png/\"  \n",
        "    #read CSV with imageid and target columns  \n",
        "    df = pd.read_csv(csv_path)  \n",
        "    #we create a new column called kfold and fill it with -1  \n",
        "    df[\"kfold\"] = -1  \n",
        "    #the next step is to randomize the rows of the data  \n",
        "    df = df.sample(frac=1).reset_index(drop=True)\n",
        "    y = df.target.values  \n",
        "    #initiate the kfold class from model_selection module  \n",
        "    kf = model_selection.StratifiedKFold(n_splits=5)  \n",
        "    #fill the new kfold column  \n",
        "    for f, (t_, v_) in enumerate(kf.split(X=df, y=y)):  \n",
        "      df.loc[v_, 'kfold'] = f  \n",
        "      #we go over the folds created  \n",
        "    for fold_ in range(5):  \n",
        "      #temporary dataframes for train and test  \n",
        "      train_df = df[df.kfold != fold_].reset_index(drop=True)  \n",
        "      test_df = df[df.kfold == fold_].reset_index(drop=True)  \n",
        "      #create train dataset  \n",
        "      #you can move this outside to save some computation time  \n",
        "      xtrain, ytrain = create_dataset(train_df, image_path)  \n",
        "      #create test dataset  \n",
        "      #you can move this outside to save some computation time  \n",
        "      xtest, ytest = create_dataset(test_df, image_path)  \n",
        "      #fit random forest without any modification of params  \n",
        "      clf = ensemble.RandomForestClassifier(n_jobs=-1)  \n",
        "      clf.fit(xtrain, ytrain)  \n",
        "      #predict probability of class 1  \n",
        "      preds = clf.predict_proba(xtest)[:, 1]  \n",
        "      #print results  \n",
        "      print(f\"FOLD: {fold_}\")  \n",
        "      print(f\"AUC = {metrics.roc_auc_score(ytest, preds)}\")  \n",
        "      print(\"\")  \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hChumduy1pVJ",
        "colab_type": "text"
      },
      "source": [
        "is a basic deep convolutional neural network,  but it is the foundation of many new deep nets (deep neural networks). We see that  the network in figure 3 is a convolutional neural network with five convolution  layers, two dense layers and an output layer.\n",
        "is a basic deep convolutional neural network,  but it is the foundation of many new deep nets (deep neural networks). We see that  the network in figure 3 is a convolutional neural network with five convolution  layers, two dense layers and an output layer.\n",
        "\n",
        "\n",
        "\n",
        "terms: filter and strides. Filters are nothing but twodimensional matrices which are initialized by a given function. “He initialization” which is also known “Kaiming normal initialization” is a good choice for  convolutional neural networks. It is because most modern networks use ReLU  (Rectified Linear Units) activation function and proper initialization is required to  avoid the problem of vanishing gradients (when gradients approach zero and  weights of network do not change). This filter is convolved with the image.  Convolution is nothing but a summation of elementwise multiplication (crosscorrelation) between the filter and the pixels it is currently overlapping in a given  image.\n",
        "\n",
        "\n",
        "Stride is a useful concept even in natural language processing, e.g. in question and  answering systems when you have to filter answer from a large text corpus. When  we are exhausted horizontally, we move the filter by the same stride downwards  vertically, starting from left again. Figure 4 also shows a filter going outside the  image. In these cases, it’s not possible to calculate the convolution. So, we skip it.  If you don’t want to skip it, you will need to pad the image. It must also be noted  that convolution will decrease the size of the image. Padding is also a way to keep  the size of the image the same.we have a 3x3 filter which is moving  with a stride of 1. Size of the original image is 6x6, and we have added padding of  1. The padding of 1 means increasing the size of the image by adding zero pixels  on each side once. In this case, the resulting image will be of the same size as the  input image, i.e. 6x6.\n",
        "\n",
        "In dilation, we expand the filter by N-1, where N is the value of dilation rate or  simply known as dilation. In this kind of kernel with dilation, you skip some pixels  in each convolution. This is particularly effective in segmentation tasks. Please note  that we have only talked about 2-dimensional convolutions. There are 1-d  convolutions too and also in higher dimensions. All work on the same underlying  concept. \n",
        "\n",
        "Next comes max-pooling. Max pooling is nothing but a filter which returns max.  So, instead of convolution, we are extracting the max value of pixels. Similarly,  average pooling or mean-pooling returns mean of pixels. They are used in the  same way as the convolutional kernel. Pooling is faster than convolution and is a  way to down-sample the image. Max pooling detects edges and average pooling  smoothens the image. \n",
        "\n",
        "\n",
        "PyTorch provides an intuitive and easy way to implement deep  neural networks, and you don’t need to care about back-propagation. We define the  network in a python class and a forward function that tells PyTorch how the layers  are connected to each other. In PyTorch, the image notation is BS, C, H, W, where,  BS is the batch size, C channels, H is height and W is the width. Let’s see how  AlexNet is implemented in PyTorch. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "peuBNCye896W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch  \n",
        "import torch.nn as nn  \n",
        "import torch.nn.functional as F  \n",
        "class AlexNet(nn.Module):  \n",
        "  def __init__(self):  super(AlexNet, self).__init__()  \n",
        "    #convolution part  \n",
        "    self.conv1 = nn.Conv2d(  in_channels=3,  out_channels=96,  kernel_size=11,  stride=4,  padding=0  )  \n",
        "    self.pool1 = nn.MaxPool2d(kernel_size=3, stride=2)  \n",
        "    self.conv2 = nn.Conv2d(  in_channels=96,  out_channels=256,  kernel_size=5,  stride=1,  padding=2  )  \n",
        "    self.pool2 = nn.MaxPool2d(kernel_size=3, stride=2)  \n",
        "    self.conv3 = nn.Conv2d(  in_channels=256,  out_channels=384,  kernel_size=3,  stride=1,  padding=1  )  \n",
        "    self.conv4 = nn.Conv2d(in_channels=384,out_channels=384,  kernel_size=3, stride=1,padding=1)\n",
        "    self.conv5 = nn.Conv2d(in_channels=384, out_channels=256,  kernel_size=3,  stride=1,  padding=1  )  \n",
        "    self.pool3 = nn.MaxPool2d(kernel_size=3, stride=2)  \n",
        "    #dense part  \n",
        "    self.fc1 = nn.Linear(  in_features=9216,  out_features=4096  )  \n",
        "    self.dropout1 = nn.Dropout(0.5)  \n",
        "    self.fc2 = nn.Linear(  in_features=4096,  out_features=4096  )  \n",
        "    self.dropout2 = nn.Dropout(0.5)  \n",
        "    self.fc3 = nn.Linear(  in_features=4096,  out_features=1000  )  \n",
        "  \n",
        "  def forward(self, image):  \n",
        "    #get the batch size, channels, height and width  \n",
        "    #of the input batch of images  \n",
        "    #original size: (bs, 3, 227, 227)  \n",
        "    bs, c, h, w = image.size()  \n",
        "    x = F.relu(self.conv1(image)) #size: (bs, 96, 55, 55)  \n",
        "    x = self.pool1(x) #size: (bs, 96, 27, 27)  \n",
        "    x = F.relu(self.conv2(x)) #size: (bs, 256, 27, 27)  \n",
        "    x = self.pool2(x) #size: (bs, 256, 13, 13)  \n",
        "    x = F.relu(self.conv3(x)) #size: (bs, 384, 13, 13)  \n",
        "    x = F.relu(self.conv4(x)) #size: (bs, 384, 13, 13)  \n",
        "    x = F.relu(self.conv5(x)) #size: (bs, 256, 13, 13)  \n",
        "    x = self.pool3(x) #size: (bs, 256, 6, 6)  \n",
        "    x = x.view(bs, -1) #size: (bs, 9216)  \n",
        "    x = F.relu(self.fc1(x)) #size: (bs, 4096)  \n",
        "    x = self.dropout1(x) #size: (bs, 4096)  \n",
        "    #dropout does not change size  \n",
        "    #dropout is used for regularization  \n",
        "    #0.3 dropout means that only 70% of the nodes  \n",
        "    #of the current layer are used for the next layer  \n",
        "    x = F.relu(self.fc2(x)) #size: (bs, 4096)  \n",
        "    x = self.dropout2(x) #size: (bs, 4096)  \n",
        "    x = F.relu(self.fc3(x)) #size: (bs, 1000)  \n",
        "    #1000 is number of classes in ImageNet Dataset  \n",
        "    #softmax is an activation function that converts  \n",
        "    #linear output to probabilities that add up to 1\n",
        "    #for each sample in the batch  \n",
        "    x = torch.softmax(x, axis=1) #size: (bs, 1000)  \n",
        "    return x \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mHO_cAO6-3a1",
        "colab_type": "text"
      },
      "source": [
        "When you have a 3x227x227 image, and you apply a convolutional filter of size  11x11, it means that you are applying a filter of size 11x11x3 and convolving it  with an image of size 227x227x3. So, now, you need to think in 3 dimensions  instead of 2. The number of output channels is the number of different convolutional  filters of the same size applied to the image individually. So, in the first  convolutional layer, the input channels are 3, which is the original input, i.e. the R,  G, B channels. PyTorch’s torchvision offers many different models like AlexNet,  and it must be noted that this implementation of AlexNet is not the same as  torchvision’s. Torchvision’s implementation of AlexNet is a modified AlexNet  from another paper: Krizhevsky, A. One weird trick for parallelizing convolutional  neural networks. CoRR, abs/1404.5997, 2014.  \n",
        "\n",
        "\n",
        "You can design your own convolutional neural networks for your task, and many  times it is a good idea to start from something on your own. Let’s build a network  to classify images from our initial dataset of this chapter into categories of having  pneumothorax or not. But first, let’s prepare some files. The first step would be to  create a folds file, i.e. train.csv but with a new column kfold. We will create five  folds. Since I have shown how to do this for different datasets in this book, I will  skip this part and leave it an exercise for you. For PyTorch based neural networks,  we need to create a dataset class. The objective of the dataset class is to return an  item or sample of data. This sample of data should consist of everything you need  in order to train or evaluate your model. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JZsc20M9Wb1C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#dataset.py  \n",
        "import torch  \n",
        "import numpy as np  \n",
        "from PIL import Image  \n",
        "from PIL import ImageFile  \n",
        "#sometimes, you will have images without an ending bit  \n",
        "#this takes care of those kind of (corrupt) images  \n",
        "ImageFile.LOAD_TRUNCATED_IMAGES = True  \n",
        "\n",
        "class ClassificationDataset:\n",
        "  \"\"\"  A general classification dataset class that you can use for all  kinds of image classification problems. \n",
        "  For example,  binary classification, multi-class, multi-label classification  \"\"\"  \n",
        "  \n",
        "  def __init__(  self,  image_paths,  targets,  resize=None,  augmentations=None  ):  \n",
        "    \"\"\"  :param image_paths: list of path to images  \n",
        "    :param targets: numpy array  \n",
        "    :param resize: tuple, e.g. (256, 256), resizes image \n",
        "    if not None  :param augmentations: albumentation augmentations  \"\"\"  \n",
        "    self.image_paths = image_paths  \n",
        "    self.targets = targets  \n",
        "    self.resize = resize  \n",
        "    self.augmentations = augmentations  \n",
        "  def __len__(self):  \n",
        "    \"\"\"  Return the total number of samples in the dataset  \"\"\"  \n",
        "    return len(self.image_paths)  \n",
        "  def __getitem__(self, item):  \n",
        "    \"\"\"  For a given \"item\" index, return everything we need  to train a given model  \"\"\"  \n",
        "    #use PIL to open the image  \n",
        "    image = Image.open(self.image_paths[item])  \n",
        "    #convert image to RGB, we have single channel images  \n",
        "    image = image.convert(\"RGB\")  \n",
        "    #grab correct targets  \n",
        "    targets = self.targets[item]  \n",
        "    #resize if needed  \n",
        "    if self.resize is not None:  \n",
        "      image = image.resize(  (self.resize[1], self.resize[0]),  resample=Image.BILINEAR  )\n",
        "      #convert image to numpy array  \n",
        "      image = np.array(image)  \n",
        "      #if we have albumentation augmentations  \n",
        "      #add them to the image  \n",
        "    if self.augmentations is not None:  \n",
        "      augmented = self.augmentations(image=image)  \n",
        "      image = augmented[\"image\"]  \n",
        "    #pytorch expects CHW instead of HWC  \n",
        "    image = np.transpose(image, (2, 0, 1)).astype(np.float32)  \n",
        "    #return tensors of image and targets  \n",
        "    #take a look at the types!  \n",
        "    #for regression tasks,  \n",
        "    #dtype of targets will change to torch.float  \n",
        "    return {  \"image\": torch.tensor(image, dtype=torch.float),  \n",
        "            \"targets\": torch.tensor(targets, dtype=torch.long),  }   "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AOnTzoaWAtuJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#engine.py  \n",
        "import torch  \n",
        "import torch.nn as nn  \n",
        "from tqdm import tqdm  \n",
        "def train(data_loader, model, optimizer, device):  \n",
        "  \"\"\"  This function does training for one epoch  \n",
        "  :param data_loader: this is the pytorch dataloader  \n",
        "  :param model: pytorch model  :param optimizer: optimizer, for e.g. adam, sgd, etc  \n",
        "  :param device: cuda/cpu  \"\"\"  \n",
        "  #put the model in train mode  \n",
        "  model.train()  \n",
        "  #go over every batch of data in data loader  \n",
        "  for data in data_loader:\n",
        "    #remember, we have image and targets  \n",
        "    #in our dataset class  \n",
        "    inputs = data[\"image\"]  \n",
        "    targets = data[\"targets\"] \n",
        "  #move inputs/targets to cuda/cpu device  \n",
        "  inputs = inputs.to(device, dtype=torch.float)  \n",
        "  targets = targets.to(device, dtype=torch.float)  \n",
        "  #zero grad the optimizer  \n",
        "  optimizer.zero_grad()  \n",
        "  #do the forward step of model  \n",
        "  outputs = model(inputs)  \n",
        "  #calculate loss  \n",
        "  loss = nn.BCEWithLogitsLoss()(outputs, targets.view(-1, 1))  \n",
        "  #backward step the loss  \n",
        "  loss.backward()  \n",
        "  #step optimizer  \n",
        "  optimizer.step()  \n",
        "  #if you have a scheduler, you either need to  \n",
        "  #step it here or you have to step it after  \n",
        "  #the epoch. here, we are not using any learning  \n",
        "  #rate scheduler  \n",
        "  def evaluate(data_loader, model, device):  \n",
        "    \"\"\"  \n",
        "    This function does evaluation for one epoch  \n",
        "    :param data_loader: this is the pytorch dataloader  \n",
        "    :param model: pytorch model  :param device: cuda/cpu  \"\"\"  \n",
        "    \n",
        "    #put model in evaluation mode  \n",
        "    model.eval()  \n",
        "    #init lists to store targets and outputs  \n",
        "    final_targets = []  \n",
        "    final_outputs = []  \n",
        "    #we use no_grad context  \n",
        "    with torch.no_grad():  \n",
        "      for data in data_loader:  \n",
        "        inputs = data[\"image\"]  \n",
        "        targets = data[\"targets\"]  \n",
        "        inputs = inputs.to(device, dtype=torch.float)  \n",
        "        targets = targets.to(device, dtype=torch.float)\n",
        "        #do the forward step to generate prediction  \n",
        "        output = model(inputs)  \n",
        "        #convert targets and outputs to lists  \n",
        "        targets = targets.detach().cpu().numpy().tolist()  \n",
        "        output = output.detach().cpu().numpy().tolist()  \n",
        "        #extend the original list  \n",
        "        final_targets.extend(targets)  \n",
        "        final_outputs.extend(output)  \n",
        "    #return final output and final targets  \n",
        "    return final_outputs, final_targets "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TRIekuBxCfYc",
        "colab_type": "text"
      },
      "source": [
        "Once we have engine.py, we are ready to create a new file: model.py. model.py will  consist of our model. It’s a good idea to keep it separate because that allows us to  easily experiment with different models and different architectures. A PyTorch  library called pretrainedmodels has a lot of different model architectures, such as  AlexNet, ResNet, DenseNet, etc. There are different model architectures which  have been trained on large image dataset called ImageNet. We can use them with  their weights after training on ImageNet, and we can also use them without these  weights. If we train without the ImageNet weights, it means our network is learning  everything from scratch."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iPUSvtKGCgN9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#model.py  \n",
        "#ALexnet\n",
        "import torch.nn as nn  \n",
        "import pretrainedmodels  \n",
        "def get_model(pretrained):  \n",
        "  if pretrained:  \n",
        "    model = pretrainedmodels.__dict__[\"alexnet\"]\n",
        "    (  pretrained='imagenet'  )  \n",
        "  else:  \n",
        "    model = pretrainedmodels.__dict__[\"alexnet\"]\n",
        "    (  pretrained=None  )  \n",
        "  #print the model here to know whats going on.  \n",
        "  model.last_linear = nn.Sequential(  nn.BatchNorm1d(4096),  nn.Dropout(p=0.25),  nn.Linear(in_features=4096, out_features=2048), \n",
        "                                    nn.ReLU(),  nn.BatchNorm1d(2048, eps=1e-05, momentum=0.1),  nn.Dropout(p=0.5),  nn.Linear(in_features=2048, out_features=1),)  \n",
        "  return model\n",
        "\n",
        " "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wkz7zKICJqOT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#train.py  \n",
        "import os  \n",
        "import pandas as pd  \n",
        "import numpy as np  \n",
        "import albumentations  \n",
        "import torch  \n",
        "from sklearn import metrics  \n",
        "from sklearn.model_selection \n",
        "import train_test_split  \n",
        "import dataset  \n",
        "import engine  \n",
        "from model import get_model  \n",
        "if  __name__  ==  \"__main__\":  \n",
        "  #location of train.csv and train_png folder  \n",
        "  #with all the png images  \n",
        "  data_path = \"/home/abhishek/workspace/siim_png/\"  \n",
        "  #cuda/cpu device  \n",
        "  device = \"cuda\"  \n",
        "  #let's train for 10 epochs  \n",
        "  epochs = 10  \n",
        "  #load the dataframe  \n",
        "  df = pd.read_csv(os.path.join(data_path, \"train.csv\"))  \n",
        "  #fetch all image ids  \n",
        "  images = df.ImageId.values.tolist() \n",
        "\n",
        "  #a list with image locations  \n",
        "  images = [  os.path.join(data_path, \"train_png\", i + \".png\") for i in images  ]  \n",
        "  #binary targets numpy array  \n",
        "  targets = df.target.values  \n",
        "  #fetch out model, we will try both pretrained  \n",
        "  #and non-pretrained weights  \n",
        "  model = get_model(pretrained=True)  \n",
        "  #move model to device  \n",
        "  model.to(device)  \n",
        "  #mean and std values of RGB channels for imagenet dataset  \n",
        "  #we use these pre-calculated values when we use weights  \n",
        "  #from imagenet.  \n",
        "  #when we do not use imagenet weights, we use the mean and  \n",
        "  #standard deviation values of the original dataset  \n",
        "  #please note that this is a separate calculation  \n",
        "  mean = (0.485, 0.456, 0.406)  \n",
        "  std = (0.229, 0.224, 0.225)  \n",
        "  #albumentations is an image augmentation library  \n",
        "  #that allows you to do many different types of image  \n",
        "  #augmentations. here, i am using only normalization  \n",
        "  #notice always_apply=True. we always want to apply  \n",
        "  #normalization  \n",
        "  aug = albumentations.Compose(  [  albumentations.Normalize(  mean, std, max_pixel_value=255.0, always_apply=True  )  ]  )  \n",
        "  \n",
        "  #instead of using kfold, i am using train_test_split  \n",
        "  #with a fixed random state  \n",
        "  train_images, valid_images, train_targets, valid_targets =  train_test_split(  images, targets, stratify=targets, random_state=42  )  \n",
        "  #fetch the ClassificationDataset class  \n",
        "  train_dataset = dataset.ClassificationDataset(  image_paths=train_images,targets=train_targets,  resize=(227, 227),  augmentations=aug,  )  \n",
        "  #torch dataloader creates batches of data  \n",
        "  #from classification dataset class  \n",
        "  train_loader = torch.utils.data.DataLoader(  train_dataset, batch_size=16, shuffle=True, num_workers=4  )  \n",
        "  #same for validation data  \n",
        "  valid_dataset = dataset.ClassificationDataset(  image_paths=valid_images,  targets=valid_targets,  resize=(227, 227),  augmentations=aug,  )  \n",
        "  valid_loader = torch.utils.data.DataLoader(  valid_dataset, batch_size=16, shuffle=False, num_workers=4  )  \n",
        "  #simple Adam optimizer  \n",
        "  optimizer = torch.optim.Adam(model.parameters(), lr=5e-4)  \n",
        "  #train and print auc score \n",
        "  for all epochs  for epoch in range(epochs):  \n",
        "    engine.train(train_loader, model, optimizer, device=device)  \n",
        "    predictions, valid_targets = engine.evaluate(  valid_loader, model, device=device  )  \n",
        "    roc_auc = metrics.roc_auc_score(valid_targets, predictions)  \n",
        "    print(  f\"Epoch={epoch}, Valid ROC AUC={roc_auc}\"  ) \n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s-bZUQDfLAVJ",
        "colab_type": "text"
      },
      "source": [
        "The good thing about  pretrained models is that we can try many different models easily. Let’s try  resnet18 with pretrained weights.  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_I_NBEhTKzFC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#model.py  \n",
        "import torch.nn as nn  \n",
        "import pretrainedmodels  \n",
        "def get_model(pretrained):  \n",
        "  if pretrained:  \n",
        "    model = pretrainedmodels.__dict__[\"resnet18\"](  pretrained='imagenet'  )  \n",
        "  else:  \n",
        "    model = pretrainedmodels.__dict__[\"resnet18\"](  pretrained=None  )  \n",
        "  #print the model here to know whats going on.  \n",
        "  model.last_linear = nn.Sequential(  nn.BatchNorm1d(512),  nn.Dropout(p=0.25),  nn.Linear(in_features=512, out_features=2048),\n",
        "                                    nn.ReLU(),  nn.BatchNorm1d(2048, eps=1e-05, momentum=0.1),  nn.Dropout(p=0.5),  nn.Linear(in_features=2048, out_features=1),  )  \n",
        "  return model  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "11xUDL_3Ml1e",
        "colab_type": "text"
      },
      "source": [
        "This model seems to perform the best. However, you might be able to tune the  different parameters and image size in AlexNet to get a better score. Using  augmentations will improve the score further. Optimising deep neural networks is  difficult but not impossible. Choose Adam optimizer, use a low learning rate,  reduce learning rate on a plateau of validation loss, try some augmentations, try  preprocessing the images (e.g. cropping if needed, this can also be considered preprocessing), change the batch size, etc. There’s a lot that you can do to optimize  your deep neural network. \n",
        "\n",
        "\n",
        "This model seems to perform the best. However, you might be able to tune the  different parameters and image size in AlexNet to get a better score. Using  augmentations will improve the score further. Optimising deep neural networks is  difficult but not impossible. Choose Adam optimizer, use a low learning rate,  reduce learning rate on a plateau of validation loss, try some augmentations, try  preprocessing the images (e.g. cropping if needed, this can also be considered preprocessing), change the batch size, etc. There’s a lot that you can do to optimize  your deep neural network.  ResNet is an architecture much more complicated compared to AlexNet. ResNet  stands for Residual Neural Network and was introduced by K. He, X. Zhang, S. Ren  and J. Sun in the paper, deep residual learning for image recognition, in 2015.  ResNet consists of residual blocks that transfer the knowledge from one layer to  further layers by skipping some layers in between. These kinds of connections of  layers are known as skip-connections since we are skipping one or more layers.  Skip-connections help with the vanishing gradient issue by propagating the  gradients to further layers. This allows us to train very large convolutional neural  networks without loss of performance.Usually, the training loss increases at a given point if we are using a large neural network, but that can be prevented by using  skip-connections."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qL5KLSU8Ny4x",
        "colab_type": "text"
      },
      "source": [
        "A residual block is quite simple to understand. You take the output from a layer,  skip some layers and add that output to a layer further in the network. The dotted  lines mean that the input shape needs to be adjusted as max-pooling is being used  and use of max-pooling changes the size of the output. \n",
        "\n",
        " ResNet comes in many different variations: 18, 34, 50, 101 and 152 layers and all  of them are available with weights pre-trained on ImageNet dataset. These days  pretrained models work for (almost) everything but make sure that you start with  smaller models, for example, begin with resnet-18 rather than resnet-50. Some other  ImageNet pre-trained models include:  \n",
        "- Inception  \n",
        "- DenseNet (different variations) \n",
        "- NASNet  \n",
        "- PNASNet \n",
        "- VGG  \n",
        "- Xception  \n",
        "- ResNeXt  \n",
        "- EfficientNet, etc. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9fz2lE18Okf_",
        "colab_type": "text"
      },
      "source": [
        "Segmentation is a task which is quite popular in computer vision. In a segmentation  task, we try to remove/extract foreground from background. Foreground and background can have different definitions. We can also say that it is a pixel-wise  classification task in which your job is to assign a class to each pixel in a given  image. The pneumothorax dataset that we are working on is, in fact, a segmentation  task. In this task, given the chest radiographic images, we are required to segment  pneumothorax. The most popular model used for segmentation tasks is U-Net.\n",
        "\n",
        "\n",
        "background can have different definitions. We can also say that it is a pixel-wise  classification task in which your job is to assign a class to each pixel in a given  image. The pneumothorax dataset that we are working on is, in fact, a segmentation  task. In this task, given the chest radiographic images, we are required to segment  pneumothorax. The most popular model used for segmentation tasks is U-Net.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hcg68AbzPF07",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#simple_unet.py  \n",
        "import torch  \n",
        "import torch.nn as nn  \n",
        "from torch.nn import functional as F  \n",
        "def double_conv(in_channels, out_channels):  \n",
        "  \"\"\"  This function applies two convolutional layers  each followed by a ReLU activation function  \n",
        "  :param in_channels: number of input channels  :param out_channels: number of output channels  \n",
        "  :return: a down-conv layer  \"\"\"  \n",
        "  conv = nn.Sequential(  nn.Conv2d(in_channels, out_channels, kernel_size=3),  \n",
        "                       nn.ReLU(inplace=True),  nn.Conv2d(out_channels, out_channels, kernel_size=3),  nn.ReLU(inplace=True)  )  \n",
        "  return conv  \n",
        "def crop_tensor(tensor, target_tensor):\n",
        "  \"\"\"  Center crops a tensor to size of a given target tensor size  Please note that this function is applicable only to  this \n",
        "  implementation of unet. There are a few assumptions  in this implementation that might not be applicable to all  networks \n",
        "  and all other use-cases.  Both tensors are of shape (bs, c, h, w)  :param tensor: a tensor that needs to be cropped  \n",
        "  :param target_tensor: target tensor of smaller size  :return: cropped tensor  \"\"\"  \n",
        "  target_size = target_tensor.size()[2]  \n",
        "  tensor_size = tensor.size()[2]  \n",
        "  delta = tensor_size - target_size  \n",
        "  delta = delta// 2  \n",
        "  return tensor[  :,  :,  delta:tensor_size - delta,  delta:tensor_size - delta  ]  \n",
        "  \n",
        "class UNet(nn.Module):  \n",
        "  def __init__(self):  \n",
        "    super(UNet, self).__init__()  \n",
        "    \n",
        "    #we need only one max_pool as it is not learned  \n",
        "    self.max_pool_2x2 = nn.MaxPool2d(kernel_size=2, stride=2)  \n",
        "    self.down_conv_1 = double_conv(1, 64)  \n",
        "    self.down_conv_2 = double_conv(64, 128)  \n",
        "    self.down_conv_3 = double_conv(128, 256)  \n",
        "    self.down_conv_4 = double_conv(256, 512)  \n",
        "    self.down_conv_5 = double_conv(512, 1024)  \n",
        "    self.up_trans_1 = nn.ConvTranspose2d(  in_channels=1024,  out_channels=512,  kernel_size=2,  stride=2  )  \n",
        "    self.up_conv_1 = double_conv(1024, 512)  \n",
        "    self.up_trans_2 = nn.ConvTranspose2d(  in_channels=512,  out_channels=256,kernel_size=2,  stride=2  )  \n",
        "    self.up_conv_2 = double_conv(512, 256)  \n",
        "    self.up_trans_3 = nn.ConvTranspose2d(  in_channels=256,  out_channels=128,  kernel_size=2,  stride=2  )  \n",
        "    self.up_conv_3 = double_conv(256, 128)  \n",
        "    self.up_trans_4 = nn.ConvTranspose2d(  in_channels=128,  out_channels=64,  kernel_size=2,  stride=2  )  \n",
        "    self.up_conv_4 = double_conv(128, 64)  \n",
        "    self.out = nn.Conv2d(  in_channels=64,  out_channels=2,  kernel_size=1  )  \n",
        "    \n",
        "  def forward(self, image):  \n",
        "    #encoder  \n",
        "    x1 = self.down_conv_1(image)  \n",
        "    x2 = self.max_pool_2x2(x1)  \n",
        "    x3 = self.down_conv_2(x2)  \n",
        "    x4 = self.max_pool_2x2(x3)  \n",
        "    x5 = self.down_conv_3(x4)  \n",
        "    x6 = self.max_pool_2x2(x5)  \n",
        "    x7 = self.down_conv_4(x6)  \n",
        "    x8 = self.max_pool_2x2(x7)  \n",
        "    x9 = self.down_conv_5(x8)  \n",
        "    #decoder  \n",
        "    x = self.up_trans_1(x9)  \n",
        "    y = crop_tensor(x7, x)  \n",
        "    x = self.up_conv_1(torch.cat([x, y], axis=1))  \n",
        "    x = self.up_trans_2(x)  \n",
        "    y = crop_tensor(x5, x)  \n",
        "    x = self.up_conv_2(torch.cat([x, y], axis=1))  \n",
        "    x = self.up_trans_3(x)\n",
        "    y = crop_tensor(x3, x)  \n",
        "    x = self.up_conv_3(torch.cat([x, y], axis=1))  \n",
        "    x = self.up_trans_4(x)  \n",
        "    y = crop_tensor(x1, x)  \n",
        "    x = self.up_conv_4(torch.cat([x, y], axis=1))  \n",
        "    #output layer  out = self.out(x)  \n",
        "    return out  \n",
        "    \n",
        "if  __name__  ==  \"__main__\":  \n",
        "  image = torch.rand((1, 1, 572, 572))  \n",
        "  model = UNet()  \n",
        "  print(model(image))    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DViEzds8iRCT",
        "colab_type": "text"
      },
      "source": [
        "We see that the encoder part of the U-Net is a nothing but a simple convolutional  network. We can, thus, replace this with any network such as ResNet. The  replacement can also be done with pretrained weights. Thus, we can use a ResNet  based encoder which is pretrained on ImageNet and a generic decoder. In place of  ResNet, many different network architectures can be used. Segmentation Models  Pytorch12 by Pavel Yakubovskiy is an implementation of many such variations  where an encoder can be replaced by a pretrained model. Let’s apply a ResNet based  U-Net for pneumothorax detection problem.  Most of the problems like this should have two inputs: the original image and a  mask. In the case of multiple objects, there will be multiple masks. In our  pneumothorax dataset, we are provided with RLE instead. RLE stands for run- length encoding and is a way to represent binary masks to save space."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iQ2DvWhEWYcZ",
        "colab_type": "text"
      },
      "source": [
        "#Need to go more into RLE: A topic for disucssion"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QxYM-NC3Wo_n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#dataset.py  \n",
        "import os  \n",
        "import glob  \n",
        "import torch  \n",
        "import numpy as np  \n",
        "import pandas as pd  \n",
        "from PIL import Image, ImageFile  \n",
        "from tqdm import tqdm  \n",
        "from collections import defaultdict  \n",
        "from torchvision import transforms  \n",
        "from albumentations import (  \n",
        "    Compose,  OneOf,  RandomBrightnessContrast,  RandomGamma,  ShiftScaleRotate,  )  \n",
        "ImageFile.LOAD_TRUNCATED_IMAGES = True  \n",
        "class SIIMDataset(torch.utils.data.Dataset):  \n",
        "  def __init__(  self,  image_ids,  transform=True,  preprocessing_fn=None  ): \n",
        "     \"\"\"  Dataset class for segmentation problem  :param image_ids: ids of the images, list  :param transform: True/False, no transform in validation  \n",
        "     :param preprocessing_fn: a function for preprocessing image  \"\"\" \n",
        "     #we create a empty dictionary to store image  \n",
        "     #and mask paths  \n",
        "     self.data = defaultdict(dict)  \n",
        "     #for augmentations  \n",
        "     self.transform = transform  \n",
        "     #preprocessing function to normalize  \n",
        "     #images  \n",
        "     self.preprocessing_fn = preprocessing_fn  \n",
        "     #albumentation augmentations  \n",
        "     #we have shift, scale & rotate  \n",
        "     #applied with 80% probability  \n",
        "     #and then one of gamma and brightness/contrast  \n",
        "     #is applied to the image  \n",
        "     #albumentation takes care of which augmentation  \n",
        "     #is applied to image and mask  \n",
        "     self.aug = Compose(  [  ShiftScaleRotate(  \n",
        "         shift_limit=0.0625,  scale_limit=0.1,  \n",
        "         rotate_limit=10, p=0.8  ),  \n",
        "         OneOf(  [  RandomGamma(  gamma_limit=(90, 110)  ),  \n",
        "                  RandomBrightnessContrast(  brightness_limit=0.1,  contrast_limit=0.1  ),  ],  \n",
        "               p=0.5,  ),  ]  )  \n",
        "     #going over all image_ids to store  \n",
        "     #image and mask paths  \n",
        "     for imgid in image_ids:  \n",
        "       files = glob.glob(os.path.join(TRAIN_PATH, imgid, \"*.png\"))  \n",
        "       self.data[counter] = {  \"img_path\": os.path.join(  TRAIN_PATH, imgid + \".png\" \n",
        "                                                        ),  \"mask_path\": os.path.join(  TRAIN_PATH, imgid + \"_mask.png\"  ),  }  \n",
        "  def __len__(self):  \n",
        "    #return length of dataset  \n",
        "    return len(self.data)  \n",
        "  def __getitem__(self, item):  \n",
        "    #for a given item index,  \n",
        "    #return image and mask tensors  \n",
        "    #read image and mask paths  \n",
        "    img_path = self.data[item][\"img_path\"]  \n",
        "    mask_path = self.data[item][\"mask_path\"]  \n",
        "    #read image and convert to RGB  \n",
        "    img = Image.open(img_path)  \n",
        "    img = img.convert(\"RGB\")  \n",
        "    #PIL image to numpy array  \n",
        "    img = np.array(img)  \n",
        "    #read mask image  \n",
        "    mask = Image.open(mask_path)  \n",
        "    #convert to binary float matrix  \n",
        "    mask = (mask >= 1).astype(\"float32\")  \n",
        "    #if this is training data, apply transforms  \n",
        "    if self.transform is True:  \n",
        "      augmented = self.aug(image=img, mask=mask)  \n",
        "      img = augmented[\"image\"]  \n",
        "      mask = augmented[\"mask\"]  \n",
        "    #preprocess the image using provided  \n",
        "    #preprocessing tensors. this is basically  \n",
        "    #image normalization  \n",
        "    img = self.preprocessing_fn(img)  \n",
        "    #return image and mask tensors  \n",
        "    return {  \"image\": transforms.ToTensor()(img),  \"mask\": transforms.ToTensor()(mask).float(),  } \n",
        "                                                        "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z_mZ9qkUYZ1p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#train.py  \n",
        "import os  \n",
        "import sys  \n",
        "import torch  \n",
        "import numpy as np  \n",
        "import pandas as pd  \n",
        "import segmentation_models_pytorch as smp  \n",
        "import torch.nn as nn  \n",
        "import torch.optim as optim  \n",
        "from apex import amp  \n",
        "from collections import OrderedDict  \n",
        "from sklearn import model_selection  \n",
        "from tqdm import tqdm  \n",
        "from torch.optim import lr_scheduler  \n",
        "from dataset import SIIMDataset  \n",
        "#training csv file path  \n",
        "TRAINING_CSV = \"../input/train_pneumothorax.csv\"  \n",
        "#training and test batch sizes  \n",
        "TRAINING_BATCH_SIZE = 16  \n",
        "TEST_BATCH_SIZE = 4  \n",
        "#number of epochs  \n",
        "EPOCHS = 10  \n",
        "#define the encoder for U-Net  \n",
        "#check: https://github.com/qubvel/segmentation_models.pytorch  \n",
        "#for all supported encoders  \n",
        "ENCODER = \"resnet18\"  \n",
        "#we use imagenet pretrained weights for the encoder  \n",
        "ENCODER_WEIGHTS = \"imagenet\" \n",
        "#train on gpu  \n",
        "DEVICE = \"cuda\"  \n",
        "def train(dataset, data_loader, model, criterion, optimizer):  \n",
        "  \"\"\"  training function that trains for one epoch  \n",
        "  :param dataset: dataset class (SIIMDataset) \n",
        "  :param data_loader: torch dataset loader  \n",
        "  :param model: model  \n",
        "  :param criterion: loss function  \n",
        "  :param optimizer: adam, sgd, etc.  \"\"\"  \n",
        "  #put the model in train mode  \n",
        "  model.train()  \n",
        "  #calculate number of batches  \n",
        "  num_batches = int(len(dataset)/ data_loader.batch_size)  \n",
        "  #init tqdm to track progress  \n",
        "  tk0 = tqdm(data_loader, total=num_batches)  \n",
        "  #loop over all batches  \n",
        "  for d in tk0:  \n",
        "    #fetch input images and masks  \n",
        "    #from dataset batch  \n",
        "    inputs = d[\"image\"]  \n",
        "    targets = d[\"mask\"]  \n",
        "    #move images and masks to cpu/gpu device  \n",
        "    inputs = inputs.to(DEVICE, dtype=torch.float)  \n",
        "    targets = targets.to(DEVICE, dtype=torch.float)  \n",
        "    #zero grad the optimizer  \n",
        "    optimizer.zero_grad()  \n",
        "    #forward step of model  \n",
        "    outputs = model(inputs)  \n",
        "    #calculate loss  \n",
        "    loss = criterion(outputs, targets)  \n",
        "    #backward loss is calculated on a scaled loss  \n",
        "    #context since we are using mixed precision training  \n",
        "    #if you are not using mixed precision training, \n",
        "     #you can use loss.backward() and delete the following  \n",
        "     #two lines of code  \n",
        "     with amp.scale_loss(loss, optimizer) as scaled_loss:  \n",
        "       scaled_loss.backward()  \n",
        "     #step the optimizer  optimizer.step()  \n",
        "     #close tqdm  \n",
        "     tk0.close() "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vT2OLqXFZkrn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def evaluate(dataset, data_loader, model):  \n",
        "  \"\"\"  evaluation function to calculate loss on validation  set for one epoch  \n",
        "  :param dataset: dataset class (SIIMDataset)  \n",
        "  :param data_loader: torch dataset loader  \n",
        "  :param model: model  \"\"\"  \n",
        "  #put model in eval mode  \n",
        "  model.eval()  \n",
        "  #init final_loss to 0  \n",
        "  final_loss = 0  \n",
        "  #calculate number of batches and init tqdm  \n",
        "  num_batches = int(len(dataset)/ data_loader.batch_size)  \n",
        "  tk0 = tqdm(data_loader, total=num_batches)  \n",
        "  #we need no_grad context of torch. this save memory  \n",
        "  with torch.no_grad():  \n",
        "    for d in tk0:  \n",
        "      inputs = d[\"image\"]  \n",
        "      targets = d[\"mask\"]  \n",
        "      inputs = inputs.to(DEVICE, dtype=torch.float)  \n",
        "      targets = targets.to(DEVICE, dtype=torch.float)  \n",
        "      output = model(inputs)  \n",
        "      loss = criterion(output, targets)  \n",
        "      #add loss to final loss  \n",
        "      final_loss += loss  \n",
        "  #close tqdm  \n",
        "  tk0.close()  \n",
        "  #return average loss over all batches  \n",
        "  return final_loss/ num_batches  \n",
        "  \n",
        "  if  __name__  ==  \"__main__\": \n",
        "     #read the training csv file  \n",
        "     df = pd.read_csv(TRAINING_CSV)  \n",
        "     #split data into training and validation  \n",
        "     df_train, df_valid = model_selection.train_test_split(  df, random_state=42, test_size=0.1  )  \n",
        "     #training and validation images lists/arrays  \n",
        "     training_images = df_train.image_id.values  \n",
        "     validation_images = df_valid.image_id.values\n",
        "\n",
        "     #fetch unet model from segmentation models  \n",
        "     #with specified encoder architecture  \n",
        "     model = smp.Unet(  encoder_name=ENCODER,  encoder_weights=ENCODER_WEIGHTS,  classes=1,  activation=None,  )  \n",
        "     #segmentation model provides you with a preprocessing  \n",
        "     #function that can be used for normalizing images  \n",
        "     #normalization is only applied on images and not masks  \n",
        "     prep_fn = smp.encoders.get_preprocessing_fn(  ENCODER,  ENCODER_WEIGHTS  )  \n",
        "     #send model to device  \n",
        "     model.to(DEVICE)  \n",
        "     #init training dataset  \n",
        "     #transform is True for training data  \n",
        "     train_dataset = SIIMDataset(  training_images,  transform=True,  preprocessing_fn=prep_fn,  )  \n",
        "     #wrap training dataset in torch's dataloader  \n",
        "     train_loader = torch.utils.data.DataLoader(  train_dataset,  batch_size=TRAINING_BATCH_SIZE,  shuffle=True,  num_workers=12  )  \n",
        "     #init validation dataset  \n",
        "     #augmentations is disabled  \n",
        "     valid_dataset = SIIMDataset(  validation_images,  transform=False,  preprocessing_fn=prep_fn,  )  \n",
        "     #wrap validation dataset in torch's dataloader  \n",
        "     valid_loader = torch.utils.data.DataLoader(  valid_dataset,  batch_size=TEST_BATCH_SIZE,  shuffle=True,  num_workers=4  )  \n",
        "     #NOTE: define the criterion here  \n",
        "     #this is left as an excercise  \n",
        "     #code won't work without defining this  \n",
        "     #criterion =  ……  \n",
        "     \n",
        "     #we will use Adam optimizer for faster convergence  \n",
        "     optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)  \n",
        "     #reduce learning rate when we reach a plateau on loss  \n",
        "     scheduler = lr_scheduler.ReduceLROnPlateau(  optimizer, mode=\"min\", patience=3, verbose=True  )  \n",
        "     #wrap model and optimizer with NVIDIA's apex  \n",
        "     #this is used for mixed precision training  \n",
        "     #if you have a GPU that supports mixed precision,  \n",
        "     #this is very helpful as it will allow us to fit larger images  \n",
        "     #and larger batches  \n",
        "     model, optimizer = amp.initialize(  model, optimizer, opt_level=\"O1\", verbosity=0  )  \n",
        "     #if we have more than one GPU, we can use both of them!  \n",
        "     if torch.cuda.device_count() > 1:  \n",
        "       print(f\"Let's use {torch.cuda.device_count()} GPUs!\")  \n",
        "       model = nn.DataParallel(model)  \n",
        "      #some logging  \n",
        "      print(f\"Training batch size: {TRAINING_BATCH_SIZE}\")  \n",
        "      print(f\"Test batch size: {TEST_BATCH_SIZE}\")  \n",
        "      print(f\"Epochs: {EPOCHS}\")  \n",
        "      print(f\"Image size: {IMAGE_SIZE}\")  \n",
        "      print(f\"Number of training images: {len(train_dataset)}\")  \n",
        "      print(f\"Number of validation images: {len(valid_dataset)}\")  \n",
        "      print(f\"Encoder: {ENCODER}\")  \n",
        "      #loop over all epochs  \n",
        "      for epoch in range(EPOCHS):  \n",
        "        print(f\"Training Epoch: {epoch}\")  \n",
        "      #train for one epoch  \n",
        "        train(  train_dataset,  train_loader,  model,  criterion,  optimizer )\n",
        "        print(f\"Validation Epoch: {epoch}\")  \n",
        "        #calculate validation loss  \n",
        "        val_log = evaluate(  valid_dataset,  valid_loader,  model  )  \n",
        "        #step the scheduler  \n",
        "        scheduler.step(val_log[\"loss\"])  \n",
        "        print(\"\\n\") "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OIimLJ_6bX3X",
        "colab_type": "text"
      },
      "source": [
        "In segmentation problems you can use a variety of loss functions, for example,  pixel-wise binary cross-entropy, focal loss, dice loss etc. I’m leaving it for the  reader to decide the appropriate loss given the evaluation metric. When you train a  model like this, you will create a model that tries to predict the location of  pneumothorax, as shown in figure 9. In the above code, we used mixed precision  training using NVIDIA apex. Please note that this is available natively in PyTorch  from version 1.6.0+. \n",
        "of the commonly used functions in a python package called  Well That’s Fantastic Machine Learning (WTFML). Let’s see how this helps us to  build a multi-class classification model for plant images from the plant pathology  challenge of FGVC 202013. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TRxE0OoHbjSK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os  \n",
        "import pandas as pd  \n",
        "import numpy as np  \n",
        "import albumentations  \n",
        "import argparse  \n",
        "import torch  \n",
        "import torchvision  \n",
        "import torch.nn as nn  \n",
        "import torch.nn.functional as F  \n",
        "from sklearn import metrics  \n",
        "from sklearn.model_selection import train_test_split  \n",
        "from wtfml.engine import Engine  \n",
        "from wtfml.data_loaders.image import ClassificationDataLoader  \n",
        "class DenseCrossEntropy(nn.Module):  \n",
        "  #Taken from:  #https://www.kaggle.com/pestipeti/plant-pathology-2020-pytorch  \n",
        "  def __init__(self):  \n",
        "    super(DenseCrossEntropy, self).__init__()  \n",
        "  def forward(self, logits, labels):  \n",
        "    logits = logits.float()  labels = labels.float()  \n",
        "    logprobs = F.log_softmax(logits, dim=-1)  \n",
        "    loss = -labels * logprobs  loss = loss.sum(-1)  \n",
        "    return loss.mean()  \n",
        "class Model(nn.Module):  \n",
        "  def __init__(self):  \n",
        "    super().__init__()  \n",
        "    self.base_model = torchvision.models.resnet18(pretrained=True)  \n",
        "    in_features = self.base_model.fc.in_features  \n",
        "    self.out = nn.Linear(in_features, 4)  \n",
        "  def forward(self, image, targets=None):  \n",
        "    batch_size, C, H, W = image.shape\n",
        "    x = self.base_model.conv1(image)  \n",
        "    x = self.base_model.bn1(x)  \n",
        "    x = self.base_model.relu(x)  \n",
        "    x = self.base_model.maxpool(x)  \n",
        "    x = self.base_model.layer1(x)  \n",
        "    x = self.base_model.layer2(x)  \n",
        "    x = self.base_model.layer3(x)  \n",
        "    x = self.base_model.layer4(x)  \n",
        "    x = F.adaptive_avg_pool2d(x, 1).reshape(batch_size, -1)  \n",
        "    x = self.out(x)  \n",
        "    loss = None  \n",
        "    if targets is not None:  \n",
        "      loss = DenseCrossEntropy()(x, targets.type_as(x))  \n",
        "    return x, loss  \n",
        "    \n",
        "if  __name__  ==  \"__main__\":  \n",
        "  parser = argparse.ArgumentParser()  \n",
        "  parser.add_argument(  \"--data_path\", type=str,  )  \n",
        "  parser.add_argument(  \"--device\", type=str,  )  \n",
        "  parser.add_argument(  \"--epochs\", type=int,  )  \n",
        "  args = parser.parse_args()  \n",
        "  df = pd.read_csv(os.path.join(args.data_path, \"train.csv\"))  \n",
        "  images = df.image_id.values.tolist()  \n",
        "  images = [  os.path.join(args.data_path, \"images\", i + \".jpg\")  for i in images  ]  \n",
        "  targets = df[[\"healthy\", \"multiple_diseases\", \"rust\", \"scab\"]].values  \n",
        "  model = Model()  \n",
        "  model.to(args.device)  \n",
        "  mean = (0.485, 0.456, 0.406)  \n",
        "  std = (0.229, 0.224, 0.225)\n",
        "  aug = albumentations.Compose(  [  albumentations.Normalize(  mean,  std,  max_pixel_value=255.0,  always_apply=True  )  ]  )  \n",
        "  (  train_images, valid_images,  train_targets, valid_targets  ) = train_test_split(images, targets)  \n",
        "  \n",
        "  train_loader = ClassificationDataLoader(  image_paths=train_images,  targets=train_targets,  resize=(128, 128),  augmentations=aug,  )\n",
        "  .fetch(  batch_size=16,  num_workers=4,  drop_last=False,  shuffle=True,  tpu=False  )  \n",
        "  \n",
        "  valid_loader = ClassificationDataLoader(  image_paths=valid_images,  targets=valid_targets,  resize=(128, 128),  augmentations=aug,  )\n",
        "  .fetch(  batch_size=16,  num_workers=4,  drop_last=False,  shuffle=False,  tpu=False  )  \n",
        "  \n",
        "  optimizer = torch.optim.Adam(model.parameters(), lr=5e-4)  \n",
        "  scheduler = torch.optim.lr_scheduler.StepLR(  optimizer, step_size=15, gamma=0.6  )\n",
        "\n",
        "  for epoch in range(args.epochs):  \n",
        "    train_loss = Engine.train(  train_loader, model, optimizer, device=args.device  )  \n",
        "    valid_loss = Engine.evaluate(  valid_loader, model, device=args.device  )  \n",
        "    print(  f\"{epoch}, Train Loss={train_loss} Valid Loss={valid_loss}\"  )    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dVma_znw6F_D",
        "colab_type": "text"
      },
      "source": [
        "#NATURAL LANGUAGE PROCESSING\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8XnXF4bt6RWX",
        "colab_type": "text"
      },
      "source": [
        "You can have a simple counter that counts the number of positive and negative  words in the sentence. If the number of positive words is higher, it is a positive  sentiment, and if the number of negative words is higher, it is a sentence with a  negative sentiment. If none of them are present in the sentence, you can say that the  sentence has a neutral sentiment. This is one of the oldest ways, and some people  still use it. It does not require much code either. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fQobLtm-6CLo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def find_sentiment(sentence, pos, neg):  \n",
        "  \"\"\"  This function returns sentiment of sentence  \n",
        "  :param sentence: sentence, a string  \n",
        "  :param pos: set of positive words  \n",
        "  :param neg: set of negative words  \n",
        "  :return: returns positive, negative or neutral sentiment  \"\"\" \n",
        "  #\"this is a sentence!\" becomes:  \n",
        "  #[\"this\", \"is\" \"a\", \"sentence!\"]  \n",
        "  #note that im splitting on all whitespaces  \n",
        "  #if you want to split by space use .split(\"\")  \n",
        "  sentence = sentence.split()  \n",
        "  #make sentence into a set  \n",
        "  sentence = set(sentence)  \n",
        "  #check number of common words with positive  \n",
        "  num_common_pos = len(sentence.intersection(pos))  \n",
        "  #check number of common words with negative  \n",
        "  num_common_neg = len(sentence.intersection(neg))  \n",
        "  #make conditions and return  \n",
        "  #see how return used eliminates if else  \n",
        "  if num_common_pos > num_common_neg:  \n",
        "    return \"positive\"  \n",
        "  if num_common_pos < num_common_neg:  \n",
        "    return \"negative\"  \n",
        "  return \"neutral\" "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UXHs9OQf67Pn",
        "colab_type": "text"
      },
      "source": [
        "“hi, how are you?”  \n",
        "gets split into  [“hi,”, “how”, “are”, “you?”]  \n",
        "This is not ideal, because you see the comma and question mark, they are not split.  It is therefore not recommended to use this method if you don’t have a preprocessing that handles these special characters before the split. Splitting a string  into a list of words is known as tokenization. One of the most popular tokenization  comes from NLTK (Natural Language Tool Kit).  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EhFZOAesbjej",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "sentence = \"hi, how are you?\" \n",
        "sentence.split()\n",
        "['hi,', 'how', 'are', 'you?']\n",
        "word_tokenize(sentence)\n",
        "['hi',',', 'how', 'are', 'you','?'] "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uNyYscjl7W2O",
        "colab_type": "text"
      },
      "source": [
        "One of the basic models that you should always try with a classification problem in  NLP is bag of words. In bag of words, we create a huge sparse matrix that stores  counts of all the words in our corpus (corpus = all the documents = all the  sentences). For this, we will use CountVectorizer from scikit-learn."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D4wabq327cga",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer  \n",
        "#create a corpus of sentences  \n",
        "corpus = [  \"hello, how are you?\",  \"im getting bored at home. And you? What do you think?\",  \"did you know about counts\",  \"let's see if this works!\",  \"YES!!!!\"  ]  \n",
        "#initialize CountVectorizer  \n",
        "ctv = CountVectorizer()  \n",
        "#fit the vectorizer on corpus  \n",
        "ctv.fit(corpus)  \n",
        "corpus_transformed = ctv.transform(corpus) \n",
        "#Output\n",
        "# (0, 2) 1  \n",
        "# (0, 9) 1  \n",
        "# (0, 11) 1  \n",
        "# (0, 22) 1 \n",
        "#(1, 1) 1  (1, 3) 1  (1, 4) 1  (1, 7) 1  (1, 8) 1  (1, 10) 1  (1, 13) 1  (1, 17) 1  (1, 19) 1  (1, 22) 2  (2, 0) 1  (2, 5) 1  (2, 6) 1  (2, 14) 1  (2, 22) 1  (3, 12) 1  (3, 15) 1  (3, 16) 1  (3, 18) 1  (3, 20) 1  (4, 21) 1 \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KBUaRe2q8T_F",
        "colab_type": "text"
      },
      "source": [
        "It is the sparse  representation. So, our corpus is now a sparse matrix, where, for first sample, we  have four elements, for sample 2 we have ten elements, and so on, for sample 3 we  have five elements and so on. We also see that these elements have a count  associated with them. Some are seen twice, some are seen only once. For example,  in sample 2 (row 1), we see that column 22 has a value of two. Why is that? And  what is column 22?  The way CountVectorizer works is it first tokenizes the sentence and then assigns a  value to each token. So, each token is represented by a unique index. These unique  indices are the columns that we see. The CountVectorizer stores this information. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TXu8PD9j8eD0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(ctv.vocabulary_)  \n",
        "{'hello': 9, 'how': 11, 'are': 2, 'you': 22, 'im': 13, 'getting': 8,  'bored': 4, 'at': 3, 'home': 10, 'and': 1, 'what': 19, 'do': 7, 'think':  17, 'did': 6, 'know': 14, 'about': 0, 'counts': 5, 'let': 15, 'see': 16,  'if': 12, 'this': 18, 'works': 20, 'yes': 21} "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mzkuJxLz8qjq",
        "colab_type": "text"
      },
      "source": [
        "We see that index 22 belongs to “you” and in the second sentence, we have used  “you” twice. Thus, the count is 2. I hope it’s clear now what is bag of words. But  we are missing some special characters. Sometimes those special characters can be  useful too. For example, “?” denotes a question in most sentences. Let’s integrate  word_tokenize from scikit-learn in CountVectorizer and see what happens. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_AqGrbNH8u6X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer  \n",
        "from nltk.tokenize import word_tokenize  \n",
        "#create a corpus of sentences  \n",
        "corpus = [  \"hello, how are you?\",  \"im getting bored at home. And you? What do you think?\",  \"did you know about counts\",  \n",
        "          \"let's see if this works!\",  \"YES!!!!\"  ]  \n",
        "#initialize CountVectorizer with word_tokenize from nltk  \n",
        "#as the tokenizer  \n",
        "ctv = CountVectorizer(tokenizer=word_tokenize, token_pattern=None)  \n",
        "#fit the vectorizer on corpus  \n",
        "ctv.fit(corpus) \n",
        "corpus_transformed = ctv.transform(corpus)  \n",
        "print(ctv.vocabulary_)\n",
        "\n",
        "{'hello': 14,',': 2, 'how': 16, 'are': 7, 'you': 27,'?': 4, 'im': 18,  'getting': 13, 'bored': 9, 'at': 8, 'home': 15,'.': 3, 'and': 6, 'what':  24, 'do': 12, 'think': 22, 'did': 11, 'know': 19, 'about': 5, 'counts':  10, 'let': 20, \"'s\": 1, 'see': 21, 'if': 17, 'this': 23, 'works': 25,  '!': 0, 'yes': 26} "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O8PIYM7g9Kkk",
        "colab_type": "text"
      },
      "source": [
        "Now, we have more words in the vocabulary. Thus, we can now create a sparse  matrix by using all the sentences in IMDB dataset and can build a model. The ratio  to positive and negative samples in this dataset is 1:1, and thus, we can use accuracy  as the metric. We will use StratifiedKFold and create a single script to train five folds. Which model to use you ask? Which is the fastest model for high dimensional  sparse data? Logistic regression. We will use logistic regression for this dataset to  start with and to create our first actual benchmark. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lLXghsUB8_W1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#import what we need  \n",
        "import pandas as pd  \n",
        "from nltk.tokenize import word_tokenize  \n",
        "from sklearn import linear_model  \n",
        "from sklearn import metrics  \n",
        "from sklearn import model_selection  \n",
        "from sklearn.feature_extraction.text \n",
        "import CountVectorizer  \n",
        "if  __name__  ==  \"__main__\":  \n",
        "#read the training data  \n",
        "df = pd.read_csv(\"../input/imdb.csv\")  \n",
        "#map positive to 1 and negative to 0  \n",
        "df.sentiment = df.sentiment.apply(  lambda x: 1 if x == \"positive\" else 0  )  \n",
        "#we create a new column called kfold and fill it with -1  \n",
        "df[\"kfold\"] = -1  \n",
        "#the next step is to randomize the rows of the data  \n",
        "df = df.sample(frac=1).reset_index(drop=True)  \n",
        "#fetch labels  \n",
        "y = df.sentiment.values  \n",
        "#initiate the kfold class from model_selection module  \n",
        "kf = model_selection.StratifiedKFold(n_splits=5)  \n",
        "#fill the new kfold column  \n",
        "for f, (t_, v_) in enumerate(kf.split(X=df, y=y)):  \n",
        "  df.loc[v_, 'kfold'] = f  \n",
        "#we go over the folds created  \n",
        "for fold_ in range(5):  \n",
        "  #temporary dataframes for train and test  \n",
        "  train_df = df[df.kfold != fold_].reset_index(drop=True)\n",
        "  test_df = df[df.kfold == fold_].reset_index(drop=True)  \n",
        "  #initialize CountVectorizer with NLTK's word_tokenize  \n",
        "  #function as tokenizer  \n",
        "  count_vec = CountVectorizer(  tokenizer=word_tokenize,  token_pattern=None  )  \n",
        "  #fit count_vec on training data reviews  \n",
        "  count_vec.fit(train_df.review)  \n",
        "  #transform training and validation data reviews  \n",
        "  xtrain = count_vec.transform(train_df.review)  \n",
        "  xtest = count_vec.transform(test_df.review)  \n",
        "  #initialize logistic regression model  \n",
        "  model = linear_model.LogisticRegression()  \n",
        "  #fit the model on training data reviews and sentiment  \n",
        "  model.fit(xtrain, train_df.sentiment)  \n",
        "  #make predictions on test data \n",
        "  #threshold for predictions is 0.5  \n",
        "  preds = model.predict(xtest)  \n",
        "  #calculate accuracy  \n",
        "  accuracy = metrics.accuracy_score(test_df.sentiment, preds)  \n",
        "  print(f\"Fold: {fold_}\")  \n",
        "  print(f\"Accuracy = {accuracy}\")  \n",
        "  print(\"\")  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "liaL_lfbBqg0",
        "colab_type": "text"
      },
      "source": [
        " "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "87sjfUfl-Hxq",
        "colab_type": "text"
      },
      "source": [
        "To use this model, we need to change one import  and the line with the model. Let’s see how this model performs. We will use  MultinomialNB from scikit-learn. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r0uQbxX3-Iox",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#import what we need  \n",
        "import pandas as pd  \n",
        "from nltk.tokenize import word_tokenize  \n",
        "from sklearn import naive_bayes  \n",
        "from sklearn import metrics  \n",
        "from sklearn import model_selection  \n",
        "from sklearn.feature_extraction.text import CountVectorizer  \n",
        ".  .  .  .  \n",
        "#initialize naive bayes model  \n",
        "model = naive_bayes.MultinomialNB()  \n",
        "#fit the model on training data reviews and sentiment  \n",
        "model.fit(xtrain, train_df.sentiment)  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y3arVE3L-jZQ",
        "colab_type": "text"
      },
      "source": [
        "We see that this score is low. But the naïve bayes model is superfast.  Another method in NLP that most of the people these days tend to ignore or don’t  care to know about is called TF-IDF.\n",
        "\n",
        " TF is term frequencies, and IDF is inverse  document frequency. It might seem difficult from these terms, but things will  become apparent with the formulae for TF and IDF.  \n",
        "   \n",
        " TF(t) = Number of times a term t appears in a document/  Total number of terms in the document  \n",
        " \n",
        " IDF(t) = LOG( Total number of documents /  Number of documents with term t in it)\n",
        "\n",
        "And TF-IDF for a term t is defined as:  \n",
        "\n",
        "TF-IDF(t) = TF(t) * IDF(t)  \n",
        "\n",
        "Similar to CountVectorizer in scikit-learn, we have TfidfVectorizer. Let’s try using  it the same way we used CountVectorizer.  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uhtn9GPt_ElM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer  \n",
        "from nltk.tokenize import word_tokenize  \n",
        "#create a corpus of sentences  \n",
        "corpus = [\"hello, how are you?\",  \"im getting bored at home. And you? What do you think?\",  \"did you know about counts\",  \"let's see if this works!\",  \"YES!!!!\"  ]  \n",
        "#initialize TfidfVectorizer with word_tokenize from nltk  \n",
        "#as the tokenizer  \n",
        "tfv = TfidfVectorizer(tokenizer=word_tokenize, token_pattern=None)  \n",
        "#fit the vectorizer on corpus  \n",
        "tfv.fit(corpus)  \n",
        "corpus_transformed = tfv.transform(corpus)  \n",
        "print(corpus_transformed)  \n",
        "\n",
        "#(0, 27) 0.2965698850220162  (0, 16) 0.4428321995085722  (0, 14) 0.4428321995085722  (0, 7) 0.4428321995085722  (0, 4) 0.35727423026525224  (0, 2) 0.4428321995085722  (1, 27) 0.35299699146792735  (1, 24) 0.2635440111190765  (1, 22) 0.2635440111190765  (1, 18) 0.2635440111190765  (1, 15) 0.2635440111190765  (1, 13) 0.2635440111190765  (1, 12) 0.2635440111190765  (1, 9) 0.2635440111190765  (1, 8) 0.2635440111190765  (1, 6) 0.2635440111190765  (1, 4) 0.42525129752567803  (1, 3) 0.2635440111190765  (2, 27) 0.31752680284846835  (2, 19) 0.4741246485558491  (2, 11) 0.4741246485558491  (2, 10) 0.4741246485558491  (2, 5) 0.4741246485558491  (3, 25) 0.38775666010579296  (3, 23) 0.38775666010579296  (3, 21) 0.38775666010579296 (3, 20) 0.38775666010579296  (3, 17) 0.38775666010579296  (3, 1) 0.38775666010579296  (3, 0) 0.3128396318588854  (4, 26) 0.2959842226518677  (4, 0) 0.9551928286692534 "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O7pK1_OkA5bm",
        "colab_type": "text"
      },
      "source": [
        "We see that instead of integer values, this time we get floats. Replacing  CountVectorizer with TfidfVectorizer is also a piece of cake. Scikit-learn also offers  TfidfTransformer. If you have count values, you can use TfidfTransformer and get  the same behaviour as TfidfVectorizer. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oa1Je94IBAa9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#import what we need  \n",
        "import pandas as pd  \n",
        "from nltk.tokenize import word_tokenize  \n",
        "from sklearn import linear_model  \n",
        "from sklearn import metrics  \n",
        "from sklearn import model_selection  \n",
        "from sklearn.feature_extraction.text import TfidfVectorizer \n",
        " .  .  .  \n",
        " #we go over the folds created  \n",
        " for fold_ in range(5):  \n",
        "   #temporary dataframes for train and test  \n",
        "   train_df = df[df.kfold != fold_].reset_index(drop=True)  \n",
        "   test_df = df[df.kfold == fold_].reset_index(drop=True)  \n",
        "   #initialize TfidfVectorizer with NLTK's word_tokenize  \n",
        "   #function as tokenizer  \n",
        "   tfidf_vec = TfidfVectorizer(  tokenizer=word_tokenize,  token_pattern=None  )  \n",
        "   #fit tfidf_vec on training data reviews  \n",
        "   tfidf_vec.fit(train_df.review)  \n",
        "   #transform training and validation data reviews  \n",
        "   xtrain = tfidf_vec.transform(train_df.review)  \n",
        "   xtest = tfidf_vec.transform(test_df.review)  \n",
        "   #initialize logistic regression model\n",
        "   model = linear_model.LogisticRegression()  \n",
        "   #fit the model on training data reviews and sentiment  \n",
        "   model.fit(xtrain, train_df.sentiment)  \n",
        "   #make predictions on test data  \n",
        "   #threshold for predictions is 0.5  \n",
        "   preds = model.predict(xtest)  \n",
        "   #calculate accuracy  \n",
        "   accuracy = metrics.accuracy_score(test_df.sentiment, preds)  \n",
        "   print(f\"Fold: {fold_}\")  \n",
        "   print(f\"Accuracy = {accuracy}\")  \n",
        "   print(\"\") "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F_H_zXqLB-vh",
        "colab_type": "text"
      },
      "source": [
        "We see that these scores are a bit higher than CountVectorizer, and thus, it becomes  the new benchmark that we would want to beat.  Another interesting concept in NLP is n-grams. N-grams are combinations of  words in order. N-grams are easy to create. You just need to take care of the order.  To make things even more comfortable, we can use n-gram implementation from  NLTK."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NZO0sBrSCBXc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from nltk import ngrams  \n",
        "from nltk.tokenize import word_tokenize  \n",
        "#let's see 3 grams  \n",
        "N = 3  \n",
        "#input sentence  \n",
        "sentence = \"hi, how are you?\"  \n",
        "#tokenized sentence  \n",
        "tokenized_sentence = word_tokenize(sentence)  \n",
        "#generate n_grams  \n",
        "n_grams = list(ngrams(tokenized_sentence, N))  \n",
        "print(n_grams) \n",
        "\n",
        "[('hi',',', 'how'),  (',', 'how', 'are'),  ('how', 'are', 'you'),  ('are', 'you','?')] "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bSgtzWgzCb3t",
        "colab_type": "text"
      },
      "source": [
        "Similarly, we can also create 2-grams, or 4-grams, etc. Now, these n-grams become  a part of our vocab, and when we calculate counts or tf-idf, we consider one n-gram  as one entirely new token. So, in a way, we are incorporating context to some extent.  Both CountVectorizer and TfidfVectorizer implementations of scikit-learn offers ngrams by ngram_range parameter, which has a minimum and maximum limit. By  default, this is (1, 1). When we change it to (1, 3), we are looking at unigrams,  bigrams and trigrams. The code change is minimal. Since we had the best result till  now with tf-idf, let’s see if including n-grams up to trigrams improves the model.  The only change required is in the initialization of TfidfVectorizer. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ECA-hUqbCf2g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tfidf_vec = TfidfVectorizer(  tokenizer=word_tokenize,  token_pattern=None,  ngram_range=(1, 3)  ) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9VSsj2d6C2l4",
        "colab_type": "text"
      },
      "source": [
        "There are a lot more things in the basics of NLP. One term that you must be aware  of is stemming. Another is lemmatization. Stemming and lemmatization reduce a  word to its smallest form. In the case of stemming, the processed word is called the  stemmed word, and in the case of lemmatization, it is known as the lemma. It must  be noted that lemmatization is more aggressive than stemming and stemming is  more popular and widely used. Both stemming and lemmatization come from  linguistics. And you need to have an in-depth knowledge of a given language if you  plan to make a stemmer or lemmatizer for that language. Going into too much detail  of these would mean adding one more chapter in this book. Both stemming and  lemmatization can be done easily by using the NLTK package. Let’s take a look at  some examples for both of them. There are many different types of stemmers and  lemmatizers. I will show an example using the most common Snowball Stemmer  and WordNet Lemmatizer. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "831nfC5UC8HI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from nltk.stem import WordNetLemmatizer  \n",
        "from nltk.stem.snowball import SnowballStemmer  \n",
        "#initialize lemmatizer  \n",
        "lemmatizer = WordNetLemmatizer()\n",
        "#initialize stemmer  \n",
        "stemmer = SnowballStemmer(\"english\")  \n",
        "words = [\"fishing\", \"fishes\", \"fished\"]  \n",
        "for word in words:  \n",
        "  print(f\"word={word}\")  \n",
        "  print(f\"stemmed_word={stemmer.stem(word)}\")  \n",
        "  print(f\"lemma={lemmatizer.lemmatize(word)}\")  \n",
        "  print(\"\")  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C9APxw1zDpt8",
        "colab_type": "text"
      },
      "source": [
        "word=fishing  \n",
        "stemmed_word=fish  \n",
        "lemma=fishing\n",
        "\n",
        "word=fishes  \n",
        "stemmed_word=fish  \n",
        "lemma=fish \n",
        "\n",
        "word=fished  \n",
        "stemmed_word=fish  \n",
        "lemma=fished  \n",
        "\n",
        "\n",
        "As you can see, stemming and lemmatization are very different from each other.  When we do stemming, we are given the smallest form of a word which may or  may not be a word in the dictionary for the language the word belongs to. However,  in the case of lemmatization, this will be a word. You can now try on your own to  add stemming and lemmatizations and see if it improves your result. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YUtTMknOEB0G",
        "colab_type": "text"
      },
      "source": [
        "One more topic that you should be aware of is topic extraction. Topic extraction  can be done using non-negative matrix factorization (NMF) or latent semantic  analysis (LSA), which is also popularly known as singular value decomposition or  SVD. These are decomposition techniques that reduce the data to a given number  of components. You can fit any of these on sparse matrix obtained from  CountVectorizer or TfidfVectorizer.  Let’s apply it on TfidfVetorizer that we have used before. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-_EDB_WgEP_Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd  \n",
        "from nltk.tokenize import word_tokenize  \n",
        "from sklearn import decomposition  \n",
        "from sklearn.feature_extraction.text import TfidfVectorizer  \n",
        "#create a corpus of sentences  \n",
        "#we read only 10k samples from training data  \n",
        "#for this example  \n",
        "corpus = pd.read_csv(\"../input/imdb.csv\", nrows=10000)  \n",
        "corpus = corpus.review.values  \n",
        "#initialize TfidfVectorizer with word_tokenize from nltk  \n",
        "#as the tokenizer  \n",
        "tfv = TfidfVectorizer(tokenizer=word_tokenize, token_pattern=None)  \n",
        "#fit the vectorizer on corpus  \n",
        "tfv.fit(corpus)  \n",
        "#transform the corpus using tfidf  \n",
        "corpus_transformed = tfv.transform(corpus)  \n",
        "#initialize SVD with 10 components  \n",
        "svd = decomposition.TruncatedSVD(n_components=10)  \n",
        "#fit SVD  \n",
        "corpus_svd = svd.fit(corpus_transformed)  \n",
        "#choose first sample and create a dictionary  \n",
        "#of feature names and their scores from svd  \n",
        "#you can change the sample_index variable to  \n",
        "#get dictionary for any other sample  sample_index = 0  \n",
        "feature_scores = dict(  zip(  tfv.get_feature_names(),  corpus_svd.components_[sample_index]  )  )  \n",
        "#once we have the dictionary, we can now  #sort it in decreasing order and get the  \n",
        "#top N topics  \n",
        "N = 5  \n",
        "print(sorted(feature_scores, key=feature_scores.get, reverse=True)[:N]) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W2F5w3M3JzZ0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "N = 5  \n",
        "for sample_index in range(5):  \n",
        "  feature_scores = dict(  zip(  tfv.get_feature_names(),  corpus_svd.components_[sample_index]  )  )  \n",
        "  print(  sorted(  feature_scores,  key=feature_scores.get,  reverse=True  )[:N]  ) \n",
        "\n",
        " ['the',',','.', 'a', 'and']  ['br', '<', '>','/', '-']  ['i', 'movie','!', 'it', 'was']  [',','!',\"''\", '``', 'you']  ['!', 'the','...',\"''\", '``']  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sz3RvYI-KVV-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import re  \n",
        "import string  \n",
        "def clean_text(s): \n",
        "   \"\"\"  This function cleans the text a bit  :param s: string :return: cleaned string  \"\"\"  \n",
        "   #split by all whitespaces  \n",
        "   s = s.split()  \n",
        "   #join tokens by single space  \n",
        "   #why we do this?  \n",
        "   #this will remove all kinds of weird space  #\"hi. how are you\" becomes  #\"hi. how are you\"  \n",
        "   s = \" \".join(s)  \n",
        "   #remove all punctuations using regex and string module  \n",
        "   s = re.sub(f'[{re.escape(string.punctuation)}]','', s)  \n",
        "   #you can add more cleaning here if you want  \n",
        "   #and then return the cleaned string  \n",
        "   return s \n",
        "\n",
        "import pandas as pd\n",
        "corpus = pd.read_csv(\"../input/imdb.csv\", nrows=10000)  \n",
        "corpus.loc[:, \"review\"] = corpus.review.apply(clean_text) \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q45AkQ8uKxKm",
        "colab_type": "text"
      },
      "source": [
        "['the', 'a', 'and', 'of', 'to']  \n",
        "['i', 'movie', 'it', 'was', 'this']\n",
        "['the', 'was', 'i', 'were', 'of']\n",
        "['her', 'was', 'she', 'i', 'he']  \n",
        "['br', 'to', 'they', 'he', 'show']  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w33qcCJYL2Q9",
        "colab_type": "text"
      },
      "source": [
        "You can  make it even better by removing stopwords in your cleaning function. What are  stopwords? These are high-frequency words that exist in every language. For  example, in the English language, these words are “a”, “an”, “the”, “for”, etc.  Removing stopwords is not always a wise choice and depends a lot on the business  problem. A sentence like “I need a new dog” after removing stopwords will become  “need new dog”, so we don’t know who needs a new dog.  We lose a lot of context information if we remove stopwords all the time. You can  find stopwords for many languages in NLTK, and if it’s not there, you can find it  by a quick search on your favourite search engine.  Let’s move to an approach most of us like to use these days: deep learning. But first,  we must know what word embeddings are. You have seen that till now we  converted the tokens into numbers. So, if there are N unique tokens in a given  corpus, they can be represented by integers ranging from 0 to N-1. Now we will  represent these integer tokens with vectors. This representation of words into  vectors is known as word embeddings or word vectors. Google’s Word2Vec is one  of the oldest approaches to convert words into vectors. We also have FastText from  Facebook and GloVe (Global Vectors for Word Representation) from Stanford.  These approaches are quite different from each other.  The basic idea is to build a shallow network that learns the embeddings for words  by reconstruction of an input sentence. So, you can train a network to predict a  missing word by using all the words around and during this process, the network  will learn and update embeddings for all the words involved. This approach is also  known as Continuous Bag of Words or CBoW model. You can also try to take  one word and predict the context words instead. This is called skip-gram model.  Word2Vec can learn embedding using these two methods. \n",
        "\n",
        "FastText learns embeddings for character n-grams instead. Just like word n-grams,  if we use characters, it is known as character n-grams, and finally, GloVe learns  these embeddings by using co-occurrence matrices. So, we can say that all these  different types of embeddings are in the end returning a dictionary where the key is  a word in the corpus (for example English Wikipedia) and value is a vector of size  N (usually 300). "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j46cZk3JN9QH",
        "colab_type": "text"
      },
      "source": [
        "hi ─> [vector (v1) of size 300] \n",
        ", ─> [vector (v2) of size 300] \n",
        "how ─> [vector (v3) of size 300]  \n",
        "are ─> [vector (v4) of size 300] \n",
        "you ─> [vector (v5) of size 300]\n",
        "? ─> [vector (v6) of size 300]  \n",
        "     \n",
        "There are multiple ways to use this information. One of the simplest ways would  be to use the embeddings as they are. As you can see in the example above, we have  a 1x300 embedding vector for each word. Using this information, we can calculate  the embedding for the whole sentence. There are multiple ways to do it. One such method is shown as follows. In this function, we take all the individual word vectors  in a given sentence and create a normalized word vector from all word vectors of  the tokens. This provides us with a sentence vector. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X1MLDp2XORk4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np  \n",
        "def sentence_to_vec(s, embedding_dict, stop_words, tokenizer):  \n",
        "  \"\"\"  Given a sentence and other information,  \n",
        "  this function returns embedding for the whole sentence  \n",
        "  :param s: sentence, string  \n",
        "  :param embedding_dict: dictionary word:vector  \n",
        "  :param stop_words: list of stop words, if any  \n",
        "  :param tokenizer: a tokenization function  \"\"\"  \n",
        "  #convert sentence to string and lowercase it  \n",
        "  words = str(s).lower()  \n",
        "  #tokenize the sentence  \n",
        "  words = tokenizer(words)  \n",
        "  #remove stop word tokens  \n",
        "  words = [w for w in words if not w in stop_words]  \n",
        "  #keep only alpha-numeric tokens  \n",
        "  words = [w for w in words if w.isalpha()]  \n",
        "  #initialize empty list to store embeddings \n",
        "  M = []  \n",
        "  for w in words:  \n",
        "    #for every word, fetch the embedding from  #the dictionary and append to list of  \n",
        "    #embeddings  \n",
        "    if w in embedding_dict:  \n",
        "      M.append(embedding_dict[w])  \n",
        "      \n",
        "  #if we dont have any vectors, return zeros  \n",
        "  if len(M) == 0:  \n",
        "    return np.zeros(300)  \n",
        "  #convert list of embeddings to array  \n",
        "  M = np.array(M)  \n",
        "  #calculate sum over axis=0  \n",
        "  v = M.sum(axis=0)\n",
        "  #return normalized vector  \n",
        "  return v/ np.sqrt((v ** 2).sum())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PS6EvlLJO3Mu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#fasttext.py  \n",
        "import io  \n",
        "import numpy as np  \n",
        "import pandas as pd  \n",
        "from nltk.tokenize import word_tokenize  \n",
        "from sklearn import linear_model  \n",
        "from sklearn import metrics  \n",
        "from sklearn import model_selection  \n",
        "from sklearn.feature_extraction.text import TfidfVectorizer  \n",
        "def load_vectors(fname): \n",
        "   #taken from: https://fasttext.cc/docs/en/english-vectors.html  \n",
        "   fin = io.open(  fname,  'r',  encoding='utf-8',  newline='\\n',  errors='ignore'  )  \n",
        "   n, d = map(int, fin.readline().split())  \n",
        "   data = {}  \n",
        "   for line in fin:  \n",
        "     tokens = line.rstrip().split('')  \n",
        "     data[tokens[0]] = list(map(float, tokens[1:]))  \n",
        "  return data  \n",
        "def sentence_to_vec(s, embedding_dict, stop_words, tokenizer):  \n",
        "  .  \n",
        "  .  \n",
        "  .  \n",
        "  if  __name__  ==  \"__main__\":  \n",
        "    #read the training data  \n",
        "    df = pd.read_csv(\"../input/imdb.csv\")\n",
        "    #map positive to 1 and negative to 0  \n",
        "    df.sentiment = df.sentiment.apply(  lambda x: 1 if x == \"positive\" else 0  )  \n",
        "    #the next step is to randomize the rows of the data  \n",
        "    df = df.sample(frac=1).reset_index(drop=True)  \n",
        "    #load embeddings into memory  \n",
        "    print(\"Loading embeddings\")  \n",
        "    embeddings = load_vectors(\"../input/crawl-300d-2M.vec\")  \n",
        "    #create sentence embeddings  \n",
        "    print(\"Creating sentence vectors\")  \n",
        "    vectors = []  \n",
        "    for review in df.review.values:  \n",
        "      vectors.append(  sentence_to_vec(  s = review,  embedding_dict = embeddings,  stop_words = [],  tokenizer = word_tokenize  )  )  \n",
        "    vectors = np.array(vectors)  \n",
        "    #fetch labels  \n",
        "    y = df.sentiment.values  \n",
        "    #initiate the kfold class from model_selection module  \n",
        "    kf = model_selection.StratifiedKFold(n_splits=5)  \n",
        "    #fill the new kfold column  \n",
        "    for fold_, (t_, v_) in enumerate(kf.split(X=vectors, y=y)):  \n",
        "      print(f\"Training fold: {fold_}\")  \n",
        "      #temporary dataframes for train and test  \n",
        "      xtrain = vectors[t_,:]  \n",
        "      ytrain = y[t_]  \n",
        "      xtest = vectors[v_,:]  \n",
        "      ytest = y[v_]  \n",
        "      #initialize logistic regression model  \n",
        "      model = linear_model.LogisticRegression()  \n",
        "      #fit the model on training data reviews and sentiment\n",
        "      model.fit(xtrain, ytrain)  \n",
        "      #make predictions on test data  \n",
        "      #threshold for predictions is 0.5  \n",
        "      preds = model.predict(xtest)  \n",
        "      #calculate accuracy  \n",
        "      accuracy = metrics.accuracy_score(ytest, preds)  \n",
        "      print(f\"Accuracy = {accuracy}\")  \n",
        "      print(\"\")   "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kZxUBBLqPyPS",
        "colab_type": "text"
      },
      "source": [
        "Wow! That’s quite unexpected. We get excellent results, and all we did was to use  the FastText embeddings. Try changing the embeddings to GloVe and see what  happens. I’m leaving it as an exercise for you.  When we talk about text data, we must keep one thing in our mind. Text data is very  similar to the time series data. Any sample in our reviews is a sequence of tokens  at different timestamps which are in increasing order, and each token can be  represented as a vector/embedding.This means that we can use models that are widely used for time series data such as  Long Short Term Memory (LSTM) or Gated Recurrent Units (GRU) or even  Convolutional Neural Networks (CNNs). Let’s see how to train a simple bidirectional LSTM model on this dataset.  First of all, we will create a project. Feel free to name it whatever you want. And  then our first step will be splitting the data for cross-validation. \n",
        "\n",
        "\n",
        "First of all, we will create a project. Feel free to name it whatever you want. And  then our first step will be splitting the data for cross-validation. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N5QlMsP1P3bF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#create_folds.py  \n",
        "#import pandas and model_selection module of scikit-learn  \n",
        "import pandas as pd  \n",
        "from sklearn import model_selection  \n",
        "if  __name__  ==  \"__main__\":  \n",
        "  #Read training data  \n",
        "  df = pd.read_csv(\"../input/imdb.csv\")  \n",
        "  #map positive to 1 and negative to 0  \n",
        "  df.sentiment = df.sentiment.apply(  lambda x: 1 if x == \"positive\" else 0  )  \n",
        "  #we create a new column called kfold and fill it with -1  \n",
        "  df[\"kfold\"] = -1\n",
        "  #the next step is to randomize the rows of the data  \n",
        "  df = df.sample(frac=1).reset_index(drop=True)  \n",
        "  #fetch labels  \n",
        "  y = df.sentiment.values  \n",
        "  #initiate the kfold class from model_selection module  \n",
        "  kf = model_selection.StratifiedKFold(n_splits=5)  \n",
        "  #fill the new kfold column  \n",
        "  for f, (t_, v_) in enumerate(kf.split(X=df, y=y)):  \n",
        "    df.loc[v_, 'kfold'] = f  \n",
        "  #save the new csv with kfold column  \n",
        "  df.to_csv(\"../input/imdb_folds.csv\", index=False)   "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pkZwYDK1RdQ9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Creation of dataset\n",
        "#dataset.py  \n",
        "import torch  \n",
        "class IMDBDataset:  \n",
        "  def __init__(self, reviews, targets):  \n",
        "    \"\"\"  :param reviews: this is a numpy array  :param targets: a vector, numpy array  \"\"\"  \n",
        "    self.reviews = reviews  \n",
        "    self.target = targets  \n",
        "  def __len__(self):  \n",
        "    #returns length of the dataset  \n",
        "    return len(self.reviews)  \n",
        "  def __getitem__(self, item):  \n",
        "    #for any given item, which is an int,  \n",
        "    #return review and targets as torch tensor  \n",
        "    #item is the index of the item in concern  \n",
        "    review = self.reviews[item,:]  \n",
        "    target = self.target[item]  \n",
        "    return {\n",
        "        \"review\": torch.tensor(review, dtype=torch.long),  \n",
        "        \"target\": torch.tensor(target, dtype=torch.float)  }  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gigd-pjERyCE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#LSTM.py\n",
        "import torch  \n",
        "import torch.nn as nn  \n",
        "class LSTM(nn.Module):  \n",
        "  def __init__(self, embedding_matrix):  \n",
        "    \"\"\"  :param embedding_matrix: numpy array with vectors for all words  \"\"\"  \n",
        "    super(LSTM, self).__init__()  \n",
        "    #number of words = number of rows in embedding matrix  \n",
        "    num_words = embedding_matrix.shape[0]  \n",
        "    #dimension of embedding is num of columns in the matrix  \n",
        "    embed_dim = embedding_matrix.shape[1]  \n",
        "    #we define an input embedding layer  \n",
        "    self.embedding = nn.Embedding(  num_embeddings=num_words,  embedding_dim=embed_dim  )  \n",
        "    #embedding matrix is used as weights of  \n",
        "    #the embedding layer  \n",
        "    self.embedding.weight = nn.Parameter(  torch.tensor(  embedding_matrix,  dtype=torch.float32  )  )  \n",
        "    #we dont want to train the pretrained embeddings  \n",
        "    self.embedding.weight.requires_grad = False  \n",
        "    #a simple bidirectional LSTM with  \n",
        "    #hidden size of 128\n",
        "    self.lstm = nn.LSTM(  embed_dim,  128,  bidirectional=True,  batch_first=True,  )  \n",
        "    #output layer which is a linear layer  \n",
        "    #we have only one output  \n",
        "    #input (512) = 128 + 128 for mean and same for max pooling  \n",
        "    self.out = nn.Linear(512, 1)  \n",
        "  def forward(self, x):  \n",
        "    #pass data through embedding layer  \n",
        "    #the input is just the tokens  \n",
        "    x = self.embedding(x)  \n",
        "    #move embedding output to lstm  \n",
        "    x, _ = self.lstm(x)  \n",
        "    #apply mean and max pooling on lstm output  \n",
        "    avg_pool = torch.mean(x, 1)  \n",
        "    max_pool, _ = torch.max(x, 1)  \n",
        "    #concatenate mean and max pooling  \n",
        "    #this is why size is 512  #128 for each direction = 256  \n",
        "    #avg_pool = 256 and max_pool = 256  \n",
        "    out = torch.cat((avg_pool, max_pool), 1)  \n",
        "    #pass through the output layer and return the output  \n",
        "    out = self.out(out)  \n",
        "    #return linear output  \n",
        "    return out   "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CQBSf4ywTxWF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#engine.py  \n",
        "import torch  \n",
        "import torch.nn as nn  \n",
        "def train(data_loader, model, optimizer, device):  \n",
        "  \"\"\"This is the main training function that trains model  for one epoch  \n",
        "  :param data_loader: this is the torch dataloader  \n",
        "  :param model: model (lstm model)  \n",
        "  :param optimizer: torch optimizer, e.g. adam, sgd, etc.  \n",
        "  :param device: this can be \"cuda\" or \"cpu\"  \"\"\"  \n",
        "  #set model to training mode  \n",
        "  model.train()  \n",
        "  #go through batches of data in data loader  \n",
        "  for data in data_loader:  \n",
        "    #fetch review and target from the dict  \n",
        "    reviews = data[\"review\"]  \n",
        "    targets = data[\"target\"]  \n",
        "    \n",
        "    #move the data to device that we want to use  \n",
        "    reviews = reviews.to(device, dtype=torch.long)  \n",
        "    targets = targets.to(device, dtype=torch.float)  \n",
        "    #clear the gradients  \n",
        "    optimizer.zero_grad()  \n",
        "    #make predictions from the model  \n",
        "    predictions = model(reviews)  \n",
        "    #calculate the loss  \n",
        "    loss = nn.BCEWithLogitsLoss()(  predictions,  targets.view(-1, 1)  )  \n",
        "    #compute gradient of loss w.r.t.  \n",
        "    #all parameters of the model that are trainable  \n",
        "    loss.backward()  \n",
        "    #single optimization step  \n",
        "    optimizer.step()  \n",
        "  def evaluate(data_loader, model, device):  \n",
        "    #initialize empty lists to store predictions  \n",
        "    #and targets  \n",
        "    final_predictions = []  \n",
        "    final_targets = []  \n",
        "    #put the model in eval mode\n",
        "    model.eval()  \n",
        "    #disable gradient calculation  \n",
        "    with torch.no_grad():  \n",
        "      for data in data_loader:  \n",
        "        reviews = data[\"review\"]  \n",
        "        targets = data[\"target\"]  \n",
        "        reviews = reviews.to(device, dtype=torch.long)  \n",
        "        targets = targets.to(device, dtype=torch.float)  \n",
        "        #make predictions  \n",
        "        predictions = model(reviews)  \n",
        "        #move predictions and targets to list  \n",
        "        #we need to move predictions and targets to cpu too  \n",
        "        predictions = predictions.cpu().numpy().tolist()  \n",
        "        targets = data[\"target\"].cpu().numpy().tolist()  \n",
        "        final_predictions.extend(predictions)  \n",
        "        final_targets.extend(targets)  \n",
        "        #return final predictions and targets  \n",
        "    return final_predictions, final_targets   "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_GetFsBOVCYq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#These functions will help us in train.py which is used for training multiple folds. \n",
        "#train.py  \n",
        "import io  \n",
        "import torch  \n",
        "import numpy as np  \n",
        "import pandas as pd  \n",
        "\n",
        "#yes, we use tensorflow  \n",
        "#but not for training the model!  \n",
        "import tensorflow as tf  \n",
        "from sklearn import metrics  \n",
        "import config  \n",
        "import dataset  \n",
        "import engine  \n",
        "import lstm  \n",
        "def load_vectors(fname):  \n",
        "  #taken from: https://fasttext.cc/docs/en/english-vectors.html \n",
        "  fin = io.open(  fname,  'r',  encoding='utf-8',  newline='\\n',  errors='ignore'  )  \n",
        "  n, d = map(int, fin.readline().split())  \n",
        "  data = {}  \n",
        "  for line in fin:  \n",
        "    tokens = line.rstrip().split('')  \n",
        "    data[tokens[0]] = list(map(float, tokens[1:]))  \n",
        "  return data  \n",
        "def create_embedding_matrix(word_index, embedding_dict):  \n",
        "  \"\"\"  This function creates the embedding matrix.  \n",
        "  :param word_index: a dictionary with word:index_value  \n",
        "  :param embedding_dict: a dictionary with word:embedding_vector  \n",
        "  :return: a numpy array with embedding vectors for all known words  \"\"\"  \n",
        "  #initialize matrix with zeros  \n",
        "  embedding_matrix = np.zeros((len(word_index) + 1, 300))  \n",
        "  #loop over all the words  \n",
        "  for word, i in word_index.items():  \n",
        "    #if word is found in pre-trained embeddings,  \n",
        "    #update the matrix. if the word is not found,  \n",
        "    #the vector is zeros!  \n",
        "    if word in embedding_dict:  \n",
        "      embedding_matrix[i] = embedding_dict[word]  \n",
        "    #return embedding matrix  \n",
        "  return embedding_matrix  \n",
        "  \n",
        "def run(df, fold):  \n",
        "  \"\"\"  Run training and validation for a given fold  and dataset  \n",
        "  :param df: pandas dataframe with kfold column  \n",
        "  :param fold: current fold, int  \"\"\"  \n",
        "  #fetch training dataframe  \n",
        "  train_df = df[df.kfold != fold].reset_index(drop=True)  \n",
        "  #fetch validation dataframe\n",
        "  valid_df = df[df.kfold == fold].reset_index(drop=True)  \n",
        "  print(\"Fitting tokenizer\")  \n",
        "  #we use tf.keras for tokenization  \n",
        "  #you can use your own tokenizer and then you can \n",
        "   #get rid of tensorflow  \n",
        "   tokenizer = tf.keras.preprocessing.text.Tokenizer()  \n",
        "   tokenizer.fit_on_texts(df.review.values.tolist())  \n",
        "   #convert training data to sequences  \n",
        "   #for example: \"bad movie\" gets converted to  \n",
        "   #[24, 27] where 24 is the index for bad and 27 is the  \n",
        "   #index for movie  \n",
        "   xtrain = tokenizer.texts_to_sequences(train_df.review.values)  \n",
        "   #similarly convert validation data to  \n",
        "   #sequences  \n",
        "   xtest = tokenizer.texts_to_sequences(valid_df.review.values)  \n",
        "   #zero pad the training sequences given the maximum length  \n",
        "   #this padding is done on left hand side  \n",
        "   #if sequence is > MAX_LEN, it is truncated on left hand side too  \n",
        "   xtrain = tf.keras.preprocessing.sequence.pad_sequences(  xtrain, maxlen=config.MAX_LEN  )  \n",
        "   #zero pad the validation sequences  \n",
        "   xtest = tf.keras.preprocessing.sequence.pad_sequences(  xtest, maxlen=config.MAX_LEN  )  \n",
        "   #initialize dataset class for training  \n",
        "   train_dataset = dataset.IMDBDataset(  reviews=xtrain,  targets=train_df.sentiment.values  )  \n",
        "   #create torch dataloader for training  \n",
        "   #torch dataloader loads the data using dataset  \n",
        "   #class in batches specified by batch size  \n",
        "   train_data_loader = torch.utils.data.DataLoader(  train_dataset,  batch_size=config.TRAIN_BATCH_SIZE,  num_workers=2  )  \n",
        "   #initialize dataset class for validation\n",
        "   valid_dataset = dataset.IMDBDataset(  reviews=xtest,  targets=valid_df.sentiment.values  )  \n",
        "   #create torch dataloader for validation  \n",
        "   valid_data_loader = torch.utils.data.DataLoader(  valid_dataset,  batch_size=config.VALID_BATCH_SIZE,  num_workers=1  )  \n",
        "   print(\"Loading embeddings\")  \n",
        "   #load embeddings as shown previously  \n",
        "   embedding_dict = load_vectors(\"../input/crawl-300d-2M.vec\")  \n",
        "   embedding_matrix = create_embedding_matrix(  tokenizer.word_index, embedding_dict  )  \n",
        "   #create torch device, since we use gpu, we are using cuda  \n",
        "   device = torch.device(\"cuda\")  \n",
        "   #fetch our LSTM model  \n",
        "   model = lstm.LSTM(embedding_matrix) \n",
        "    #send model to device  \n",
        "  model.to(device)  \n",
        "  #initialize Adam optimizer  \n",
        "  optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)  \n",
        "  print(\"Training Model\")  \n",
        "  #set best accuracy to zero  \n",
        "  best_accuracy = 0  \n",
        "  #set early stopping counter to zero  \n",
        "  early_stopping_counter = 0  \n",
        "  #train and validate for all epochs  \n",
        "  for epoch in range(config.EPOCHS):  \n",
        "    #train one epoch  \n",
        "    engine.train(train_data_loader, model, optimizer, device)  \n",
        "    #validate  \n",
        "    outputs, targets = engine.evaluate(  valid_data_loader, model, device  )  \n",
        "    #use threshold of 0.5  \n",
        "    #please note we are using linear layer and no sigmoid\n",
        "    #you should do this 0.5 threshold after sigmoid  \n",
        "    outputs = np.array(outputs) >= 0.5  \n",
        "    #calculate accuracy  \n",
        "    accuracy = metrics.accuracy_score(targets, outputs)  \n",
        "    print(  f\"FOLD:{fold}, Epoch: {epoch}, Accuracy Score = {accuracy}\"  )  \n",
        "    #simple early stopping  \n",
        "    if accuracy > best_accuracy:  \n",
        "      best_accuracy = accuracy  \n",
        "    else:  \n",
        "      early_stopping_counter += 1  \n",
        "    if early_stopping_counter > 2:  \n",
        "      break  \n",
        "    if  __name__  ==  \"__main__\":  \n",
        "      #load data  df = pd.read_csv(\"../input/imdb_folds.csv\")  \n",
        "      #train for all folds  \n",
        "      run(df, fold=0)  \n",
        "      run(df, fold=1)  \n",
        "      run(df, fold=2)  \n",
        "      run(df, fold=3)  \n",
        "      run(df, fold=4)    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tRJcOdv9Xq92",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#config.py  \n",
        "#we define all the configuration here  \n",
        "MAX_LEN = 128  \n",
        "TRAIN_BATCH_SIZE = 16  \n",
        "VALID_BATCH_SIZE = 8  \n",
        "EPOCHS = 10 \n",
        "\n",
        "❯ python train.py  \n",
        "FOLD:0, Epoch: 3, Accuracy Score = 0.9015  \n",
        "FOLD:1, Epoch: 4, Accuracy Score = 0.9007  \n",
        "FOLD:2, Epoch: 3, Accuracy Score = 0.8924  \n",
        "FOLD:3, Epoch: 2, Accuracy Score = 0.9  \n",
        "FOLD:4, Epoch: 1, Accuracy Score = 0.878 "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VhxkxL0aYQkz",
        "colab_type": "text"
      },
      "source": [
        "You must have noticed that we used pre-trained embeddings and a simple bidirectional LSTM. If you want to change the model, you can just change model in  lstm.py and keep everything as it is. This kind of code requires minimal changes  for experiments and is easily understandable. For example, you can learn the  embeddings on your own instead of using pretrained embeddings, you can use some  other pretrained embeddings, you can combine multiple pretrained embeddings,  you can use GRU, you can use spatial dropout after embedded, you can add a GRU  layer after LSTM, you can add two LSTM layers, you can have LSTM-GRU-LSTM  config, you can replace LSTM with a convolutional layer, etc. without making  many changes to the code. Most of what I mention requires changes only to model  class.  When you use pretrained embeddings, try to see for how many words you are not  able to find embeddings and why. The more words for which you have pre-trained  embeddings, the better are the results. I present to you the following un-commented  (!) function that you can use to create embedding matrix for any kind of pre-trained  embedding which is in the same format as glove or fastText (some changes might  be needed.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gHOTN7_XYWMZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_embeddings(word_index, embedding_file, vector_length=300):  \n",
        "  \"\"\"  A general function to create embedding matrix  \n",
        "  :param word_index: word:index dictionary  \n",
        "  :param embedding_file: path to embeddings file  \n",
        "  :param vector_length: length of vector  \"\"\"  \n",
        "  max_features = len(word_index) + 1  \n",
        "  words_to_find = list(word_index.keys())  \n",
        "  more_words_to_find = []\n",
        "  for wtf in words_to_find:  \n",
        "    more_words_to_find.append(wtf)  \n",
        "    more_words_to_find.append(str(wtf).capitalize())  \n",
        "  more_words_to_find = set(more_words_to_find)  \n",
        "  def get_coefs(word, *arr):  \n",
        "    return word, np.asarray(arr, dtype='float32')  \n",
        "  embeddings_index = dict(  get_coefs(*o.strip().split(\"\"))  \n",
        "  for o in open(embedding_file)  \n",
        "  if o.split(\" \")[0]  \n",
        "  in more_words_to_find  \n",
        "  and len(o) > 100  )  \n",
        "  embedding_matrix = np.zeros((max_features, vector_length))  \n",
        "  for word, i in word_index.items():  \n",
        "    if i >= max_features:  \n",
        "      continue  \n",
        "    embedding_vector = embeddings_index.get(word)  \n",
        "    if embedding_vector is None:  \n",
        "      embedding_vector = embeddings_index.get(  str(word).capitalize()  )  \n",
        "    if embedding_vector is None:  \n",
        "      embedding_vector = embeddings_index.get(  str(word).upper()  )  \n",
        "    if (embedding_vector is not None  and len(embedding_vector) == vector_length):  \n",
        "      embedding_matrix[i] = embedding_vector  \n",
        "  return embedding_matrix  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a1PaR9jqZe5-",
        "colab_type": "text"
      },
      "source": [
        "The function can also  be modified to use stemmed words or lemmatized words. In the end, you want to  have the least number of unknown words in your training corpus. One more trick is  to learn the embedding layer, i.e., make it trainable and then train the network. \n",
        "\n",
        "So far, we have built a lot of models for a classification problem. However, it is the  era of muppets, and more and more people are moving towards transformer-based  models. Transformer based networks are able to handle dependencies which are  long term in nature. LSTM looks at the next word only when it has seen the previous  word. This is not the case with transformers. It can look at all the words in the whole sentence simultaneously. Due to this, one more advantage is that it can easily be  parallelized and uses GPUs more efficiently.  Transformers is a very broad topic, and there are too many models: BERT,  RoBERTa, XLNet, XLM-RoBERTa, T5, etc. I will show you a general approach  that you can use for all these models (except T5) for the classification problem that  we have been discussing. Please note that these transformers are hungry in terms of  computational power needed to train them. Thus, if you do not have a high-end  system, it might take much longer to train a model compared to LSTM or TF-IDF  based models. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-HPDLNrtaL45",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#config.py  \n",
        "import transformers  \n",
        "#this is the maximum number of tokens in the sentence  \n",
        "MAX_LEN = 512  \n",
        "#batch sizes is small because model is huge!  \n",
        "TRAIN_BATCH_SIZE = 8  \n",
        "VALID_BATCH_SIZE = 4  \n",
        "#let's train for a maximum of 10 epochs  \n",
        "EPOCHS = 10  \n",
        "#define path to BERT model files  \n",
        "BERT_PATH = \"../input/bert_base_uncased/\" \n",
        " #this is where you want to save the model  \n",
        "MODEL_PATH = \"model.bin\"  \n",
        "#training file  \n",
        "TRAINING_FILE = \"../input/imdb.csv\"  \n",
        "#define the tokenizer  \n",
        "#we use tokenizer and model  \n",
        "#from huggingface's transformers  \n",
        "TOKENIZER = transformers.BertTokenizer.from_pretrained(  BERT_PATH,  do_lower_case=True  ) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kMMrhDkhabKw",
        "colab_type": "text"
      },
      "source": [
        "The config file here is the only place where we define tokenizer and other  parameters we would like to change frequently—this way we can do many  experiments without requiring a lot of changes. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wq2UrtOFab9H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#dataset.py  \n",
        "import config  \n",
        "import torch  \n",
        "class BERTDataset:  \n",
        "  def __init__(self, review, target):  \n",
        "    \"\"\"  :param review: list or numpy array of strings  \n",
        "    :param targets: list or numpy array which is binary  \"\"\"  \n",
        "    self.review = review  \n",
        "    self.target = target  \n",
        "    #we fetch max len and tokenizer from config.py  \n",
        "    self.tokenizer = config.TOKENIZER  \n",
        "    self.max_len = config.MAX_LEN  \n",
        "  def __len__(self):  \n",
        "    #this returns the length of dataset  \n",
        "    return len(self.review)  \n",
        "  def __getitem__(self, item):  \n",
        "    #for a given item index, return a dictionary  \n",
        "    #of inputs  \n",
        "    review = str(self.review[item])  \n",
        "    review = \" \".join(review.split())  \n",
        "    #encode_plus comes from hugginface's transformers  \n",
        "    #and exists for all tokenizers they offer  \n",
        "    #it can be used to convert a given string  \n",
        "    #to ids, mask and token type ids which are  \n",
        "    #needed for models like BERT  \n",
        "    #here, review is a string  \n",
        "    inputs = self.tokenizer.encode_plus(  review,  None,  add_special_tokens=True,  max_length=self.max_len,  pad_to_max_length=True,)\n",
        "    #ids are ids of tokens generated  \n",
        "    #after tokenizing reviews  \n",
        "    ids = inputs[\"input_ids\"]  \n",
        "    #mask is 1 where we have input  \n",
        "    #and 0 where we have padding  \n",
        "    mask = inputs[\"attention_mask\"]  \n",
        "    #token type ids behave the same way as  \n",
        "    #mask in this specific case  \n",
        "    #in case of two sentences, this is 0  \n",
        "    #for first sentence and 1 for second sentence  \n",
        "    token_type_ids = inputs[\"token_type_ids\"]  \n",
        "    #now we return everything  \n",
        "    #note that ids, mask and token_type_ids  \n",
        "    #are all long datatypes and targets is float  \n",
        "    return {  \"ids\": torch.tensor(  ids, dtype=torch.long  ), \n",
        "            \"mask\": torch.tensor(  mask, dtype=torch.long  ), \n",
        "            \"token_type_ids\": torch.tensor(  token_type_ids, dtype=torch.long  ), \n",
        "            \"targets\": torch.tensor(  self.target[item], dtype=torch.float  )  }"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UvMKvuIVblzP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Model.py\n",
        "#model.py  \n",
        "import config  \n",
        "import transformers  \n",
        "import torch.nn as nn  \n",
        "class BERTBaseUncased(nn.Module):  \n",
        "  def __init__(self):  \n",
        "    super(BERTBaseUncased, self).__init__()  \n",
        "    #we fetch the model from the BERT_PATH defined in  \n",
        "    #config.py  \n",
        "    self.bert = transformers.BertModel.from_pretrained( config.BERT_PATH  )  \n",
        "    #add a dropout for regularization  \n",
        "    self.bert_drop = nn.Dropout(0.3)  \n",
        "    #a simple linear layer for output  \n",
        "    #yes, there is only one output  \n",
        "    self.out = nn.Linear(768, 1)  \n",
        "  def forward(self, ids, mask, token_type_ids):  \n",
        "    #BERT in its default settings returns two outputs  \n",
        "    #last hidden state and output of bert pooler layer  \n",
        "    #we use the output of the pooler which is of the size  \n",
        "    #(batch_size, hidden_size)  \n",
        "    #hidden size can be 768 or 1024 depending on  \n",
        "    #if we are using bert base or large respectively  \n",
        "    #in our case, it is 768  \n",
        "    #note that this model is pretty simple  \n",
        "    #you might want to use last hidden state  \n",
        "    #or several hidden states  \n",
        "    _, o2 = self.bert(  ids,  attention_mask=mask,  token_type_ids=token_type_ids  )  \n",
        "    #pass through dropout layer  \n",
        "    bo = self.bert_drop(o2)  \n",
        "    #pass through linear layer  \n",
        "    output = self.out(bo)  \n",
        "    #return output  \n",
        "    return output "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fo0EMV48bkyC",
        "colab_type": "text"
      },
      "source": [
        "This model returns a single output. We can use binary cross-entropy loss with  logits which first applies sigmoid and then calculates the loss. This is done in  engine.py.  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pRk1hT4PcNlv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#engine.py  \n",
        "import torch  \n",
        "import torch.nn as nn  \n",
        "def loss_fn(outputs, targets):  \n",
        "  \"\"\"  This function returns the loss.\n",
        "  :param outputs: output from the model (real numbers)  \n",
        "  :param targets: input targets (binary)  \"\"\"  \n",
        "  return nn.BCEWithLogitsLoss()(outputs, targets.view(-1, 1))  \n",
        "def train_fn(data_loader, model, optimizer, device, scheduler):  \n",
        "  \"\"\"  This is the training function which trains for one epoch  \n",
        "  :param data_loader: it is the torch dataloader object  \n",
        "  :param model: torch model, bert in our case  \n",
        "  :param optimizer: adam, sgd, etc  \n",
        "  :param device: can be cpu or cuda  \n",
        "  :param scheduler: learning rate scheduler  \"\"\"  \n",
        "  #put the model in training mode  \n",
        "  model.train()  \n",
        "  #loop over all batches  \n",
        "  for d in data_loader:  \n",
        "    #extract ids, token type ids and mask  \n",
        "    #from current batch  \n",
        "    #also extract targets  \n",
        "    ids = d[\"ids\"]  \n",
        "    token_type_ids = d[\"token_type_ids\"]  \n",
        "    mask = d[\"mask\"]  \n",
        "    targets = d[\"targets\"]  \n",
        "    #move everything to specified device  \n",
        "    ids = ids.to(device, dtype=torch.long)  \n",
        "    token_type_ids = token_type_ids.to(device, dtype=torch.long)  \n",
        "    mask = mask.to(device, dtype=torch.long)  \n",
        "    targets = targets.to(device, dtype=torch.float)  \n",
        "    #zero-grad the optimizer  \n",
        "    optimizer.zero_grad()  \n",
        "    #pass through the model  \n",
        "    outputs = model(  ids=ids,  mask=mask,  token_type_ids=token_type_ids  )  \n",
        "    #calculate loss  \n",
        "    loss = loss_fn(outputs, targets)  \n",
        "    #backward step the loss  \n",
        "    loss.backward()  \n",
        "    #step optimizer\n",
        "    optimizer.step()  \n",
        "    #step scheduler  \n",
        "    scheduler.step()\n",
        "\n",
        "def eval_fn(data_loader, model, device):  \n",
        "  \"\"\"  this is the validation function that generates  predictions on validation data  \n",
        "  :param data_loader: it is the torch dataloader object  \n",
        "  :param model: torch model, bert in our case  \n",
        "  :param device: can be cpu or cuda  \n",
        "  :return: output and targets  \"\"\"  \n",
        "  #put model in eval mode  model.eval()  \n",
        "  #initialize empty lists for  \n",
        "  #targets and outputs  \n",
        "  fin_targets = []  \n",
        "  fin_outputs = []  \n",
        "  #use the no_grad scope  \n",
        "  #its very important else you might  \n",
        "  #run out of gpu memory  \n",
        "  with torch.no_grad():  \n",
        "    #this part is same as training function \n",
        "    #except for the fact that there is no  \n",
        "    #zero_grad of optimizer and there is no loss  \n",
        "    #calculation or scheduler steps.  \n",
        "    for d in data_loader:  \n",
        "      ids = d[\"ids\"]  \n",
        "      token_type_ids = d[\"token_type_ids\"]  \n",
        "      mask = d[\"mask\"]  \n",
        "      targets = d[\"targets\"]  \n",
        "      ids = ids.to(device, dtype=torch.long)  \n",
        "      token_type_ids = token_type_ids.to(device, dtype=torch.long)  \n",
        "      mask = mask.to(device, dtype=torch.long)  \n",
        "      targets = targets.to(device, dtype=torch.float)  \n",
        "      outputs = model(  ids=ids,  mask=mask,  token_type_ids=token_type_ids  )  \n",
        "      #convert targets to cpu and extend the final list  \n",
        "      targets = targets.cpu().detach()  \n",
        "      fin_targets.extend(targets.numpy().tolist()) \n",
        "      #convert outputs to cpu and extend the final list  \n",
        "      outputs = torch.sigmoid(outputs).cpu().detach()  \n",
        "      fin_outputs.extend(outputs.numpy().tolist())  \n",
        "  return fin_outputs, fin_targets \n",
        "     "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DoMx3KTOdoqz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#train.py  \n",
        "import config  \n",
        "import dataset  \n",
        "import engine  \n",
        "import torch  \n",
        "import pandas as pd  \n",
        "import torch.nn as nn  \n",
        "import numpy as np  \n",
        "from model import BERTBaseUncased  \n",
        "from sklearn import model_selection  \n",
        "from sklearn import metrics  \n",
        "from transformers import AdamW  \n",
        "from transformers import get_linear_schedule_with_warmup  \n",
        "\n",
        "def train():  \n",
        "  #this function trains the model  \n",
        "  #read the training file and fill NaN values with \"none\"  \n",
        "  #you can also choose to drop NaN values in this  \n",
        "  #specific dataset  \n",
        "  dfx = pd.read_csv(config.TRAINING_FILE).fillna(\"none\")  \n",
        "  #sentiment = 1 if its positive  \n",
        "  #else sentiment = 0  \n",
        "  dfx.sentiment = dfx.sentiment.apply(  lambda x: 1 if x == \"positive\" else 0  )  \n",
        "  #we split the data into single training  \n",
        "  #and validation fold  \n",
        "  df_train, df_valid = model_selection.train_test_split(  dfx,  test_size=0.1,  random_state=42,  stratify=dfx.sentiment.values )\n",
        "\n",
        "  #reset index  \n",
        "  df_train = df_train.reset_index(drop=True)  \n",
        "  df_valid = df_valid.reset_index(drop=True)  \n",
        "  \n",
        "  #initialize BERTDataset from dataset.py  \n",
        "  #for training dataset  \n",
        "  train_dataset = dataset.BERTDataset(  review=df_train.review.values,  target=df_train.sentiment.values  )  \n",
        "  #create training dataloader  \n",
        "  train_data_loader = torch.utils.data.DataLoader(  train_dataset,  batch_size=config.TRAIN_BATCH_SIZE,  num_workers=4  )  \n",
        "  #initialize BERTDataset from dataset.py  \n",
        "  #for validation dataset  \n",
        "  valid_dataset = dataset.BERTDataset(  review=df_valid.review.values,  target=df_valid.sentiment.values  )  \n",
        "  #create validation data loader  \n",
        "  valid_data_loader = torch.utils.data.DataLoader(  valid_dataset,  batch_size=config.VALID_BATCH_SIZE,  num_workers=1  )  \n",
        "  #initialize the cuda device  \n",
        "  #use cpu if you dont have GPU  \n",
        "  device = torch.device(\"cuda\")  \n",
        "  #load model and send it to the device  \n",
        "  model = BERTBaseUncased()  model.to(device)  \n",
        "  #create parameters we want to optimize  \n",
        "  #we generally dont use any decay for bias  \n",
        "  #and weight layers  \n",
        "  param_optimizer = list(model.named_parameters())  \n",
        "  no_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]  \n",
        "  optimizer_parameters = [ {  \"params\": [  p for n, p in param_optimizer if  not any(nd in n for nd in no_decay)  ],  \"weight_decay\": 0.001,  },\n",
        "                          {  \"params\": [  p for n, p in param_optimizer if  any(nd in n for nd in no_decay)  ],  \"weight_decay\": 0.0,  },  ]  \n",
        "  #calculate the number of training steps  \n",
        "  #this is used by scheduler  \n",
        "  num_train_steps = int(  len(df_train)/ config.TRAIN_BATCH_SIZE * config.EPOCHS  )  \n",
        "  #AdamW optimizer  \n",
        "  #AdamW is the most widely used optimizer  \n",
        "  #for transformer based networks  \n",
        "  optimizer = AdamW(optimizer_parameters, lr=3e-5)  \n",
        "  #fetch a scheduler  \n",
        "  #you can also try using reduce lr on plateau  \n",
        "  scheduler = get_linear_schedule_with_warmup(  optimizer,  num_warmup_steps=0,  \n",
        "                                              num_training_steps=num_train_steps  )  \n",
        "  #if you have multiple GPUs  #model model to DataParallel  \n",
        "  #to use multiple GPUs  \n",
        "  model = nn.DataParallel(model)  \n",
        "  #start training the epochs  \n",
        "  best_accuracy = 0  \n",
        "  for epoch in range(config.EPOCHS):  \n",
        "    engine.train_fn(  train_data_loader, model, optimizer, device, scheduler  )  \n",
        "    outputs, targets = engine.eval_fn(valid_data_loader, model, device  )  \n",
        "    outputs = np.array(outputs) >= 0.5  accuracy = metrics.accuracy_score(targets, outputs)  \n",
        "    print(f\"Accuracy Score = {accuracy}\")  \n",
        "    if accuracy > best_accuracy:  \n",
        "      torch.save(model.state_dict(), config.MODEL_PATH)  \n",
        "      best_accuracy = accuracy  \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i6W5Gb2we5uW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if  __name__  ==  \"__main__\":  \n",
        "  train() "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eQ7fkCUOe6y0",
        "colab_type": "text"
      },
      "source": [
        "We could improve that model probably  by a percent by using different data processing or by tuning the parameters such as  layers, nodes, dropout, learning rate, changing the optimizer, etc. Then we will have  ~2% benefit from BERT. BERT, on the other hand, took much longer to train, has  a lot of parameters and is also slow when it comes to inference. In the end, you  should look at your business and choose wisely. Don’t choose BERT only because  it’s “cool”.  It must be noted that the only task we discussed here is classification but changing  it to regression, multi-label or multi-class will require only a couple of lines of code  changes. For example, the same problem in multi-class classification setting will  have multiple outputs and Cross-Entropy loss. Everything else should remain the  same. Natural language processing is huge, and we discussed only a small fraction  of it. Apparently, this is a huge fraction as most of the industrial models are  classification or regression models."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ueHUeFLlCpc0",
        "colab_type": "text"
      },
      "source": [
        "#Approaching ensembling and stacking "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5_F7URvCFGRR",
        "colab_type": "text"
      },
      "source": [
        "When we hear these two words, the first thing that comes to our mind is that it’s all  about online/offline machine learning competitions. This used to be the case a few  years ago, but now with the advancements in computing power and cheaper virtual  instances, people have started using ensemble models even in industries. For  example, it’s very easy to deploy multiple neural networks and serve them in realtime with a response time of less than 500ms. Sometimes, a huge neural network or  a large model can also be replaced by a few other models which are small in size,  perform similar to the large model and are twice as fast. If this is the case, which  model(s) will you choose? I, personally, would prefer multiple small models, which  are faster and give the same performance as a much larger and slower model. Please  remember that smaller models are also easier and faster to tune.  Ensembling is nothing but a combination of different models. The models can be  combined by their predictions/probabilities. The simplest way to combine models  would be just to do an average.  \n",
        "\n",
        "Ensemble Probabilities = (M1_proba + M2_proba + … + Mn_Proba)/ n  \n",
        "\n",
        "This is simple and yet the most effective way of combining models. In simple  averaging, the weights are equal to all models. One thing that you should keep in  mind for any method of combining is that you should always combine  predictions/probabilities of models which are different from each other. In simple  words, the combination of models which are not highly correlated works better than  the combination of models which are very correlated with each other. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ydDRxHYhF6sf",
        "colab_type": "text"
      },
      "source": [
        "If you do not have probabilities, you can combine predictions too. The most simple  way of doing this is to take a vote. Suppose we are doing a multi-class classification  with three classes: 0, 1 and 2.  \n",
        "[0, 0, 1]: Highest voted class: 0  \n",
        "[0, 1, 2]: Highest voted class: None (Choose one randomly)  \n",
        "[2, 2, 2]: Highest voted class: 2  \n",
        "\n",
        "The following simple functions can accomplish these simple operations. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6-nNQtjkCr8b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np  \n",
        "def mean_predictions(probas):  \n",
        "  \"\"\"  Create mean predictions  \n",
        "  :param probas: 2-d array of probability values  \n",
        "  :return: mean probability  \"\"\"  \n",
        "  return np.mean(probas, axis=1)  \n",
        "def max_voting(preds):  \n",
        "  \"\"\"  Create mean predictions  \n",
        "  :param probas: 2-d array of prediction values  \n",
        "  :return: max voted predictions  \"\"\"  \n",
        "  idxs = np.argmax(preds, axis=1)  \n",
        "  return np.take_along_axis(preds, idxs[:, None], axis=1) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OAQaUgmwZooY",
        "colab_type": "text"
      },
      "source": [
        "probas have a single probability (i.e. binary classification, usually  class 1) in each column. Each column is thus a new model. Similarly, for preds,  each column is a prediction from different models. Both these functions assume a  2-dimensional numpy array. You can modify it according to your requirements. For  example, you might have a 2-d array of probabilities for each model. In that case,  the function will change a bit. Another way of combining multiple models is by  ranks of their probabilities. This type of combination works quite good when the  concerned metric is the area under curve as AUC is all about ranking samples. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TN1bt0R1aBDq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def rank_mean(probas):  \n",
        "  \"\"\"  Create mean predictions using ranks  \n",
        "  :param probas: 2-d array of probability values  \n",
        "  :return: mean ranks  \"\"\"  \n",
        "  ranked = []  \n",
        "  for i in range(probas.shape[1]):  \n",
        "    rank_data = stats.rankdata(probas[:, i])  \n",
        "    ranked.append(rank_data)\n",
        "    ranked = np.column_stack(ranked)  \n",
        "  return np.mean(ranked, axis=1)  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3PYoiehsa-Mc",
        "colab_type": "text"
      },
      "source": [
        "Probabilities can also be combined by weights.  \n",
        "Final Probabilities = w1*M1_proba + w2*M2_proba + … + wn*Mn_proba  Where (w1 + w2 + w3 + … + wn) = 1.0  \n",
        "\n",
        "For example, if you have a random forest model that gives very high AUC and a  logistic regression model with a little lower AUC, you can combine them with 70% for random forest and 30% for logistic regression. So, how did I come up with these  numbers? Let’s add another model, let’s say now we also have an xgboost model  that gives an AUC higher than random forest. Now I will combine them with a ratio  of 3:2:1 for xgboost: random forest: logistic regression.\n",
        "\n",
        "Assume that we have three monkeys with three knobs with values that range  between 0 and 1. These monkeys turn the knobs, and we calculate the AUC score  at each value they turn the knob to. Eventually, the monkeys will find a combination  that gives the best AUC. Yes, it is a random search! Before doing these kinds of  searches, you must remember the two most important rules of ensembling. \n",
        "\n",
        "1.**The first rule of ensembling is that you always create folds before starting with  ensembling.**\n",
        "\n",
        "\n",
        "\n",
        "2.**The second rule of ensembling is that you always create folds before starting with  ensembling.**\n",
        "\n",
        "YES. These are the two most important rules, and no, there is no mistake in what I  wrote. The first step is to create folds. For simplicity, let’s say we divide the data  into two parts: fold 1 and fold 2. Please note that this is only done for simplicity in  explaining. In a real-world scenario, you should create more folds.  Now, we train our random forest model, logistic regression model and our xgboost  model on fold 1 and make predictions on fold 2. After this, we train the models  from scratch on fold 2 and make predictions on fold 1. Thus, we have created  predictions for all of the training data. Now to combine these models, we take fold  1 and all the predictions for fold 1 and create an optimization function that tries to  find the best weights so as to minimize error or maximize AUC against the targets  for fold 2. So, we are kind of training an optimization model on fold 1 with the  predicted probabilities for the three models and evaluating it on fold 2. Let’s first  look at a class we can use to find the best weights of multiple models to optimize  for AUC"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w4nMfzIiafBy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np  \n",
        "from functools import partial  \n",
        "from scipy.optimize import fmin  \n",
        "from sklearn import metrics\n",
        "\n",
        "class OptimizeAUC:  \n",
        "  \"\"\"  Class for optimizing AUC.  This class is all you need to find best weights for  \n",
        "  any model and for any metric and for any types of predictions.  With very small changes, \n",
        "  this class can be used for optimization of  weights in ensemble models of _any_ type of predictions  \"\"\"  \n",
        "  \n",
        "  def __init__(self):  \n",
        "    self.coef_  = 0  \n",
        "    \n",
        "  def _auc(self, coef, X, y):  \n",
        "    \"\"\"  This functions calulates and returns AUC.  \n",
        "    :param coef: coef list, of the same length as number of models  \n",
        "    :param X: predictions, in this case a 2d array  \n",
        "    :param y: targets, in our case binary 1d array  \"\"\"  \n",
        "    \n",
        "    #multiply coefficients with every column of the array  \n",
        "    #with predictions.  \n",
        "    #this means: element 1 of coef is multiplied by column 1  \n",
        "    #of the prediction array, element 2 of coef is multiplied  \n",
        "    #by column 2 of the prediction array and so on!  \n",
        "    x_coef = X * coef  \n",
        "    #create predictions by taking row wise sum  \n",
        "    predictions = np.sum(x_coef, axis=1)  \n",
        "    #calculate auc score  \n",
        "    auc_score = metrics.roc_auc_score(y, predictions)  \n",
        "    #return negative auc  \n",
        "    return -1.0 * auc_score  \n",
        "  def fit(self, X, y):  \n",
        "    #remember partial from hyperparameter optimization chapter?  \n",
        "    loss_partial = partial(self._auc, X=X, y=y)  \n",
        "    #dirichlet distribution. you can use any distribution you want  \n",
        "    #to initialize the coefficients  \n",
        "    #we want the coefficients to sum to 1  \n",
        "    initial_coef = np.random.dirichlet(np.ones(X.shape[1]), size=1)  \n",
        "    #use scipy fmin to minimize the loss function, in our case auc  \n",
        "    self.coef_ = fmin(loss_partial, initial_coef, disp=True)  \n",
        "    \n",
        "  def predict(self, X):\n",
        "    #this is similar to  _auc function  \n",
        "    x_coef = X * self.coef_  \n",
        "    predictions = np.sum(x_coef, axis=1)  \n",
        "    return predictions  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kMPUqjWwhofK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Using simple averaging\n",
        "\n",
        "import xgboost as xgb  \n",
        "from sklearn.datasets import make_classification  \n",
        "from sklearn import ensemble  \n",
        "from sklearn import linear_model  \n",
        "from sklearn import metrics  \n",
        "from sklearn import model_selection  \n",
        "\n",
        "#make a binary classification dataset with 10k samples  \n",
        "#and 25 features  \n",
        "X, y = make_classification(n_samples=10000, n_features=25)  \n",
        "#split into two folds (for this example)  \n",
        "xfold1, xfold2, yfold1, yfold2 = model_selection.train_test_split(  X,  y,  test_size=0.5,  stratify=y  )  \n",
        "#fit models on fold 1 and make predictions on fold 2  \n",
        "#we have 3 models:  \n",
        "#logistic regression, random forest and xgboost  \n",
        "logreg = linear_model.LogisticRegression()  \n",
        "rf = ensemble.RandomForestClassifier()  \n",
        "xgbc = xgb.XGBClassifier()  \n",
        "#fit all models on fold 1 data  \n",
        "logreg.fit(xfold1, yfold1)  \n",
        "rf.fit(xfold1, yfold1)  \n",
        "xgbc.fit(xfold1, yfold1)  \n",
        "#predict all models on fold 2  \n",
        "#take probability for class 1  \n",
        "pred_logreg = logreg.predict_proba(xfold2)[:, 1]  \n",
        "pred_rf = rf.predict_proba(xfold2)[:, 1]  \n",
        "pred_xgbc = xgbc.predict_proba(xfold2)[:, 1]\n",
        "#create an average of all predictions  \n",
        "#that is the simplest ensemble  \n",
        "avg_pred = (pred_logreg + pred_rf + pred_xgbc)/ 3  \n",
        "#a 2d array of all predictions  \n",
        "fold2_preds = np.column_stack((  pred_logreg,  pred_rf,  pred_xgbc,  avg_pred  ))  \n",
        "#calculate and store individual AUC values  \n",
        "aucs_fold2 = []  \n",
        "for i in range(fold2_preds.shape[1]):  \n",
        "  auc = metrics.roc_auc_score(yfold2, fold2_preds[:, i])  \n",
        "  aucs_fold2.append(auc)  \n",
        "print(f\"Fold-2: LR AUC = {aucs_fold2[0]}\")  \n",
        "print(f\"Fold-2: RF AUC = {aucs_fold2[1]}\")  \n",
        "print(f\"Fold-2: XGB AUC = {aucs_fold2[2]}\")  \n",
        "print(f\"Fold-2: Average Pred AUC = {aucs_fold2[3]}\")  \n",
        "#now we repeat the same for the other fold  \n",
        "#this is not the ideal way, if you ever have to repeat code,  \n",
        "#create a function!  \n",
        "#fit models on fold 2 and make predictions on fold 1  \n",
        "logreg = linear_model.LogisticRegression()  \n",
        "rf = ensemble.RandomForestClassifier()  \n",
        "xgbc = xgb.XGBClassifier()  \n",
        "logreg.fit(xfold2, yfold2)  \n",
        "rf.fit(xfold2, yfold2)  \n",
        "xgbc.fit(xfold2, yfold2)  \n",
        "pred_logreg = logreg.predict_proba(xfold1)[:, 1]  \n",
        "pred_rf = rf.predict_proba(xfold1)[:, 1]  \n",
        "pred_xgbc = xgbc.predict_proba(xfold1)[:, 1]  \n",
        "avg_pred = (pred_logreg + pred_rf + pred_xgbc)/ 3  \n",
        "fold1_preds = np.column_stack((  pred_logreg,  pred_rf,  pred_xgbc,  avg_pred  ))\n",
        "aucs_fold1 = []  \n",
        "for i in range(fold1_preds.shape[1]):  \n",
        "  auc = metrics.roc_auc_score(yfold1, fold1_preds[:, i])  \n",
        "  aucs_fold1.append(auc)  \n",
        "  \n",
        "print(f\"Fold-1: LR AUC = {aucs_fold1[0]}\")  \n",
        "print(f\"Fold-1: RF AUC = {aucs_fold1[1]}\")  \n",
        "print(f\"Fold-1: XGB AUC = {aucs_fold1[2]}\")  \n",
        "print(f\"Fold-1: Average prediction AUC = {aucs_fold1[3]}\")  \n",
        "\n",
        "#find optimal weights using the optimizer  \n",
        "opt = OptimizeAUC()  \n",
        "#dont forget to remove the average column  \n",
        "opt.fit(fold1_preds[:, :-1], yfold1)  \n",
        "opt_preds_fold2 = opt.predict(fold2_preds[:, :-1])  \n",
        "auc = metrics.roc_auc_score(yfold2, opt_preds_fold2)  \n",
        "print(f\"Optimized AUC, Fold 2 = {auc}\")  \n",
        "print(f\"Coefficients = {opt.coef_}\")  \n",
        "\n",
        "opt = OptimizeAUC()  \n",
        "opt.fit(fold2_preds[:, :-1], yfold2)  \n",
        "opt_preds_fold1 = opt.predict(fold1_preds[:, :-1])  \n",
        "auc = metrics.roc_auc_score(yfold1, opt_preds_fold1)  \n",
        "print(f\"Optimized AUC, Fold 1 = {auc}\")  \n",
        "print(f\"Coefficients = {opt.coef_}\")   "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bZShuXeHjzFQ",
        "colab_type": "text"
      },
      "source": [
        "❯ python auc_opt.py  \n",
        "Fold-2: LR AUC = 0.9145446769443348  \n",
        "Fold-2: RF AUC = 0.9269918948683287 \n",
        "Fold-2: XGB AUC = 0.9302436595508696  \n",
        "Fold-2: Average Pred AUC = 0.927701495890154  \n",
        "\n",
        "Fold-1: LR AUC = 0.9050872233256017  \n",
        "Fold-1: RF AUC = 0.9179382818311258  \n",
        "Fold-1: XGB AUC = 0.9195837242005629  \n",
        "Fold-1: Average prediction AUC = 0.9189669233123695  \n",
        "\n",
        "Optimization terminated successfully.  \n",
        "Current function value: -0.920643  \n",
        "Iterations: 50  \n",
        "Function evaluations: 109  \n",
        "\n",
        "Optimized AUC, \n",
        "Fold 2 = 0.9305386199756128  \n",
        "\n",
        "Coefficients = [-0.00188194 0.19328336 0.35891836] \n",
        "\n",
        "Optimization terminated successfully.  \n",
        "\n",
        "Current function value: -0.931232  \n",
        "Iterations: 56  \n",
        "Function evaluations: 113  \n",
        "Optimized AUC, \n",
        "Fold 1 = 0.9192523637234037  \n",
        "Coefficients = [-0.15655124 0.22393151 0.58711366]  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fC1QY4iUlB6y",
        "colab_type": "text"
      },
      "source": [
        "We see that average is better but using the optimizer to find the threshold is even  better! Sometimes, the average is the best choice. As you can see, the coefficients  do not add up to 1.0, but that’s okay as we are dealing with AUC and AUC cares  only about ranks.  Even random forest is an ensemble model. Random forest is just a combination of  many simple decision trees. Random forest comes in a category of ensemble models  which is popularly known as bagging. In bagging, we create small subsets of data  and train multiple simple models. The final result is obtained by a combination of  predictions, such as average, of all such small models.  And the xgboost model that we used is also an ensemble model. \n",
        "\n",
        "All gradient  boosting models are ensemble models and come under the umbrella name:  boosting. Boosting models work similar to bagging models, except for the fact that  consecutive models in boosting are trained on error residuals and tend to minimize  the errors of preceding models. This way, boosting models can learn the data  perfectly and are thus susceptible to overfitting.  What we saw in the code snippets till now considers only one column. \n",
        "\n",
        "This is not  always the case, and there will be many times when you have to deal with multiple  columns for predictions. For example, you might have a problem where you are  predicting one class out of multiple classes, i.e., multi-class classification problem.  For a multi-class classification problem, you can easily choose the voting approach.  But voting might not always be the best approach. If you want to combine the  probabilities, you will have a two-dimensional array instead of a vector as we had  previously when we were optimizing for AUC. With multiple classes, you can try  optimizing for log-loss instead (or some other business-relevant metric). To  combine, you can use a list of numpy arrays instead of a numpy array in the fit  function (X) and subsequently, you would also need to change the optimizer and the  predict function."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5XBmSMcllQlI",
        "colab_type": "text"
      },
      "source": [
        "STACKING\n",
        "\n",
        "It is pretty straightforward. If you have correct crossvalidation and keep the folds same throughout the journey of your modelling task,  nothing should overfit.  \n",
        "\n",
        "Let me describe the idea to you in simple points.  \n",
        "\n",
        "- Divide the training data into folds.  \n",
        "\n",
        "- Train a bunch of models: M1, M2…..Mn.  \n",
        "\n",
        "- Create full training predictions (using out of fold training) and test  predictions using all these models.  \n",
        "\n",
        "- Till here it is Level – 1 (L1).  \n",
        "\n",
        "- Use the fold predictions from these models as features to another model.  This is now a Level – 2 (L2) model.  \n",
        "\n",
        "- Use the same folds as before to train this L2 model.  \n",
        "\n",
        "- Now create OOF (out of fold) predictions on the training set and the test  set.  \n",
        "\n",
        "- Now you have L2 predictions for training data and also the final test set  predictions. \n",
        "\n",
        "You can keep repeating the L1 part and can create as many levels as you want.  Sometimes, you will also come across a term called blending. If you do, don’t  worry about it much. It is nothing but stacking with a holdout set instead of multiple  folds.  It must be noted that what I have described in this chapter can be applied to any  kind of problem: classification, regression, multi-label classification, etc. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T-QnPsLhoyGE",
        "colab_type": "text"
      },
      "source": [
        "**CODE REPRODUCTION AND MODEL SERVING**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5XbKcYczpBEN",
        "colab_type": "text"
      },
      "source": [
        "anymore. The preferred way of sharing code and  collaborating with others is by using a source code management system. Git is one  of the most popular source code management systems. So, let’s say you have  learned git and formatted the code properly, have written proper documentation and  have open-sourced your project. Is that enough? No. It’s not. It’s because you wrote  code on your computer and that might not work on someone else’s computer  because of many different reasons. So, it would be nice if when you distribute the  code, you could replicate your computer and others can too when they install your  software or run your code. To do this, the most popular way these days is to use  Docker Containers. To use docker containers, you need to install docker. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dvIBb6FokL_C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "$sudo apt install docker.io  \n",
        "$sudo systemctl start docker  \n",
        "$sudo systemctl enable docker  \n",
        "$sudo groupadd docker  \n",
        "$sudo usermod -aG docker $USER "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uNmTFcLUpVKX",
        "colab_type": "text"
      },
      "source": [
        "These commands work in Ubuntu 18.04. The best thing about docker is that it can  be installed on any machine: Linux, Windows, OSX. So, it doesn’t matter which  machine you have if you work inside the docker container all the time!  Docker containers can be considered as small virtual machines. You can create a  container for your code, and then everyone will be able to use it and access it. Let’s  see how we can create containers that can be used for training a model. We will use  the BERT model that we trained in the natural language processing chapter and try  to containerize the training code. \n",
        "\n",
        "#requirements.txt\n",
        "The file consists of all the python libraries that you are using in your  project. That is the python libraries that can be downloaded via PyPI (pip). For  training our BERT model to detect positive/negative sentiment, we use torch,  transformers, tqdm, scikit-learn, pandas and numpy. Let’s write them in  requirements.txt. You can just write the names, or you can also include the version.  It’s always the best to include version, and that’s what you should do. When you  include version, it makes sure that others have the same version as yours and not  the latest version as the latest version might change something"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yVOQXUeeprB5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#requirements.txt  \n",
        "pandas==1.0.4  \n",
        "scikit-learn==0.22.1  \n",
        "torch==1.5.0  \n",
        "transformers==2.11.0 "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f1iw8alTpw_b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Dockerfile  \n",
        "#First of all, we include where we are getting the image  \n",
        "#from. Image can be thought of as an operating system.  \n",
        "#You can do \"FROM ubuntu:18.04\"  \n",
        "#this will start from a clean ubuntu 18.04 image.  \n",
        "#All images are downloaded from dockerhub  \n",
        "#Here are we grabbing image from nvidia's repo  \n",
        "#they created a docker image using ubuntu 18.04  \n",
        "#and installed cuda 10.1 and cudnn7 in it. Thus, we don't have to  \n",
        "#install it. Makes our life easy.  \n",
        "FROM nvidia/cuda:10.1-cudnn7-runtime-ubuntu18.04  \n",
        "#this is the same apt-get command that you are used to  \n",
        "#except the fact that, we have -y argument. Its because  \n",
        "#when we build this container, we cannot press Y when asked for  \n",
        "RUN apt-get update && apt-get install -y\\  \n",
        "git\\  \n",
        "curl\\  \n",
        "ca-certificates\\  \n",
        "python3\\\n",
        "python3-pip\\\n",
        "sudo\\\n",
        "&& rm -rf /var/lib/apt/lists/*  \n",
        "\n",
        "#We add a new user called \"abhishek\"  \n",
        "#this can be anything. Anything you want it  \n",
        "#to be. Usually, we don't use our own name,  \n",
        "#you can use \"user\" or \"ubuntu\"  \n",
        "RUN useradd -m abhishek  \n",
        "#make our user own its own home directory  \n",
        "RUN chown -R abhishek:abhishek /home/abhishek/  \n",
        "#copy all files from this direrctory to a  \n",
        "#directory called app inside the home of abhishek  \n",
        "#and abhishek owns it.  \n",
        "COPY --chown=abhishek *.* /home/abhishek/app/  \n",
        "#change to user abhishek  \n",
        "USER abhishek  RUN mkdir /home/abhishek/data/  \n",
        "#Now we install all the requirements  \n",
        "#after moving to the app directory \n",
        "#PLEASE NOTE that ubuntu 18.04 image  \n",
        "#has python 3.6.9 and not python 3.7.6  \n",
        "#you can also install conda python here and use that  \n",
        "#however, to simplify it, I will be using python 3.6.9  \n",
        "#inside the docker container!!!!  \n",
        "RUN cd /home/abhishek/app/ && pip3 install -r requirements.txt  \n",
        "#install mkl. its needed for transformers  \n",
        "RUN pip3 install mkl  \n",
        "#when we log into the docker container,  \n",
        "#we will go inside this directory automatically  \n",
        "WORKDIR /home/abhishek/app\n",
        "\n",
        "#Once we have created the docker file, we need to build it. Building the docker  container is a very simple command. \n",
        "docker build -f Dockerfile -t bert:train. "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SGkFUM09q8VO",
        "colab_type": "text"
      },
      "source": [
        "This command builds a container from the provided Dockerfile. The name of the  docker container is bert:train. This produces the following output:\n",
        "\n",
        "❯ docker build -f Dockerfile -t bert:train.  \n",
        "\n",
        "Sending build context to Docker daemon 19.97kB  \n",
        "\n",
        "Step 1/7: FROM nvidia/cuda:10.1-cudnn7-ubuntu18.04  ---> 3b55548ae91f  \n",
        "Step 2/7: RUN apt-get update && apt-get install -y git curl cacertificates python3 python3-pip sudo && rm -rf  /var/lib/apt/lists/*  \n",
        ".  .  .  .  \n",
        "\n",
        "Removing intermediate container 8f6975dd08ba  ---> d1802ac9f1b4  \n",
        "\n",
        "Step 7/7: WORKDIR /home/abhishek/app  ---> Running in 257ff09502ed  Removing intermediate container 257ff09502ed  ---> e5f6eb4cddd7  Successfully built e5f6eb4cddd7  Successfully tagged bert:train  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pjzbanC9rOFa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "$docker run -ti bert:train /bin/bash \n",
        "\n",
        "#You need to remember that whatever you do in this shell will be lost once you exit  the shell. \n",
        "#And you can run the training inside the docker container using:   \n",
        "$docker run -ti bert:train python3 train.py "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g3tTRDRTr_JH",
        "colab_type": "text"
      },
      "source": [
        "Traceback (most recent call last):  File \"train.py\", line 2, in <module>  import config  File \"/home/abhishek/app/config.py\", line 28, in <module>  do_lower_case=True  File \"/usr/local/lib/python3.6/distpackages/transformers/tokenization_utils.py\", line 393, in  from_pretrained  return cls._from_pretrained(*inputs, **kwargs)  File \"/usr/local/lib/python3.6/distpackages/transformers/tokenization_utils.py\", line 496, in  _from_pretrained  list(cls.vocab_files_names.values()),  OSError: Model name '../input/bert_base_uncased/' was not found in  tokenizers model name list (bert-base-uncased, bert-large-uncased, bertbase-cased, bert-large-cased, bert-base-multilingual-uncased, bert-basemultilingual-cased, bert-base-chinese, bert-base-german-cased, bertlarge-uncased-whole-word-masking, bert-large-cased-whole-word-masking,  bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-casedwhole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bertbase-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-basefinnish-cased-v1, bert-base-finnish-uncased-v1, bert-base-dutch-cased).  We assumed '../input/bert_base_uncased/' was a path, a model identifier,  or url to a directory containing vocabulary files named ['vocab.txt'] but  couldn't find such vocabulary files at this path or url.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a7dpts27sSFo",
        "colab_type": "text"
      },
      "source": [
        "Because it’s very important to understand this error. This error says that the code  was unable to find the directory “../input/bert_base_cased”. Why does this happen?  We were able to train without docker, and we can see that the directory and all the  files exist. It happens because docker is like a virtual machine! It has its own  filesystem and the files from your local machine are not shared to the docker  container. If you want to use a path from your local machine and want to modify it  too, you would need to mount it to the docker container when running it. When we  look at this folder path, we know that it is one level up in a folder called input. Let’s  change the config.py file a bit! "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eqichbv7sV0k",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#config.py  \n",
        "import os  \n",
        "import transformers  \n",
        "#fetch home directory  \n",
        "#in our docker container, it is  \n",
        "#/home/abhishek  \n",
        "HOME_DIR = os.path.expanduser(\"~\")  \n",
        "#this is the maximum number of tokens in the sentence  \n",
        "MAX_LEN = 512  \n",
        "#batch sizes is low because model is huge!  \n",
        "TRAIN_BATCH_SIZE = 8  \n",
        "VALID_BATCH_SIZE = 4  \n",
        "#let's train for a maximum of 10 epochs  \n",
        "EPOCHS = 10 \n",
        "#define path to BERT model files  \n",
        "#Now we assume that all the data is stored inside  \n",
        "#/home/abhishek/data  \n",
        "BERT_PATH = os.path.join(HOME_DIR, \"data\", \"bert_base_uncased\")  \n",
        "#this is where you want to save the model  \n",
        "MODEL_PATH = os.path.join(HOME_DIR, \"data\", \"model.bin\")  \n",
        "#training file  \n",
        "TRAINING_FILE = os.path.join(HOME_DIR, \"data\", \"imdb.csv\")  \n",
        "TOKENIZER = transformers.BertTokenizer.from_pretrained(  BERT_PATH,  do_lower_case=True  ) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_8KfwmKes67M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#taken from: https://github.com/NVIDIA/nvidia-docker/  \n",
        "#Add the package repositories  \n",
        "distribution=$(. /etc/os-release;echo $ID$VERSION_ID)  \n",
        "curl -s -L https://nvidia.github.io/nvidia-docker/gpgkey | sudo apt-key  add -  \n",
        "curl -s -L https://nvidia.github.io/nvidia-docker/$distribution/nvidiadocker.list | sudo tee /etc/apt/sources.list.d/nvidia-docker.list  \n",
        "sudo apt-get update && sudo apt-get install -y nvidia-container-toolkit  \n",
        "sudo systemctl restart docker \n",
        "\n",
        "$docker run --gpus 1 -v  \n",
        "/home/abhishek/workspace/approaching_almost/input/:/home/abhishek/data/ -  ti bert:train python3 train.py "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "axDt7IdXs4u7",
        "colab_type": "text"
      },
      "source": [
        "Where –gpus 1 says that we use 1 GPU inside the docker container and -v is  mounting a volume. So, we are mounting our local directory,  /home/abhishek/workspace/approaching_almost/input/ to /home/abhishek/data/ in  the docker container. This step is going to take a while, but when it’s done, you will  have model.bin inside the local folder.  So, with some very simple changes, you have now “dockerized” your training code.  You can now take this code and train on (almost) any system you want. \n",
        "The next part is “serving” this model that we have trained to the end-user. Suppose,  you want to extract sentiment from a stream of incoming tweets. To do this kind of  task, you must create an API that can be used to input the sentence and in turns  returns an output with sentiment probabilities. The most common way of building  an API using Python is with Flask, which is a micro web service framework. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FYDrVkc-t3aV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#api.py  \n",
        "import config  \n",
        "import flask\n",
        "import time  \n",
        "import torch  \n",
        "import torch.nn as nn  \n",
        "from flask import Flask  \n",
        "from flask import request  \n",
        "from model import BERTBaseUncased  \n",
        "app = Flask(__name__)  \n",
        "#init model to None  \n",
        "MODEL = None  \n",
        "#choose device  \n",
        "#please note that we are using cuda device  \n",
        "#you can also use cpu!  \n",
        "DEVICE = \"cuda\"  \n",
        "def sentence_prediction(sentence):  \n",
        "  \"\"\"  A prediction function that takes an input sentence  and returns the probability for it being associated  to a positive sentiment  \"\"\"  \n",
        "  #fetch the tokenizer and max len of tokens from config.py  \n",
        "  tokenizer = config.TOKENIZER  \n",
        "  max_len = config.MAX_LEN  \n",
        "  #the processing is same as it was done for training  \n",
        "  review = str(sentence)  \n",
        "  review = \" \".join(review.split())  \n",
        "  #encode the sentence into ids,  \n",
        "  #truncate to max length &  \n",
        "  #add CLS and SEP tokens  \n",
        "  inputs = tokenizer.encode_plus(  review,  None,  add_special_tokens=True,  max_length=max_len  )  \n",
        "  #fetch input ids, mask & token type ids  \n",
        "  ids = inputs[\"input_ids\"]  \n",
        "  mask = inputs[\"attention_mask\"]  \n",
        "  token_type_ids = inputs[\"token_type_ids\"]\n",
        "  #add padding if needed  \n",
        "  padding_length = max_len - len(ids)  \n",
        "  ids = ids + ([0] * padding_length)  \n",
        "  mask = mask + ([0] * padding_length)  \n",
        "  token_type_ids = token_type_ids + ([0] * padding_length)  \n",
        "  #convert all the inputs to torch tensors  \n",
        "  #we use unsqueeze(0) since we have only one sample  \n",
        "  #this makes the batch size 1  \n",
        "  ids = torch.tensor(ids, dtype=torch.long).unsqueeze(0)  \n",
        "  mask = torch.tensor(mask, dtype=torch.long).unsqueeze(0)  \n",
        "  token_type_ids = torch.tensor(token_type_ids,  dtype=torch.long).unsqueeze(0)  \n",
        "  #send everything to device  \n",
        "  ids = ids.to(DEVICE, dtype=torch.long)  \n",
        "  token_type_ids = token_type_ids.to(DEVICE, dtype=torch.long)  \n",
        "  mask = mask.to(DEVICE, dtype=torch.long)  \n",
        "  #use the model to make predictions  \n",
        "  outputs = MODEL(ids=ids, mask=mask, token_type_ids=token_type_ids)  \n",
        "  #take sigmoid of prediction and return the output  \n",
        "  outputs = torch.sigmoid(outputs).cpu().detach().numpy()  \n",
        "  return outputs[0][0]  \n",
        "@app.route(\"/predict\", methods=[\"GET\"])  \n",
        "def predict():  \n",
        "  #this is our endpoint!  \n",
        "  #this endpoint can be accessed by http://HOST:PORT/predict  \n",
        "  #the endpoint needs sa sentence and can only use GET  \n",
        "  #POST request is not allowed  \n",
        "  sentence = request.args.get(\"sentence\")  \n",
        "  #keep track of time  \n",
        "  start_time = time.time()  \n",
        "  #make prediction  \n",
        "  positive_prediction = sentence_prediction(sentence)  \n",
        "  #negative = 1 - positive  \n",
        "  negative_prediction = 1 - positive_prediction  \n",
        "  #create return dictionary  \n",
        "  response = {}  \n",
        "  response[\"response\"] = {  \"positive\": str(positive_prediction),\n",
        "                          \"negative\": str(negative_prediction),  \n",
        "                          \"sentence\": str(sentence),  \n",
        "                          \"time_taken\": str(time.time() - start_time),  }  \n",
        "                          \n",
        "  #we use jsonify from flask for dictionaries  \n",
        "  return flask.jsonify(response)    \n",
        "if  __name__  ==  \"__main__\":  \n",
        "  #init the model  \n",
        "  MODEL = BERTBaseUncased()  \n",
        "  #load the dictionary  \n",
        "  MODEL.load_state_dict(torch.load(  config.MODEL_PATH, map_location=torch.device(DEVICE)  ))  \n",
        "  #send model to device  \n",
        "  MODEL.to(DEVICE)  \n",
        "  #put model in eval mode  \n",
        "  MODEL.eval()  \n",
        "  #start the application  \n",
        "  #0.0.0.0 means that this endpoint can be  \n",
        "  #accessed from all computers in a network  \n",
        "  app.run(host=\"0.0.0.0\")    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KnFig_QD19BC",
        "colab_type": "text"
      },
      "source": [
        "And you start the API by running the command “python api.py”. The API will start  on localhost on port 5000.  A sample cURL request and its response is shown as follows.  \n",
        "\n",
        "❯ curl \n",
        "\n",
        " $'http://192.168.86.48:5000/predict?sentence=this%20is%20the%20best%20boo  k%20ever'  {\"response\":{\"negative\":\"0.0032927393913269043\",\"positive\":\"0.99670726\",\"  sentence\":\"this is the best book  ever\",\"time_taken\":\"0.029126882553100586\"}}  \n",
        "\n",
        " As you can see that we got a high probability for positive sentiment for the provided  input sentence. You can also access the results by visiting  http://127.0.0.1:5000/predict?sentence=this%20book%20is%20too%20complicat  ed%20for%20me in your favourite browser. This will return a JSON again. \n",
        "\n",
        "\n",
        " { \n",
        "    response: {  negative: \"0.8646619468927383\",  \n",
        "    positive: \"0.13533805\",  \n",
        "    sentence: \"this book is too complicated for me\",  \n",
        "    time_taken: \"0.03852701187133789\"  }  } "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4hKumPai2zlA",
        "colab_type": "text"
      },
      "source": [
        "Now, we have created a simple API that we can use to serve a small number of  users. Why small? Because this API will serve only one request at a time. Let’s use  CPU and make it work for many parallel requests using gunicorn which is a python  WSGI HTTP server for UNIX. Gunicorn can create multiple processes for the API,  and thus, we can serve many customers at once. You can install gunicorn by using  “pip install gunicorn”.  To convert the code compatible with gunicorn, we need to remove init main and  move everything out of it to the global scope. Also, we are now using CPU instead  of the GPU. See the modified code as follows. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9D6TQYOi26rA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#api.py  \n",
        "import config  \n",
        "import flask  \n",
        "import time  \n",
        "import torch  \n",
        "import torch.nn as nn  \n",
        "from flask import Flask  \n",
        "from flask import request  \n",
        "from model import BERTBaseUncased  \n",
        "app = Flask(__name__)  \n",
        "#now we use cpu!  \n",
        "DEVICE = \"cpu\"  \n",
        "#init the model \n",
        "MODEL = BERTBaseUncased()  \n",
        "#load the dictionary  \n",
        "MODEL.load_state_dict(torch.load(  config.MODEL_PATH, map_location=torch.device(DEVICE)  ))  \n",
        "#send model to device  \n",
        "MODEL.to(DEVICE)  \n",
        "#put model in eval mode  \n",
        "MODEL.eval()  \n",
        "def sentence_prediction(sentence):  \n",
        "  \"\"\"  A prediction function that takes an input sentence  and returns the probability for it being associated  to a positive sentiment  \"\"\" \n",
        "   .  .  .  \n",
        "   return outputs[0][0]  \n",
        "@app.route(\"/predict\", methods=[\"GET\"])  \n",
        "def predict():  \n",
        "  #this is our endpoint!  \n",
        "  .  .  .  \n",
        "  return flask.jsonify(response) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jn8mQWaT3Vxj",
        "colab_type": "text"
      },
      "source": [
        "And we run this API using the following command.  \n",
        "\n",
        "$gunicorn api:app --bind 0.0.0.0:5000 --workers 4  \n",
        "\n",
        "This means we are running our flask app with 4 workers on a provided IP address  and port. Since there are 4 workers, we are now serving 4 simultaneous requests.  Please note that now our endpoint uses CPU and thus, it does not need a GPU  machine and can run on any standard server/VM. Still, we have one problem, we  have done everything in our local machine, so we must dockerize it. Take a look at  the following uncommented Dockerfile which can be used to deploy this API. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1nJo1tGTJaUz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Notice the difference between the old Dockerfile for training and this one. There  are not many differences.  \n",
        "#CPU Dockerfile  FROM ubuntu:18.04  \n",
        "RUN apt-get update && apt-get install -y\\  \n",
        "git\\  \n",
        "curl\\  \n",
        "ca-certificates\\  \n",
        "python3\\  \n",
        "python3-pip\\  \n",
        "sudo\\  \n",
        "&& rm -rf /var/lib/apt/lists/*  \n",
        "\n",
        "RUN useradd -m abhishek  \n",
        "\n",
        "RUN chown -R abhishek:abhishek /home/abhishek/  \n",
        "\n",
        "COPY --chown=abhishek *.* /home/abhishek/app/  \n",
        "USER abhishek  \n",
        "RUN mkdir /home/abhishek/data/  \n",
        "RUN cd /home/abhishek/app/ && pip3 install -r requirements.txt  \n",
        "RUN pip3 install mkl  \n",
        "WORKDIR /home/abhishek/app "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0MFLByQqJz1y",
        "colab_type": "text"
      },
      "source": [
        "Let’s build a new docker container. \n",
        " \n",
        "$docker build -f Dockerfile -t bert:api. \n",
        " \n",
        "When the docker container is built, we can now run the API directly by using the  following command. \n",
        "$docker run -p 5000:5000 -v  /home/abhishek/workspace/approaching_almost/input/:/home/abhishek/data/ -  ti bert:api /home/abhishek/.local/bin/gunicorn api:app --bind  0.0.0.0:5000 --workers 4  \n",
        "\n",
        "Please note that we expose port 5000 from the container to 5000 outside the  container. This can also be done in a nice way if you use docker-compose. Docker compose is a tool that can allow you to run different services from different or the  same containers at the same time. You can install docker-compose using “pip install  docker-compose” and then run “docker-compose up” after building the container.  To use docker-compose, you need a docker-compose.yml file. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pz4i7x2tKKoy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#docker-compose.yml  \n",
        "#specify a version of the compose  \n",
        "version: '3.7'  \n",
        "\n",
        "#you can add multiple services  \n",
        "services:  \n",
        "#specify service name. we call our service: api  \n",
        "api:  \n",
        "#specify image name  \n",
        "image: bert:api  \n",
        "#the command that you would like to run inside the container  \n",
        "command: /home/abhishek/.local/bin/gunicorn api:app --bind  0.0.0.0:5000 --workers 4  \n",
        "#mount the volume  \n",
        "volumes:  \n",
        "-  \n",
        "/home/abhishek/workspace/approaching_almost/input/:/home/abhishek/data/  \n",
        "#this ensures that our ports from container will be  \n",
        "#exposed as it is  \n",
        "network_mode: host "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V5Bt-_KrKsBo",
        "colab_type": "text"
      },
      "source": [
        "Now you can rerun the API just by using the command mentioned above, and it will  work the same way as before. Congratulations! Now you have managed to  dockerized the prediction API too, and it is ready for deployment anywhere you  want. In this chapter, we learned docker, building APIs using flask, serving API  using gunicorn and docker and docker-compose. There is a lot more to docker  than we have seen here, but this should give you a start. Rest can be learned as you  progress. We have also skipped on many tools like kubernetes, bean-stalk,  sagemaker, heroku and many others that people use these days for deploying  models in production. “What am I going to write? Click on modify docker container  in figure X”? It’s not feasible and advisable to describe these in a book, so I will be  using a different medium complimenting this part of the book. Remember that once  you have dockerized your application, deploying using any of these  technologies/platforms is a piece of cake. Always remember to make your code and  model usable and well-documented for others so that anyone can use what you have  developed without asking you several times. This will save you time, and it will also save their time. Good, open-source, re-usable code also looks good in your  portfolio."
      ]
    }
  ]
}